{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "", "text": "<p>LangTorch is a framework that accelerates development of complex language model applications by leveraging familiar PyTorch concepts.</p> <p>While existing frameworks focus on connecting language models to other services, LangTorch aims to change the way you approach creating LLM applications by introducing a text tensor system governed by simple rules that unify working with texts, chats, markup languages, prompt templates and prompting techniques, tokens, embeddings, LLM calls and tools with seamless parallelization working with OpenAI APIs and integrated with PyTorch. The result? Less code, faster development and more intricate LLM applications.</p> LangTorch on GitHub"}, {"location": "#the-langtorch-framework", "title": "The LangTorch Framework", "text": "<p>Instead of providing wrapper classes for users to memorize, LangTorch introduces flexible objects that, while governed by simple rules, enable all kinds of text formatting, templating and LLM operations. This lets developers think about what they want to build, instead of how the classes were named. LangTorch components subclass their numerical PyTorch counterparts, which lets users apply their existing coding skills to building novel LLM app architectures. The design of the package is geared towards users who want to test and create new architectures or methods that increase the utility of LLMs, rather than streamlining the most common chat and RAG architectures.</p> <p>Note the package is early in its development and some features may be unstable or unfinished!  Use for research rather than production purposes</p> <p>The framework is centered around these objects and utilities (to skip to results, see how compact full Retriever and RAG implementations are):</p> TextTensors        inherits functionalities from <code>torch.Tensor</code>, but holds structured text as entries. <code>TextTensors</code> allow us to structure text information geometrically and dynamically inject this data into LLM calls.The utility of Tensors, as used in Torch, relies on their ability to calculate simultaneously products of several weights. The corresponding feature in LangTorch allows several prompts to be formatted on several inputs, by defining the product of <code>text1*text2</code> similarly to <code>text1.format(**text2)</code>.       <pre><code>prompt_templates = TextTensor(\n                   [[\"{name}: {greeting}!\"],  \n                    [\"{greeting}, {name}!\"]]\n                   )\n\nx = prompt_templates * TextTensor(\n            {\"greeting\": \"Hello\", \"name\": \"Alice\"}\n            ) \nprint(x)\n</code></pre> Output:<pre><code>[[Alice: Hello!]\n [Hello, Alice!]]\n</code></pre> <pre><code>from langtorch import TextModule\n\nprompts = [[\"Summarize this text: \"],  \n           [\"Simplify this text: \" ]]\nparallel_tasks = TextModule(prompts, activation=\"gpt-4\")\n\n# Do multiple tasks on multiple inputs\ninputs = TextTensor([text1, text2, text3])\noutputs = parallel_tasks(inputs)\nprint(outputs.shape)\n</code></pre> Output:<pre><code>torch.Size([2, 3])\n</code></pre> Supporting PyTorch utilities   LangTorch implements textual versions of many PyTorch utilities. TextTensors can be accessed, reshaped, squeezed, unsqueezed and so on just like torch tensors.   Moreover, using regular torch functions they can be summed (text join), stacked, concatenated, used in tensor datasets, and even traced with autograd. In the near future this ability will be used in automatic prompt optimization given a text description of our \"loss function\".       TextTensors OpsPyTorch DatasetsTransformations <p> <pre><code>x = TextTensor([[\"a\", \"b\"],\n                [\"c\", \"d\"]])\n# Indexing\nx_col = x[:, 0] \n# Reshaping, stacking, squeezing\nx_col = x_col.reshape(2, 1)\nx_col = torch.concat((x_col, x_col, x_col))\n\nprint(x_col.squeeze())\n</code></pre> Output:<pre><code>[ a  c  a  c  a  c ]\n</code></pre> </p> <p> <pre><code>from torch.utils.data import DataLoader, TensorDataset\n\ninputs, targets = TextTensor(df[\"task\"]), TextTensor(df[\"answer\"])\ndataset = TensorDataset(inputs, targets)\n\n# Batch size for llm calls\ndataloader = DataLoader(dataset, batch_size=16)\n\nllm = OpenAI(\"gpt-4\")\ncheck_answer = TextModule(\"Is this response the same as the target? \", activation=llm)\n\nfor i, (inputs, targets) in enumerate(dataloader):\n    llm_answers = llm(inputs)\n    loss = check_answer(llm_answers +\"\\nTarget: \"+targets)\n</code></pre> </p> <p> <pre><code>x = TextTensor([[\"a\", \"b\"],\n                [\"c\", \"d\"]])\n# Addition concatenates texts\nx2 = x + x[0]   # Adding first row with broadcasting\nprint(x2)\n\n# We can add many entries by summing\nx3 = torch.sum(x, dim=0)\n\nprint(f\"x2:\\n{x2}\\nx3:\\n{x3}\")\n</code></pre> Output:<pre><code>x2:\n[[aa  bb]\n [ca  db]]\nx3:\n[ ac  bd ]\n</code></pre> </p> <pre><code>import torch  \n\ntensor1 = TextTensor([[\"Yes\"], [\"No\"]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1, tensor2)\n</code></pre> Output:<pre><code>tensor([[0.6923, 0.6644, 0.6317, 0.5749],\n        [0.5457, 0.7728, 0.5387, 0.7036]])\n</code></pre> Text, Tokens and Embeddings   Apart from streamlining work on texts, <code>TextTensors</code> can hold multiple types of representations of the same text  and automatically switch between acting as strings when printed, as embeddings when inputted into <code>torch.cosine_similarity</code> and as tokens when passed to a local LLM.       Richer text representations   Every <code>Text</code> entry of a <code>TextTensor</code> is structured, such that they can be constructed from and formatted into any markup language. This unified structure allows for a systematic treatment of diverse objects like chat histories, chunked documents, document stores, dictionaries, text meta-data, extracted named entities and code.   The  example illustrates how a  <code>Text</code> can be chunked into segments that occupy separate entries of the created TextTensor, which allows us to quickly transform each paragraph separately.      <pre><code>from langtorch import Text\n\ntext = Text(some_string)\ntransform = TextModule(\"Rewrite to improve clarity: \", \n                        activation=\"gpt-4\")\n# Chunk into paragraphs and transform\nparagraphs = text.split(\"\\n\")\nresult = transform(pargraphs)\n\n# Join new entries separated with new line characters\nrewritten_text = (result +\"\\n\").sum() \n</code></pre>"}, {"location": "#textmodule", "title": "TextModules", "text": "are the compositional building blocks. A subclass of <code>torch.nn.Module</code>, <code>TextModules</code> are reusable \"layers\", whose weights are <code>TextTensors</code>, and which instead of an activation function can include the activation of an LLM call on the dynamically created text inputs.Instead of many clunky retrievers, chains, chats or agents, all can be defined as <code>TextModules</code> with different forward calls. These can then be easily included as submodules in complex modules that in the forward pass perform any parts of common architectures, like operations on embeddings, retrieval, parallel LLM API calls, batched local LLM inference, actions and so on."}, {"location": "#dive-in-and-get-started", "title": "Dive In and Get Started", "text": "<p>Install LangTorch with: <pre><code>pip install langtorch\n</code></pre></p> <p>Next steps:</p> <ul> <li> <p> Quick-Start Tutorial</p> <p>Get building with LangTorch in 5 minutes</p> <p> Getting started</p> </li> <li> <p> Reference</p> <p>Learn how to use <code>Text</code> objects, <code>TextTensors</code>, <code>TextModules</code>, <code>Activations</code>  and <code>torch</code> functions with LangTorch</p> <p> Reference.</p> </li> <li> <p> Discord</p> <p>Join the Discord community to for fast bug fixes and support!</p> <p> Discord</p> </li> <li> <p> Github</p> <p> Star </p> <p> GitHub Repo</p> </li> </ul> <p>Join us in improving the LLM application dev  experience!</p>"}, {"location": "#about", "title": "About", "text": "<p>LangTorch is an open source python package by Adam Sobieszek (University of Warsaw; Jutro Medical). Contributions, comments and issue submissions are very much welcomed.</p> <p>Contact: contact@langtorch.org</p> <p>I'd like to thank Tadeusz Price, Hubert Plisiecki, Jakub Podolak and Miko\u0142aj Boro\u0144ski for their help in testing and explaining the package, as well as the University of Warsaw and Jutro Medical for supporting early development.</p>"}, {"location": "quickstart/", "title": "Quickstart Guide: Dive into LangTorch", "text": ""}, {"location": "quickstart/#installation", "title": "Installation", "text": "<p>LangTorch works with Python 3.8 or higher. To install LangTorch using pip:</p> <pre><code>pip install langtorch\n</code></pre>"}, {"location": "quickstart/#1-simple-chains", "title": "1. Simple chains", "text": ""}, {"location": "quickstart/#getting-tensor-text-data", "title": "Getting tensor text data", "text": "<p><code>TextTensors</code> are designed to simplify simultaneous handling of text data. They inherit most functionality from PyTorch's <code>torch.Tensor</code> but hold textual data, which you may create from documents, prompt templates, completion dictionaries and more.</p> <pre><code>from langtorch import TextTensor\n\ntt = TextTensor([[\"prompt1\"], [\"prompt2\"]])\n\nlines = TextTensor(open('doc.txt','r').readlines())\n\n# Append to every entry in lines\nlines = lines + \" Have a great day!\"\n</code></pre>"}, {"location": "quickstart/#texttensor-operations-for-prompt-templating", "title": "TextTensor operations for prompt templating", "text": "<p><code>TextTensors</code> provide operations that allow for formatting and editing many entries at the same time according to array broadcasting rules. Adding a <code>TextTensors</code> appends its content to the other, while multiplication performs a more complex operation that can be used for template formatting:</p> <pre><code>completions = TextTensor([[{\"name\": \"Luciano\"}],\n                          [{\"name\": \"Massimo\"}]])\nprint(completions)\n# Output\n\n\n# Prompt template formatting\nprompts = TextTensor([\"Hello, {name}!\"]) * completions\n</code></pre> <p>A useful informal definition for the multiplication operation is that when two entries are multiplied, the right Text acts like a format operation: replacing keys with values (here, {name} with Luciano) or appending if there is nothing to replace. For a more in depth look, see TextTensor Multiplication.</p>"}, {"location": "quickstart/#performing-a-task-with-textmodules", "title": "Performing a task with TextModules", "text": "<p>TextModules like <code>nn.Module</code> implement a forward method that works on (text) tensors. By default, they can be initialized by passing a TextTensor of prompts, that in the forward pass will be formatted using the input TextTensor (just like in the example above).  </p> <p>To achieve interesting behavior, an nn.Module layer usually ends with passing the multiplied tensors to an \"activation function\". By analogy, TextModules usually end with an activation of an LLM call on the formatted prompts (for more on this parallel see. langtorch.tt). LangTorch activation like <code>OpenAI</code> execute their LLM calls on each entry of the input TextTensor in parallel.</p> <pre><code>from langtorch import TextModule, OpenAI\n\nllm = OpenAI(\"gpt4\", T = 0.) # Pass any API kwargs here to customize the call \n# Initialize TextModule with the activation\ntranslate = TextModule(\"Translate this text to Polish: {}\", activation=llm)\n\n# We can run our task on any TextTensor input\noutput = translate(prompts)\n</code></pre> <p>Remember to set your API key e.g. with:</p> <pre><code>os.environ[\"OPENAI_API_KEY\"] = key\n</code></pre>"}, {"location": "quickstart/#2-implementing-popular-methods", "title": "2. Implementing popular methods", "text": ""}, {"location": "quickstart/#parallel-and-chained-calls-with-textmodules", "title": "Parallel and Chained calls with TextModules", "text": "<p>LangTorch uses a custom implementation to speed up and cache api calls, that by default run in parallel for all TextTensor entries passed to an LLM activation. As such, running calls in parallel is done automatically if either multiple prompts, multiple input values or both are passed to an LLM. </p> <p>The simplest way to chain TextModule is to directly use <code>torch.nn.Sequential</code>. To create any complex chain you may, as in torch, define a module subclass that adds custom behavior or combines many submodules in one.  We will show these on examples of popular LLM methods.</p>"}, {"location": "quickstart/#chain-of-thought", "title": "Chain of Thought", "text": "<p>The simplest example of a custom module are those that implement prompting methods like Chain of Thought, where all we need is to append a fixed string to the input. This can be done by creating a reusable TextModule that we can chain with any task module:  </p> <pre><code>from langtorch import TextModule, OpenAI\nimport torch\n\n# Remember to add spaces or new line characters to the appended texts\nchain_of_thought = TextModule(\" Let's think step by step.\")  \ntask = TextModule(some_prompt_template)  \n\n# OpenAI activation is also a Module so we can chain it explicitly here:\ntask_module_w_CoT = torch.nn.Sequential(  \n    task,  \n    chain_of_thought,\n    OpenAI(\"gpt-4\")\n)  \n</code></pre> <p>Or subclassing <code>TextModule</code> and overriding the forward method: <pre><code># Custom TextModule override methods like __init__ and forward\nclass ChainOfThought(TextModule):\n    def forward(self, input):\n        return super()(input + \" Let's think step by step.\")\n\n# By subclassing and using super() we can still set prompts and activations\ntask = ChainOfThought(some_prompt_template, activation=llm)\n</code></pre></p>"}, {"location": "quickstart/#ensemble-self-consistency", "title": "Ensemble / Self-consistency", "text": "<p>Many benefits of being able to represent texts \"geometrically\" in a matrix / tensor comes from being able to create a meaningful structure, where e.g. a 2d matrix has columns representing different versions of the same text and subsequent entries represent subsequent paragraphs. Methods like ensemble voting and self-consistency require creating multiple completions for the same task, which can be representing by adding such a \"version\" dimension.</p> <p>In this example, we will build such a module that for each entry creates multiple answer entries and combines them back together to increase the overall performance. First, to add a new dimension with different answers given by the LLM we need only adjust the <code>n</code> parameter. Additionally, we can set the system message:</p> <pre><code>from langtorch import OpenAI\nensemble_llm = OpenAI(\"gpt-3.5-turbo\", \n                      system_message=\"You are a rewriting bot that answers only with the revised text\", \n                      T=1.1, # High temperature to sample diverse completions\n                      n = 5) # 5 completions for each entry\n</code></pre> <p>To use a concrete example, we will write a module that uses an ensemble to compress a text paragraph by paragraph. The task description is inspired by the Chain of Density method. For now, let's assume <code>paragraphs</code> is defined as a TextTensor with 15 paragraph-entries:</p> <pre><code>rewrite = TextModule([\"Compress all information from the paragraph into an entity-dense telegraphic summary: \"], activation = ensemble_llm) \nensemble_summaries = rewrite(paragraphs)\nprint(ensemble_summaries.shape)\n# Outputs (15, 5)\n</code></pre> <p>The last dimension now identifies different summaries. To combine them we could:</p> <ul> <li>Add a separator to each entry and use<code>.sum(dim=-1)</code>to create 15 entries with texts to combine and pass them to a \"create a combined text\" module </li> <li>We can do this even quicker with langtorch's \"semantic algebra\", which has a <code>mean</code> function that creates a better \"average\" text from texts along a dimension.</li> </ul> <pre><code>combined_summaries = langtorch.mean(ensemble_summaries, dim=-1)\nprint(combined_summaries.shape)\n# Outputs: (15)\n\n# We can even average again across paragraphs!\nsummary = langtorch.mean(combined_summaries, dim=-1)\nprint(summary)\n# Outputs: The discussion surrounding GPT-3's capabilities emphasizes its performance in the Turing Test, its tendency to prioritize plausibility over truth, and the societal implications of truth-related challenges (...)\n</code></pre> <p>Similar approaches can be used for more complicated ensemble methods or combined with methods like chain of thought to increase accuracy with \"self-consistency\".</p>"}, {"location": "quickstart/#working-with-structured-documents", "title": "Working with structured documents", "text": "<p>Instead of strings, each entry of a TextTensor is an instance of <code>langtorch.Text</code>, which allows for more complex text processing. The <code>Text</code> class can load documents, parse most markup languages and provide a helpful interface for accessing and modifying their structured text segments. We will prepare data for a rewrite task like before by parsing a markdown file of a paper on the abilities of language models, available here paper.md. As the text has headers and other text blocks, we'll to select only paragraphs, which can be done with <code>iloc</code> and <code>loc</code> accessors:  </p> <pre><code>from langtorch import Text\npaper = Text.from_file(\"paper.md\")\n# Text automatically parses markdown, encoding its text block structure to let us simply access and modify their content\n\n# To access the first block \nfirst_block = paper.iloc[0]\n\n# To filter block types, we can take us that Text segments are have labels, accessible via:\nprint(set(paper.keys()))\n# Outputs: {'BlockQuote', 'Para', 'Header1', 'Header3', 'Header2'}\n\n# To get all paragraphs\nparagraphs = paper.loc[\"Para\"]\n\n# Some rewritting module\nrewritten_paragraphs = rewrite(paragraphs)\n\n# We can easily edit the unfiltered text by setting rewritten paragraphs\npaper.loc[\"Para\"] = rewritten_paragraphs\nprint(paper) # Outputs a string of the paper formatted in markdown with all rewritten paragraphs in their original places \n</code></pre>"}, {"location": "quickstart/#3-using-tensor-embeddings-to-build-retrievers", "title": "3. Using Tensor Embeddings to Build Retrievers", "text": "<p>Using embeddings with TextTensors is extremely easy, as every TextTensor can generate its own embedding, as well as know to automatically act as if it was an embedding tensor when passed to torch functions like cosine similarity. These representations (available under the <code>.embedding</code> attribute) are moreover automatically created only right before they are needed (via a set embedding model, by default OpenAI's <code>text-embedding-3-small</code>). </p> <pre><code>import torch  \n\ntensor1 = TextTensor([[[\"Yes\"], [\"No\"]]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1,tensor2)\n</code></pre>"}, {"location": "quickstart/#build-custom-retriever-and-rag-modules", "title": "Build Custom Retriever and RAG modules", "text": "<p>Using how <code>TextTensor</code>s can automatically act as a<code>Tensor</code> of it's embeddings, we can very compactly implement e.g. a retriever, which for each entry in the input finds in parallel <code>k</code> entries with the highest cosine similarity among the documents it holds:</p> <p><pre><code>class Retriever(TextModule):  \n    def __init__(self, documents: TextTensor):  \n        super().__init__()  \n        self.documents = TextTensor(documents).view(-1)  \n\n    def forward(self, query: TextTensor, k: int = 5):  \n        cos_sim = torch.cosine_similarity(self.documents, query.reshape(1))  \n        return self.documents[cos_sim.topk(k)]\n</code></pre> Usage:<pre><code>retriever = Retriever(open(\"doc.txt\", \"r\").readlines())\nquery = TextTensor(\"How to build a retriever?\")\n\nprint(retriever(query))\n</code></pre></p> <p>Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower level operations, while being able to write compact code without a multitude of classes. For this reason implementations such as the retriever above are not pre-defined classes in the main package.  </p> <p>We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:</p> <pre><code>class RAG(TextModule):  \n    def __init__(self, documents: TextTensor, *args, **kwargs):  \n        super().__init__(*args, **kwargs)  \n        self.retriever = Retriever(documents)  \n\n    def forward(self, user_message: TextTensor, k: int = 5):  \n        retrieved_context = self.retriever(user_message, k) +\"\\n\"  \n        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()  \n        return super().forward(user_message)\n</code></pre> Usage:<pre><code>rag_chat = RAG(paragraphs,  \n               prompt=\"Use the context to answer the following user query: \",\n               activation=\"gpt-3.5-turbo\")\n\nassistant_response = rag_chat(user_query)\n</code></pre> <p>With only small modifications to the retriever this module could also perform batched inference \u2014 performing multiple simultaneous queries without much additional latency. Note, <code>prompt</code> and <code>activation</code> are arguments inherited from TextModule and need the <code>super().forward</code> call to work. </p> <p>We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our discord. In the likely event of encountering a bug send it on discord or post on the GitHub Repo and we will fix it ASAP.</p>"}]}