{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "", "text": "<p>LangTorch is a framework that accelerates development of complex language model applications by leveraging familiar PyTorch concepts.</p> <p>While existing frameworks focus on connecting language models to other services, LangTorch aims to change the way you approach creating LLM applications by introducing a text tensor system governed by simple rules that unify working with texts, chats, markup languages, prompt templates and prompting techniques, tokens, embeddings, LLM calls and tools under seamless parallelization and integrated with PyTorch. The result? Less code, faster development and more intricate LLM applications.     </p> Star LangTorch on:  Github <p>Quickstart guide:  </p>"}, {"location": "#the-langtorch-framework", "title": "The LangTorch Framework", "text": "<p>Instead of providing wrapper classes for users to memorize, LangTorch introduces flexible objects that, while governed by simple rules, enable all kinds of text formatting, templating and LLM operations. This lets developers think about what they want to build, instead of how the classes were named. LangTorch components subclass their numerical PyTorch counterparts, which lets users apply their existing coding skills to building novel LLM app architectures. The design of the package is geared towards users who want to test and create new architectures or methods that increase the utility of LLMs, rather than streamlining the most common chat and RAG architectures.</p> <p>Note the package is early in its development and some features may be unstable or unfinished!  Use for research rather than production purposes</p> <p>The framework is centered around these objects and utilities (to skip to results, see how compact full Retriever and RAG implementations are):</p> TextTensors        inherits functionalities from <code>torch.Tensor</code>, but holds structured text as entries. <code>TextTensors</code> allow us to structure text information geometrically and dynamically inject this data into LLM calls.The utility of Tensors, as used in Torch, relies on their ability to calculate simultaneously products of several weights. The corresponding feature in LangTorch allows several prompts to be formatted on several inputs, by defining the product of <code>text1*text2</code> similarly to <code>text1.format(**text2)</code>.       <pre><code>prompt_templates = TextTensor(\n                   [[\"{name}: {greeting}!\"],  \n                    [\"{greeting}, {name}!\"]]\n                   )\n\nx = prompt_templates * TextTensor(\n            {\"greeting\": \"Hello\", \"name\": \"Alice\"}\n            ) \nprint(x)\n</code></pre> Output:<pre><code>[[Alice: Hello!]\n [Hello, Alice!]]\n</code></pre> <pre><code>from langtorch import TextModule\n\nprompts = [[\"Summarize this text: {}\"],  \n           [\"Simplify this text: {}\" ]]\nparallel_tasks = TextModule(prompts, activation=\"gpt-4o\")\n\n# Do multiple tasks on multiple inputs\ninputs = TextTensor([text1, text2, text3])\noutputs = parallel_tasks(inputs)\nprint(outputs.shape)\n</code></pre> Output:<pre><code>torch.Size([2, 3])\n</code></pre> Supporting PyTorch utilities   LangTorch implements textual versions of many PyTorch utilities. TextTensors can be accessed, reshaped, squeezed, unsqueezed and so on just like torch tensors.   Moreover, using regular torch functions they can be summed (text join), stacked, concatenated, used in tensor datasets, and even traced with autograd. In the near future this ability will be used in automatic prompt optimization given a text description of our \"loss function\".       TextTensors OpsPyTorch DatasetsTransformations <p> <pre><code>x = TextTensor([[\"a\", \"b\"],\n                [\"c\", \"d\"]])\n# Indexing\nx_col = x[:, 0] \n# Reshaping, stacking, squeezing\nx_col = x_col.reshape(2, 1)\nx_col = torch.concat((x_col, x_col, x_col))\n\nprint(x_col.squeeze())\n</code></pre> Output:<pre><code>[ a  c  a  c  a  c ]\n</code></pre> </p> <p> <pre><code>from torch.utils.data import DataLoader, TensorDataset\n\ninputs, targets = TextTensor(df[\"task\"]), TextTensor(df[\"answer\"])\ndataset = TensorDataset(inputs, targets)\n\n# Batch size for llm calls\ndataloader = DataLoader(dataset, batch_size=16)\n\nllm = OpenAI(\"gpt-4\")\ncheck_answer = TextModule(\"Is this response the same as the target? \", activation=llm)\n\nfor i, (inputs, targets) in enumerate(dataloader):\n    llm_answers = llm(inputs)\n    loss = check_answer(llm_answers +\"\\nTarget: \"+targets)\n</code></pre> </p> <p> <pre><code>x = TextTensor([[\"a\", \"b\"],\n                [\"c\", \"d\"]])\n# Addition concatenates texts\nx2 = x + x[0]   # Adding first row with broadcasting\nprint(x2)\n\n# We can add many entries by summing\nx3 = torch.sum(x, dim=0)\n\nprint(f\"x2:\\n{x2}\\nx3:\\n{x3}\")\n</code></pre> Output:<pre><code>x2:\n[[aa  bb]\n [ca  db]]\nx3:\n[ ac  bd ]\n</code></pre> </p> <pre><code>import torch  \n\ntensor1 = TextTensor([[\"Yes\"], [\"No\"]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1, tensor2)\n</code></pre> Output:<pre><code>tensor([[0.6923, 0.6644, 0.6317, 0.5749],\n        [0.5457, 0.7728, 0.5387, 0.7036]])\n</code></pre> Text, Tokens and Embeddings   Apart from streamlining work on texts, <code>TextTensors</code> can hold multiple types of representations of the same text  and automatically switch between acting as strings when printed, as embeddings when inputted into <code>torch.cosine_similarity</code> and as tokens when passed to a local LLM.       Richer text representations   Every <code>Text</code> entry of a <code>TextTensor</code> is structured, such that they can be constructed from and formatted into any markup language. This unified structure allows for a systematic treatment of diverse objects like chat histories, chunked documents, document stores, dictionaries, text meta-data, extracted named entities and code.   The  example illustrates how a  <code>Text</code> can be chunked into segments that occupy separate entries of the created TextTensor, which allows us to quickly transform each paragraph separately.      <pre><code>from langtorch import Text\n\ntext = Text(some_string)\ntransform = TextModule(\"Rewrite to improve clarity: \", \n                        activation=\"gpt-4\")\n# Chunk into paragraphs and transform\nparagraphs = text.split(\"\\n\")\nresult = transform(pargraphs)\n\n# Join new entries separated with new line characters\nrewritten_text = (result +\"\\n\").sum() \n</code></pre>"}, {"location": "#textmodule", "title": "TextModules", "text": "are the compositional building blocks. A subclass of <code>torch.nn.Module</code>, <code>TextModules</code> are reusable \"layers\", whose weights are <code>TextTensors</code>, and which instead of an activation function can include the activation of an LLM call on the dynamically created text inputs.Instead of many clunky retrievers, chains, chats or agents, all can be defined as <code>TextModules</code> with different forward calls. These can then be easily included as submodules in complex modules that in the forward pass perform any parts of common architectures, like operations on embeddings, retrieval, parallel LLM API calls, batched local LLM inference, actions and so on."}, {"location": "#dive-in-and-get-started", "title": "Dive In and Get Started", "text": "<p>Install LangTorch with: <pre><code>pip install langtorch\n</code></pre></p> <p>Next steps:</p> <ul> <li> <p> Quick-Start Tutorial</p> <p>Get building with LangTorch in 5 minutes</p> <p> Getting started</p> </li> <li> <p> Reference</p> <p>Learn how to use <code>Text</code> objects, <code>TextTensors</code>, <code>TextModules</code>, <code>Activations</code>  and <code>torch</code> functions with LangTorch</p> <p> Reference.</p> </li> <li> <p> Discord</p> <p>Join the Discord community to for fast bug fixes and support!</p> <p> Discord</p> </li> <li> <p> Github</p> <p> Star </p> <p> GitHub Repo</p> </li> </ul> <p>Join us in improving the LLM application dev  experience!</p>"}, {"location": "#about", "title": "About", "text": "<p>LangTorch is an open source python package by Adam Sobieszek (University of Warsaw; Jutro Medical). Contributions, comments and issue submissions are very much welcomed.</p> <p>Contact: contact@langtorch.org</p> <p>I'd like to thank Tadeusz Price, Hubert Plisiecki, Jakub Podolak and Miko\u0142aj Boro\u0144ski for their help in testing and explaining the package, as well as the University of Warsaw and Jutro Medical for supporting early development.</p>"}, {"location": "quickstart/", "title": "Quickstart Guide: Dive into LangTorch", "text": "<p> &lt; &lt; To go through this guide interactively, we recommend this Colab notebook version</p>"}, {"location": "quickstart/#installation", "title": "Installation", "text": "<p>To install LangTorch using pip:</p> <pre><code>pip install langtorch\n</code></pre> <p>To use the OpenAI API as our LLM, we need to set the <code>OPENAI_API_KEY</code> environment variable. You can find your API key on platform.openai.com</p> <pre><code>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # Replace with your actual OpenAI API key\n</code></pre>"}, {"location": "quickstart/#1-perform-multiple-llm-calls-with-texttensors", "title": "1. Perform multiple LLM calls with TextTensors", "text": "<pre><code>from langtorch import TextTensor # holds texts instead of weights, supports tensor operations\nfrom langtorch import TextModule # torch.nn modules working on TextTensors, perform prompt templating and llm calls\n</code></pre> <p><code>TextTensors</code> are designed to streamline working with many pieces of text and performing parallel LLM calls. <code>langtorch.TextTensor</code> is a subclass of PyTorch's <code>torch.Tensor</code> that:</p> <ul> <li> <p>Holds text entries instead of numerical weights.</p> </li> <li> <p>Special Structure: <code>TextTensors</code> entries can represent chunked documents, prompt templates, completion dictionaries, chat histories, and more.</p> </li> <li> <p>Represents Geometrically: <code>TextTensors</code> have a shape and can be modified with PyTorch functions (reshape, stack, etc.).</p> </li> </ul> <p>In this example, we will create tensors holding prompt templates, fill them with a tensor of completion dictionaries, and send them to the OpenAI API.</p> <pre><code>prompt_tensor = TextTensor([[\"Is this an email address? {input_field}\"],  \n                            [\"Is this a valid web link? {input_field}\"]])  \n\n# Adding TextTensors appends their content according to broadcasting rules  \nprompt_tensor += \" (Answer 'Yes' or 'No')\"  \nprint(prompt_tensor)  \nprint(\"Shape =\", prompt_tensor.shape)\n</code></pre> Output:<pre><code>[[Is this an email address? input_field (Answer 'Yes' or 'No')]\n [Is this a valid web link? input_field (Answer 'Yes' or 'No')]]\nShape = torch.Size([2, 1])\n</code></pre> <p><code>TextModules</code> are <code>torch.nn.Modules</code> that work on <code>TextTensors</code>:</p> <ul> <li> <p>Tensor of Prompts: They hold a tensor of prompts instead of numerical weights.</p> </li> <li> <p>Input Handling: They accept <code>TextTensors</code> as input, which are used to format the prompt tensor.</p> </li> <li> <p>Formatting and Broadcasting: This allows formatting multiple prompts on multiple completions, controlling which prompt gets which input through broadcasting rules.</p> </li> <li> <p>Activation Function: Most torch layers end with an activation function. Similarly, <code>TextModules</code> end in an activation of an LLM call.</p> </li> </ul> <p>In this example, we will create a <code>TextModule</code> that ends in a call to an OpenAI model. This module can now execute both tasks in parallel on as many inputs as we'd like:</p> <pre><code>tasks_module = TextModule(prompt_tensor, activation=\"gpt-3.5-turbo\")\n\ninput_completions = TextTensor([{\"input_field\": \"contact@langtorch.org\"}, {\"input_field\": \"https://langtorch.org\" }])\n\n# The first row of the output are answers to \"Is this an email address?\", second to \"Is this a valid web link?\"\n\n# Columns are the two input completions\n\nprint(tasks_module(input_completions))\n</code></pre> Output:<pre><code>[[Yes   No ]\n [No    Yes]]\n</code></pre>"}, {"location": "quickstart/#comparison-with-the-openai-package", "title": "Comparison with the OpenAI Package", "text": "<p>The <code>TextModule</code> above both formats the prompts and sends them to the OpenAI activation (<code>langtorch.OpenAI</code>). Let's compare LangTorch to the OpenAI package.</p> <p>First, we'll separate the formatting and API steps.</p> <p>A core feature of <code>TextTensors</code> is that they allow us to easily format several prompts on several inputs.</p> <p>LangTorch achieves this by defining the product of two <code>TextTensors</code>: <code>text1*text2</code> as an operation akin to <code>text1.format(**text2)</code>. As shown below this is what happens in a <code>TextModule</code> before adding an activation:</p> <pre><code># Using TextModule  \ntasks_module = TextModule(prompt_tensor)  \nprompts = tasks_module(input_completions)  \n\n# Equivalently, using \"TextTensor multiplication\"  \nprompts = prompt_tensor*input_completions  \nprint(prompts)\n</code></pre> Output:<pre><code>[[Is this an email address? contact@langtorch.org (Answer 'Yes' or 'No')   Is this an email address? https://langtorch.org (Answer 'Yes' or 'No')]\n [Is this a valid web link? contact@langtorch.org (Answer 'Yes' or 'No')   Is this a valid web link? https://langtorch.org (Answer 'Yes' or 'No')]]\n</code></pre> <p>The code above introduces the multiplication operation (used in TextModules), which acts like a more powerful format operation and allows for the various features of TextTensors. For a more in depth look, see TextTensor Multiplication.</p> <p>We can send the formatted prompts to the OpenAI API by creating a <code>langtorch.OpenAI</code> module (the \"activation\") and compare speed between three API use cases:</p> <pre><code>import openai  \nimport langtorch  \nimport time  \n\nlangtorch_api = langtorch.OpenAI(\"gpt-3.5-turbo\", system_message=\"You are a helpful assistant.\", max_token=1, T=0.)  \nopenai_api = openai.OpenAI()  \n\n# Open AI package  \nstart = time.time()  \nresponses = []  \nfor prompt in prompts.flat:  \n    responses.append(openai_api.chat.completions.create(  \n        model=\"gpt-3.5-turbo\",  \n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n                  {\"role\": \"user\", \"content\": prompt}],  \n        max_tokens=1,  \n        temperature=0.  \n    ))  \nprint(f\"1.\\n{str(responses)[:125]}...\")  \nprint(f\" OpenAI loop time taken: {time.time() - start:.2f} seconds\")  \n\n# LangTorch  \nstart = time.time()  \nresponses = langtorch_api(prompts)  \nprint(f\"2.\\n{responses}\")  \nprint(f\" LangTorch time taken: {time.time() - start:.2f} seconds\")  \n\n# LangTorch on repeated requests  \nstart = time.time()  \nresponses = langtorch_api(prompts)  \nprint(f\"3.\\n{responses}\")  \nprint(f\" LangTorch on repeated requests time taken: {time.time() - start:.2f} seconds\")\n</code></pre> Output:<pre><code>1.\n[ChatCompletion(id='chatcmpl-9WkJfJJ8jPGXivDlQfPT35UZS5q2K', choices=[Choice(finish_reason='length', index=0, logprobs=None, ...\n OpenAI loop time taken: 2.18 seconds\n2.\n[[Yes   No ]\n [No    Yes]]\n LangTorch time taken: 0.84 seconds\n3.\n[[Yes   No ]\n [No    Yes]]\n LangTorch on repeated requests time taken: 0.05 seconds\n</code></pre> <p>The OpenAI Activation in LangTorch (<code>langtorch.OpenAI</code>) isn't just a wrapper around the OpenAI package. The observed speed up comes from the fact that the LangTorch implementation:</p> <ul> <li>Sends API calls in parallel, allowing multiple completions to be generated much faster than calling the OpenAI chat completion endpoint in sequence.</li> <li>Saves on tokens and speeds up subsequent calls by caching API results, especially for embeddings and when the temperature is set to zero.</li> <li>Optimizes requested API calls removing duplicates</li> </ul> <p><code>langtorch.OpenAI</code> also:</p> <ul> <li>Operates directly on <code>TextTensors</code>, returning calls in the same shape as the input.</li> <li>Handles API errors and retries by default</li> </ul>"}, {"location": "quickstart/#2-implementing-popular-methods", "title": "2. Implementing popular methods", "text": ""}, {"location": "quickstart/#chained-calls-and-simple-zero-shot-chain-of-thought", "title": "Chained Calls and Simple Zero-Shot Chain of Thought", "text": "<p>LangTorch integrates seamlessly with torch, allowing you to easily chain <code>TextModules</code> using <code>torch.nn.Sequential</code>. This can be used to chain multiple LLM calls or additional prompting methods. A simplified example is a zero-shot Chain of Thought, for which we can create a reusable <code>TextModule</code>:</p> <pre><code>CoT = TextModule(\"{*} Let's think step by step.\")\n</code></pre> <p><code>{}</code> in a prompt template is a positional argument, taking one input argument in each entry. For our chain of thought module we use the placeholder <code>{*}</code>, which is a \"wildcard\" key that places all the input entries in its place.</p> <p>Now to chain these <code>torch.nn.Sequential</code>:</p> <pre><code>import torch  \ncalculate = TextModule(\"Calculate the following: {} = ?\")  \n\ncalculate_w_CoT = torch.nn.Sequential(  \n    calculate,  \n    CoT,  \n    langtorch.OpenAI(\"gpt-3.5-turbo\") ,  \n    # You can add sequential calls here  \n)  \n\ninput_tensor = TextTensor([\"170*32\", \"123*45/10\", \"2**10*5\"])  \noutput_tensor = calculate_w_CoT(input_tensor)  \noutput_tensor.view(1,-1) # We use torch methods to reshape TextTensors and view entries in columns\n</code></pre> Output:<pre><code>[[To calculate 170 multiplied by 32, we can use\u21b5   Step 1: Multiply 123 by 45         First, calculate 2**10 (2 to the power of 10):\n   long multiplication:                            123 * 45 = 5535                    2**10 = 1024                                   \n\n       170                                         Step 2: Divide the result by 10    Next, multiply the result by 5:                \n  x     32                                         5535 / 10 = 553.5                  1024 * 5 = 5120                                \n  _________                                                                                                                          \n       340   (170 * 2)                             Therefore, 123 * 45 / 10 = 553.5   Therefore, 2**10*5 = 5120.                     \n  +  5100   (170 * 30)                                                                                                               \n  _________                                                                                                                          \n      5440                                                                                                                           \n\n  Therefore, 170 multiplied by 32 equals 5440.                                                                                      ]]\n</code></pre>"}, {"location": "quickstart/#ensemble-self-consistency", "title": "Ensemble / Self-Consistency", "text": "<p>Representing texts geometrically in a matrix or tensor allows for creating meaningful structures. Methods like ensemble voting and self-consistency involve generating multiple completions for the same task, easily represented by adding a dimension.</p> <p>In this example, we build a module that creates multiple Chain-of-Thought answers for each input. These create separate <code>TextTensor</code> entries that we combine using a \"linear layer\" to marginalize over them, improving overall performance (see Wang et al., 2022).</p> <pre><code>calculate = TextModule(\"Calculate the following: {} = ? Let's think step by step.\")  \n\nensemble_llm = langtorch.OpenAI(\"gpt-3.5-turbo\",T=1.4,n = 3) # 3 completions per input with high temperature  \n\ncombine_answers = langtorch.Linear([[ f\"\\nAnswer {i}: \" for i in [1,2,3] ]]) # Here we use properties of matrix multiplication:  \n# Linear uses matmul, where row_of_labels @ column_of_completions == one long entry with labeled completions  \n\nchose = TextModule(\"Select from these reasoning paths the most consistent final answer: {}\")  \n\nllm = langtorch.OpenAI(\"gpt-3.5-turbo\", T=0)  \n\nself_consistent_calculate = torch.nn.Sequential(  \n    calculate,  \n    ensemble_llm,  \n    combine_answers,  \n    chose,  \n    llm  \n)\n</code></pre> <pre><code>input_tensor = TextTensor(\"171*33\")\n\nprint(self_consistent_calculate(input_tensor))\n</code></pre> Output:<pre><code>[The most consistent final answer is: 171 * 33 = 5643. This is the answer provided in both Answer 2 and Answer 3, which break down the multi\n  plication process into steps and arrive at the same final result.                                                                           ]\n</code></pre> <p>Saving results from repeating these calls, let's us see accuracy increasing from 25% (using <code>calculate</code>) to well over 50% (using <code>self_consistent_calculate</code>) on this input.</p>"}, {"location": "quickstart/#3-automatic-texttensor-embeddings-for-building-retrievers", "title": "3. Automatic TextTensor Embeddings for Building Retrievers", "text": "<p><code>TextTensors</code> offer a straightforward way to work with embeddings. Every <code>TextTensor</code> can generate its own embeddings -- held in a torch tensor that preserves their shape. Moreover, <code>TextTensors</code> automatically act as their embeddings when passed to torch functions like cosine similarity.</p> <p>These representations (available under the <code>.embedding</code> attribute) are created automatically right before they are needed, using a set embedding model (default is OpenAI's <code>text-embedding-3-small</code>).</p> <pre><code>import torch  \n\ntensor1 = TextTensor([[[\"Yes\"],  \n                       [\"No\"]]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1, tensor2)\n</code></pre> Output:<pre><code>tensor([[[0.6923, 0.6644, 0.6318, 0.5749],\n         [0.5458, 0.7727, 0.5386, 0.7036]]])\n</code></pre> <p>We can access the embedding tensor under <code>.embedding</code>, change the embedding model and embed using <code>.embed()</code>:</p> <pre><code># To change embedding model and embed\ntensor1.embedding_model = \"text-embedding-3-large\"\ntensor1.embed()\n# To access the embedding tensor\ntensor1.embedding\n</code></pre> Output:<pre><code>tensor([[[[-0.0338,  0.0298, -0.0105,  ..., -0.0194, -0.0076,  0.0153]],\n         [[-0.0281,  0.0073, -0.0121,  ..., -0.0071,  0.0094,  0.0090]]]])\n</code></pre>"}, {"location": "quickstart/#working-with-embeddings-and-documents-parsing-chunking-and-indexing", "title": "Working with embeddings and documents (parsing, chunking and indexing)", "text": "<p>To enable its functionalities <code>TextTensor</code> entries aren't just strings, but structured <code>Text</code> objects, which can be created from f-string templates, dictionaries and markup documents and are represented by a sequence of <code>(label, text)</code> pairs.</p> <p>For the next task we need chunked text data. We can use the above fact to conveniently manipulate markdown files -- in this example, a paper on the abilities of language models. Download the markdown file from here:</p> <pre><code>&gt; wget https://raw.githubusercontent.com/adamsobieszek/langtorch/main/src/langtorch/conf/paper.md\n</code></pre> <p>We can create a tensor with each markdown block in a separate entry simply with:</p> <pre><code>paper = TextTensor.from_file(\"paper.md\")\n\nprint(paper[:3],\"\\n (...)\")\nprint(f\"shape = {paper.shape}\")\n</code></pre> Output:<pre><code>[# Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models\n  Adam Sobieszek &amp; Tadeusz Price, 2022                                           \n  ## Abstract                                                                    ] \n  (...)\nshape = torch.Size([80])\n</code></pre> <p>As the text has headers and other text blocks, we need to extract only paragraphs. This is where text entries being structured becomes useful, as LangTorch provides <code>iloc</code> and <code>loc</code> accessors for <code>Text</code> entries and tensors:</p> <pre><code># Select paragraphs\nparagraphs = paper.loc[\"Para\"]  \n# Remove empty entries  \nparagraphs = paragraphs[paragraphs!=\"\"]  \nprint(paragraphs[:].apply(lambda x: x[:40] + \"...\"))\n</code></pre> Output:<pre><code>[This article contributes to the debate a...\n  These are questions put to Salvador Dali...\n  We take issue with some methodological u...\n  We\u2019ll show some situations in which reve...\n  In the second part of this paper, we pro...\n  In their paper, Floridi and Chiriatti pr...\n  The logic of Floridi and Chiriatti\u2019s rev...\n  The most mature theory quantifying such ...\n...\n</code></pre>"}, {"location": "quickstart/#build-custom-retriever-and-rag-modules", "title": "Build Custom Retriever and RAG modules", "text": "<p>For complex modules we can subclass <code>TextModule</code> and as in PyTorch define our own init and forward methods.</p> <p>Using how <code>TextTensors</code> can automatically act as a<code>Tensor</code> of its embeddings, we can very compactly implement e.g. a retriever, which for an input entry finds  <code>k</code> entries with the highest cosine similarity among the documents it holds:</p> <pre><code>class Retriever(TextModule):  \n    def __init__(self, documents: TextTensor):  \n        super().__init__()  \n        self.documents = TextTensor(documents).view(-1)  \n\n    def forward(self, query: TextTensor, k: int = 5):  \n        cos_sim = torch.cosine_similarity(self.documents, query)  \n        return self.documents[cos_sim.topk(k)]\n</code></pre> <pre><code>retriever = Retriever(paragraphs)  \nquery = TextTensor(\"What's the relationship between prediction and compression?\")  \nretriever(query)\n</code></pre> Output:<pre><code>[Recall how a language model during training must compress an untenable number of conditional probabilities. The only wa\n  to do this successfully is to pick up on the regularities in language (as pioneered by Shannon 1948). Why do we claim   \n  that learning to predict words, as GPT does, can be treated as compressing some information? Let\u2019s assume we\u2019ve         \n  calculated the conditional probability distribution given only the previous word of all English words. Consider, that   \n  such a language model can either be used as a (Markavion) language generator or, following Shannon, be used for an      \n  efficient compression of English texts. Continuing this duality, it has been shown, that if a language model such as GPT\n  would be perfectly trained it can be used to optimally compress any English text (using arithmetic coding on its        \n  predicted probabilities; Shmilovici et al., 2009). Thus the relationship between prediction and compression is that     \n  training a language generator is equivalent to training a compressor, and a compressor must know something about the    \n  regularities present in its domain (as formalized in AIXI theory; Mahoney 2006). To make good predictions it is not     \n  enough to compress information about what words to use to remain grammatical (to have a syntactical capacity), but also \n...  ]\n</code></pre> <p>Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower-level operations while being able to write compact code without a multitude of classes. For this reason, implementations such as the retriever above are not pre-defined classes in the main package.</p> <p>We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:</p> <pre><code>class RAG(TextModule):  \n    def __init__(self, documents: TextTensor, *args, **kwargs):  \n        super().__init__(*args, **kwargs)  \n        self.retriever = Retriever(documents)  \n\n    def forward(self, user_message: TextTensor, k: int = 5):  \n        retrieved_context = self.retriever(user_message, k) + \"\\n\"  \n        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()  \n        return super().forward(user_message)\n</code></pre> <pre><code>rag_chat = RAG(paragraphs,  \n               prompt=\"Use the context to answer the following user query: \",  \n               activation=\"gpt-3.5-turbo\")  \nassistant_response = rag_chat(query)  \nprint(assistant_response.reshape(1,1))\n</code></pre> Output:<pre><code>[[The relationship between prediction and compression is that training a language generator, such as GPT, is equivalent to training a compres\u21b5\n  sor. This is because in order to make accurate predictions, the model must compress information about the regularities present in the langu\u21b5 \n  age it is trained on. This compression of information allows for generalization, which in turn leads to the ability to operate on novel inp\u21b5 \n  uts and create novel outputs. So, prediction leads to compression, which leads to generalization, and ultimately to computer intelligence.  ]]\n</code></pre> <p>With only small modifications to the retriever, this module could also perform batched inference \u2014 performing multiple simultaneous queries without much additional latency. Note, <code>prompt</code> and <code>activation</code> are arguments inherited from TextModule and need the <code>super().forward</code> call to work.</p> <p>We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our discord. In the likely event of encountering a bug, send it on discord or post on the GitHub Repo and we will fix it ASAP.</p>"}]}