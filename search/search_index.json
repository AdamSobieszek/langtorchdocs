{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>LangTorch is a framework that simplifies development of complex language model applications by leveraging familiar PyTorch concepts.</p> <p>While existing frameworks focus on connecting language models to other services, LangTorch aims to change the way you approach building LLM applications by introducing a unified framework for working with texts, chats, markup languages, tokens, embeddings and code interpreters with seamless parallelization and PyTorch integration. The result? Less code, faster development and more intricate LLM applications.</p>"},{"location":"#the-langtorch-framework","title":"The LangTorch Framework","text":"<p>LangTorch components are designed to hold and manipulate text data in a way that is so intuitive that LLMs can write LangTorch code on their own from only a short description. In effect LangTorch lets LLMs program themselves, which opens a new realm for agentic LLM applications. To achieve this design, textual LangTorch components subclass their numerical PyTorch counterparts, which lets users apply their existing coding skills to building novel LLM app architectures.</p> <ul> <li> <p>LangTorch starts by introducing structured Text objects, that can effectible represent and manipulate prompt templates, chat histories, document stores, dictionaries, markup languages and code.  </p> </li> <li> <p>TextTensors inheriting functionalities from <code>torch.Tensor</code>, but holding as entries the structured text. TextTensors allow us to structure text information geometrically, as well as manage multi-repres the same text data it in multiple forms (automatically acting as texts when printed, as embeddings when calculating cosine similarity and as tokens when passed to a local LLM). </p> </li> <li> <p>TextModules are the compositional building blocks. A subclass of <code>torch.nn.Module</code>, TextModules can perform all operations you may need in an LLM chain e.g. formatting, parallel LLM api calls, batched local LLM inference, embeddings, information retrieval and so on.</p> </li> <li> <p>Prompt Optimization is for the first time enabled directly with autograd. LangTorch operations save a computational graph that can be traversed backward to update gradient text values or dynamically undo and repeat calls. With a dataset of examples such gradient text values can inform and optimizer on how it should update the prompts in the chain.</p> </li> </ul>"},{"location":"#what-makes-langtorch-unique","title":"What Makes LangTorch Unique?","text":"<p>Instead of many wrapper classes to memorize, LangTorch defines simple rules for <code>TextTensor</code> operations like addition and multiplication, that in turn enable all kinds of text transformations and formatting. This lets developers think about what they want to build, instead of how the classes were named. For example creating a prompt template and filling its values can both be done with just TextTensors, which can be multiplied to transform texts with values of other texts:</p> <pre><code>template = TextTensor([[\"Explain {theory} in terms of {framework}\"],  \n                       [\"Argue how {framework} can prove {theory}\"]])  \n\nresult = template * TextTensor({\"theory\": \"active inference\", \"framework\": \"thermodynamics\" })\n\nprint(result)\n# Outputs: [[Explain active inference in terms of thermodynamics]\n#           [Argue how thermodynamics can prove active inference]]\n</code></pre> <p>As <code>TextModules</code> operate on tensor data, all operations are easily parallelizable and integrate seamlessly with PyTorch, Transformers Models and APIs. Chaining TextModules offers both low-level control and a higher level of architectural abstraction.</p> <p>If you have used PyTorch you can use this knowledge when writting LangTorch where prompt engineering, templating and LLM calls all work when used with torch constructs like <code>nn.Sequential</code>. Here is a complete example LangTorch module, that can be executed in parallel on any text input, while preserving shape and maintaining a computational graph for a backward pass. </p> <pre><code>chain = torch.nn.Sequential(\n    TextModule(\"Calculate this equation:\\n\"),\n    langtorch.methods.CoT,\n    GPT4(),\n    TextModule(\"Is this reasoning correct?\\n\"),\n    GPT4(T=0.)\n)\n\noutput = chain(TextTensor([[\"170+32 =\", \"8+3+2000 =\" ], \n                           [\"170*32 =\", \"8**3*2000 =\"]])\n)\nprint(output.shape) # Outputs: (2,2)\n</code></pre> <ul> <li> <p>Deep Integration: Each retriever, chain, chat or agent, can be implemented in LangTorch using PyTorch design patterns, such as classes with a forward method, that integrate both tensor data (e.g. embeddings) and textual data (e.g. prompt templates).</p> </li> <li> <p>Caching Results and Maintaining Context: When running your LangTorch app you can use a persistent context. This let's you cache the results of LLM calls and store all TextTensors and pending API requests in a yaml session file. This saves a lot of costs, allows the state to persist between runs, makes apps robust to crushes and creates a persistent context that can be accessed from anywhere in the code to inject content from your file system.</p> </li> </ul>"},{"location":"#dive-in-and-get-started","title":"Dive In and Get Started","text":"<p>Install LangTorch with: <pre><code>pip install langtorch\n</code></pre></p> <p>Whether you're just starting out or are an experienced developer, LangTorch has tools and functionalities to meet your needs. Our documentation provides a Quick-Start Tutorial to help you craft your first application swiftly. Once you're familiar with the basics, delve deeper with Conceptual guides and our Documentation Reference. </p> <p>For hands-on examples, visit End-to-End Tutorials or compare End-to-End implementations in LangTorch vs LangChain Examples.</p> <p>Join us in revolutionizing the future of LLM application development. Welcome to the LangTorch era.</p>"},{"location":"Todo/","title":"Todo","text":"<ul> <li>[ ] Dodaj jako pierwsze feature text being structured. Explain it in terms of defining operations on structured text in structured text.</li> <li>[ ] sekcja integrations: x pytorch x transformers x openai maybe</li> <li>[ ] Przypisa\u0107 intro z klas na na features (semantic algebra, )</li> <li>[ ] goals of the package: create a </li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide: Dive into LangTorch","text":"<p>This Guide shows how to quickly start building with LangTorch. LangTorch works with:</p> <ul> <li>Python 3.8 or higher</li> <li>PyTorch 2.0.0 or higher</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install LangTorch using pip:</p> <pre><code>pip install langtorch\n</code></pre>"},{"location":"quickstart/#starting-a-project","title":"Starting a project","text":""},{"location":"quickstart/#texttensors","title":"TextTensors","text":"<p>TextTensors in LangTorch are designed to simplify the handling of textual data. They inherit from PyTorch's <code>torch.Tensor</code> but are specialized to hold textual data. </p>"},{"location":"quickstart/#creating-a-texttensor","title":"Creating a TextTensor","text":"<pre><code>from langtorch import TextTensor\n\n# Initialize with a list of strings\ntt = TextTensor([[\"Hello, world!\"], [\"How are you?\"]])\n</code></pre>"},{"location":"quickstart/#texttensor-operations","title":"TextTensor Operations","text":"<p>Addition concatenates the text, while multiplication performs template formatting:</p> <pre><code># Concatenation\nnew_tt = tt + \" Have a great day!\"\n\n# Template formatting\nformatted_tt = TextTensor([\"{greeting}, {name}!\"]) * TextTensor({\"greeting\": \"Hello\", \"name\": \"Alice\"})\n</code></pre>"},{"location":"quickstart/#text-keys","title":"Text Keys","text":"<p>You can explicitly assign or change keys in a TextTensor using the <code>set_key()</code> method:</p> <pre><code># Assign new keys\ntt_new_keys = tt.set_key(\"new_key\")\n</code></pre>"},{"location":"quickstart/#text-the-atomic-unit-of-texttensor","title":"Text: The Atomic Unit of TextTensor","text":"<p>LangTorch introduces a <code>Text</code> class that acts like an ordered dictionary but with the functionality of a string. You can initialize it with key-value pairs and perform operations like regular strings.</p> <pre><code>from langtorch import Text\n\n# Create a Text object\ntext_obj = Text((\"greeting\", \"Hello\"), (\"object\", \"world\"))\n</code></pre> <p>You can use the <code>Text</code> class to create complex TextTensors, and then apply the <code>set_key()</code> function to change keys if needed.</p>"},{"location":"quickstart/#textmodules-the-operators","title":"TextModules: The Operators","text":"<p>TextModules are akin to PyTorch's <code>nn.Module</code> but are customized to operate on TextTensors. They multiply the input TextTensor with internal content and pass it to an \"activation function,\" which in this context is an LLM call.</p> <pre><code>from langtorch import TextModule, ChatGPT\n\nllm = ChatGPT(\"gpt4\")\n# Initialize a TextModule\ntext_mod = TextModule(\"Translate this text: {}\", activation=llm)\n</code></pre>"},{"location":"quickstart/#chaining-textmodules","title":"Chaining TextModules","text":"<p>LangTorch enables you to create complex pipelines, similar to how you'd use <code>nn.Sequential</code> in PyTorch.</p> <pre><code>import torch\n\n# Create a pipeline\npipeline = torch.nn.Sequential(\n    TextModule(\"Translate this text: {}\"),\n    TextModule(\"Summarize the translated text: {}\", activation=SomeSummarizationLLM)\n)\n</code></pre>"},{"location":"quickstart/#running-your-pipeline","title":"Running Your Pipeline","text":"<p>Execute the pipeline by simply passing your TextTensor:</p> <pre><code>output = pipeline(tt)\n</code></pre>"},{"location":"quickstart/#advanced-tips","title":"Advanced Tips","text":"<ul> <li>Cosine Similarities: LangTorch provides a <code>CosineSimilarity</code> class to compute similarities between TextTensors.</li> </ul> <pre><code>from langtorch.tt import CosineSimilarity\n\ncos = CosineSimilarity()\nsimilarities = cos(tt, TextTensor([\"1\", \"0\", \"No\", \"Yes\"]))\n</code></pre> <ul> <li> <p>Temperature Settings: For creative tasks, set a high temperature (e.g., 1.2), and for more analytical tasks, a lower temperature (e.g., 0.5).</p> </li> <li> <p>Key Management: Always make sure the TextTensors have the correct keys that the TextModule templates expect. Utilize the <code>set_key()</code> method and the <code>key</code> argument in <code>TextModule</code> for this.</p> </li> </ul>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations, you've completed the Quickstart Guide! For a deeper understanding, explore our Conceptual Guides and API Documentation.</p> <p>Feel free to contribute; check our contribution guidelines.</p> <p>Happy LangTorching! \ud83c\udf89</p>"},{"location":"guides/text/","title":"1. Text Multiplication Guide","text":""},{"location":"guides/text/#what-is-a-text","title":"What is a Text?","text":"<p>The basic object in LangTorch is <code>langtorch.TextTensor</code>, corresponding to <code>torch.nn.Tensor</code>. While <code>nn.Tensor</code> is a tensor of numbers, the <code>TextTensor</code> is a tensor of <code>Text</code> objects. Text objects are not simply strings to allow them to represent more complex objects and undergo more complex operations (e.g. multiplication, which will be a composition operation for Text objects that acts similar to the string .format method).  which are defined by a tuple <code>(content, key)</code> where content and key are regular strings.</p>"},{"location":"guides/text/#1-basics","title":"1. Basics:","text":""},{"location":"guides/text/#text","title":"Text:","text":"<ul> <li>Represents a sequence of \"named strings\". It acts as a regular string instance, but provides additional methods to manipulate the named strings within.</li> <li>The class provides multiple ways of creating instances, either through direct invocation or using patterns. That will be in the next chapter.</li> <li>The <code>Text</code> class can be initialized in multiple ways. This section focuses on constructing Text from strings, lists and dictionaries. A more convenient way is to use a special f-string-like sytnax which can parse most Text\u2019s from a single string. This is explained in the next chapter.</li> </ul> <pre><code># Creating a Text object with (key,  value) syntax:\ntext_obj = Text((\"greeting\", \"Hello\"), (\"object\", \"world\"))\nprompt = TextTensor([\"{object} says: {greating}\"}])\nprint(text_obj)  # Outputs: Helloworld\nprint(prompt*text_obj)  # Outputs: world says: Hello\n\n# Tho get substrings of the Text, the content attribute holds sub-Texts:\nprint(text_obj.content)  # Outputs: [Text((\"greeting\", \"Hello\")), Text((\"object\", \"world\"))]\n\n# Accessing the items (key-value pairs) of a Text object:\nitems = text_obj.items()\nprint(items)  # Outputs: [(\"greeting\", \"Hello\"), (\"object\", \"world\")]\n</code></pre>"},{"location":"guides/text/#2-operations-on-text","title":"2. Operations on Text:","text":""},{"location":"guides/text/#addition-operation","title":"Addition Operation:","text":"<p>The addition operation on <code>Text</code> and <code>TextTensors</code> simply concatenates them.</p> <pre><code># Concatenating two Text objects:\ntext1 = Text((\"greeting\", \"Hello\"))\ntext2 = Text((\"object\", \"world\"), \"!\")\nresult_text = text1 + text2\nprint(result_text)  # Outputs: Helloworld!\n\n# Mixing str2 and Text:\nmix_result = str1 + text2\nprint(mix_result)  # Outputs: Helloworld!\n</code></pre>"},{"location":"guides/text/#multiplication-operation","title":"Multiplication Operation:","text":"<p>The heart of this system is the multiplication operation, defined in the <code>__mul__</code> method of both classes. Here's how it works:</p>"},{"location":"guides/text/#single-key-text-multiplication","title":"Single-key Text Multiplication:","text":"<ul> <li> <p><code>str2 * str2</code>: When two Text objects are multiplied:</p> <ul> <li>If their keys match but not their contents, the contents get concatenated.</li> <li>If one is the inverse of the other (content and key swapped), the identity (an empty string) is returned.</li> <li> <p>If the content of the first <code>str2</code> matches the key of the second it acts as a format operation. For example:</p> <pre><code> Text((\"\",\"key\")) * Text(\"key\",\"value\") == Text((\"\", \"value\"))\n</code></pre> <p>You can interpret the whole operation like a multiplication of rational numbers:</p> <p>$$</p> <p>\\frac{\\text{'It works'}}{\\text{'a key'}}\\circ\\frac{\\text{ ' like this'}}{\\text{'a key'}} = \\frac{\\text{'It works like this'}}{\\text{'a key'}}\\ \\text{and}\\</p> <p>\\frac{\\text{'a phrase'}}{\\text{''}}\\circ\\frac{\\text{ 'text'}}{\\text{'a phrase'}} = \\frac{\\text{'text'}}{\\text{''}} $$</p> </li> <li> <p>For different keys, a new <code>Text</code> object is created with one <code>str2</code> instance after the other.</p> </li> <li><code>str2 * str</code>: If a <code>str2</code> object is multiplied with a regular string:</li> <li>If the <code>str2</code> doesn't have a key, the regular string is simply concatenated to its content.</li> <li>Otherwise, a new <code>Text</code> object is created with the <code>str2</code> followed by the regular string.</li> </ul> </li> </ul>"},{"location":"guides/text/#longer-text-multiplication","title":"Longer Text Multiplication:","text":"<ul> <li><code>Text * str2</code>: The str2 goes from left to right and if it finds a match for any of the rules above it applies it to that substring of Text, if none such match is found it appends itself to the Text</li> <li><code>Text * Text</code>: similar but for every in the right Text, it becomes clearer when we introduce creating Texts from strings</li> </ul>"},{"location":"guides/text/#inverse-operation","title":"Inverse Operation:","text":"<p>The inverse of a Text is obtained by swapping the key and content. To get the inverse of a Text you can either use the <code>inv()</code> method or the power operation <code>**-1</code>. The Text multiplied by its inverse is the empty string (<code>Text.identity</code>), as shown below.</p> <pre><code># For Text:\ntext_obj = Text((\"Hello\", \"greeting\"), (\"world\", \"object\"))\n# text_obj.inv() == text_obj**-1 == Text((\"greeting\", \"Hello\"), \n#                                                                                (\"object\", \"world\"))\n\n# We have:\n# text_obj*(text_obj**-1) == Text.identity == \"\"\n</code></pre>"},{"location":"guides/text/#4-additional-points","title":"4. Additional Points:","text":""},{"location":"guides/langchain_comparison/translation/","title":"Translation","text":"LangChain LangTorch ChatPromptTemplate.from_template(prompt) TextTensor(prompt) RunnablePassthrough() TextModule(\"\") StrOutputParser itemgetter(key)"},{"location":"reference/text/","title":"Text","text":"<p>...</p>"},{"location":"reference/text/#reference","title":"Reference","text":"<p>             Bases: <code>str</code></p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>class Text(str):\n    parser = TextParser\n    allowed_keys = None\n\n    @classmethod\n    def parse(cls, arg):\n        parsed_result = cls.parser.parseString(arg)\n        arg = [(res.key if \"key\" in res else \"\", res.value if \"value\" in res else \"\") if isinstance(res,\n                                                                                                    ParseResults) else res\n               for res in parsed_result]\n        return arg\n\n\n    def __new__(cls, *args, parse = \"auto\", **kwargs):\n        \"\"\"\n        Construct a new Text instance. Allows for various input formats.\n\n        Args:\n            *args: Flexible input data. Can be a parsable string, string sequences, key-value pairs, dicts...\n                                    If None is passed, it will be replaced with a Text instance with empty content.\n            parse (Union[bool, str], optional): Enable or disable the automatic parsing of string content.\n                                    The default behavior is to automatically detect language and parse it only\n                                    if the texts is written in the default langtorch syntax. Set to True to\n                                    also enable parsing of the detected languages using pandoc.\n            **kwargs: Additional named textual data entries.\n\n        Returns:\n            Text: A structured textual instance.\n\n        Raises:\n            ParseException: If automatic parsing fails. Consider disabling parsing if this occurs.\n            ValueError: When an unsupported input format is provided e.g. a TextTensor is passed.\n        \"\"\"\n        if len(args) == 0 or (len(args) == 1 and args[0] is None):\n            instance = super().__new__(cls, \"\")\n            instance._content = tuple()\n            return instance\n        content = [c if c is not None else cls.identity for c in (list(args) + list(kwargs.items()))]\n        # cast TextTensors to strings\n        for i in range(len(content)):\n            if isinstance(content[i], torch.Tensor) and hasattr(content[i], \"content\"):\n                content[i] = content[i].sum().item()\n                assert isinstance(content[i], str) or (0 &lt; len(content[i]) &lt;= 2)\n        # returns the final content tuple\n        if len(content) == 1 and cls.is_valid_tree(content, is_tuple=True):\n            content = content[0]\n        else:\n            content = cls.to_ast(*content, parse = parse, is_tuple=True)\n\n        assert cls.is_valid_tree(content, is_tuple=True), f\"Invalid tree: {content}\"\n\n        instance = super().__new__(cls, cls.concatenate_terminals(content))\n        instance._content = content\n        return instance\n\n    @classmethod\n    def to_ast(cls, *args, parse = True, is_tuple = False):\n        \"\"\"Reformats a wide array of construtor patterns into a unified AST-like format\"\"\"\n        if len(args) == 1 and isinstance(args[0], list):\n            args = args[0]\n        if len(args) == 1 and ((isinstance(args[0], tuple) and len(args[0]) &gt; 2) or isinstance(args[0], list)):\n            # List or tuple of strings / named strings\n            args = args[0]\n        elif isinstance(args[0], dict):\n            # Dictionary of named strings\n            args = args[0].items()\n        elif isinstance(args[0], Text) and len(args) == 1:\n            # Passing cls instance to itself\n            args = args[0].content\n        elif all([isinstance(arg, str) for arg in args]):\n            if parse:\n                try:\n                    result = []\n                    for arg in args:\n                        arg = cls.parse(arg)\n                        result += arg\n                    args = result\n                except ParseException as E:\n                    print(f\"Last parsed string: {arg}\")\n                    raise ParseException(str(E) + \"\\nYou may want to disable string parsing, with parse = False\")\n            else:\n                pass\n        if any([isinstance(arg, torch.Tensor) for arg in args]):\n            raise ValueError(\"You cannot initialise Text from a TextTensor. Use tensors.item() or otherwise transform the tensors to a string, list or dictionary.\")\n        def simplify(arg, parse = False):\n            if isinstance(arg, tuple) and len(arg) == 2 and not (isinstance(arg[0], str) and isinstance(arg[1], str)):\n                # Fix tuple types\n                return (str(arg[0]), simplify(arg[1]))\n            elif isinstance(arg, tuple) and len(arg) == 2 and isinstance(arg[0], str) and isinstance(arg[1], str):\n                # CORRECT\n                return arg\n            elif isinstance(arg, tuple) and len(arg) == 1:\n                # CORRECT though should be avoided\n                return arg[0]\n            elif isinstance(arg, tuple) and len(arg) &gt; 2:\n                # Assume a tuple of length != 2 was supposed to be a list\n                logging.debug(f\"Tuples of length 2 represent (key, value) in Text objects. When parsing a Text entry was a tuple of length {len(arg)},\\nit was converted to a list and may lead to errors.\")\n                arg = list(arg)\n\n            # Not a named string\n            if isinstance(arg, list) and len(arg) == 1:\n                return arg[0]\n            elif isinstance(arg, list):\n                return [simplify(element, parse=parse) for element in arg]\n            elif isinstance(arg, dict):\n                return Text.to_ast(list(arg.items()), parse=parse)\n            elif isinstance(arg, Text):\n                if len(arg.items()) == 1:\n                    return Text.to_ast(arg.items()[0], parse=parse)\n                else:\n                    return Text.to_ast(arg.items(), parse=parse)\n            elif isinstance(arg, str):\n                # CORRECT\n                return arg\n            raise ParseException(f\"Could not parse {arg} of type {type(arg)} and len {len(arg)}\")\n\n        content = [simplify(arg, parse=parse) for arg in args]\n\n        if not is_tuple: # Recursive case: In these cases we are returning a node or tree with a single root node\n            return content[0] if isinstance(content, list) and len(content) == 1  else content\n        else: # Base case: In these cases we are returning a tuple of nodes for the ._content attribute\n            def check_for_lists(tree): # A basic check for lists in the tree\n                if isinstance(tree, list):\n                    # Prune single-element lists\n                    if len(tree) == 1:\n                        raise ValueError(f\"single-element list {tree}\")\n                    else:\n                        return [check_for_lists(element) for element in tree]\n                elif isinstance(tree, tuple) and len(tree) == 2:\n                    key, value = tree\n                    return key, check_for_lists(value)\n                return tree\n\n            content = tuple(check_for_lists(arg) for arg in content)\n\n            return content\n\n    @classmethod\n    def is_terminal_node(cls, entry):\n        if isinstance(entry, str) and not isinstance(entry, Text):\n            return True\n        if isinstance(entry, tuple) and len(entry) == 2 and isinstance(entry[0], str) and isinstance(entry[1], str):\n            return True\n        return False\n\n    @classmethod\n    def is_valid_tree(cls, entry, is_tuple = False):\n        \"\"\"\n        Checks if an entry is a valid tree for a Text instance.\n\n        \"\"\"\n        if isinstance(entry, cls):\n            entry = entry.items()\n        elif isinstance(entry, tuple) and is_tuple:\n            if len(entry) == 0:\n                return True\n            entry = list(entry) if len(entry)&gt;1 else entry[0]\n        # Base cases\n        if cls.is_terminal_node(entry):\n            return True\n\n        # Recursive cases\n        if isinstance(entry, tuple) and len(entry) != 2:\n            return False\n        if isinstance(entry, tuple) and len(entry) == 2 and isinstance(entry[0], str) and (\n                isinstance(entry[1], list) or isinstance(entry[1], tuple)):\n            return all(cls.is_valid_tree(child) for child in entry[1])\n        if isinstance(entry, list) and len(entry)&gt;1:\n            return all(cls.is_valid_tree(child) for child in entry)\n        if isinstance(entry, list) and len(entry)&lt;=1:\n            return False  # Single-element lists are not valid\n\n        # If none of the above cases match, it's not valid\n        return False\n\n    @classmethod\n    def _str_formatter(cls, instance) -&gt; str:\n        \"\"\"\n        Formats the human-readable string of a Text instance. Subclasses of Text can reimplement this method!\n\n        Args:\n            instance (Text): An instance of the Text class.\n\n        Returns:\n            str: A string representation of the instance.\n        \"\"\"\n        return cls.concatenate_terminals(instance.items())\n\n    @classmethod\n    def concatenate_terminals(cls, entry) -&gt; str:\n        result = \"\"\n\n        # Base cases\n        if cls.is_terminal_node(entry):\n            return entry if isinstance(entry, str) else entry[1]\n\n        # Recursive cases\n        if isinstance(entry, tuple) and len(entry) == 2 and isinstance(entry[0], str) and (\n                isinstance(entry[1], list) or isinstance(entry[1], tuple)):\n            return ''.join(cls.concatenate_terminals(child) for child in entry[1])\n        if isinstance(entry, list):\n            return ''.join(cls.concatenate_terminals(child) for child in entry)\n\n        return result\n\n    def __str__(self):\n        return self.__class__._str_formatter(self)\n\n    @classmethod\n    def from_messages(cls, *messages):\n        \"\"\"Text from a list of dicts with keys 'role' and 'content'\"\"\"\n        if isinstance(messages[0], list):\n            messages = messages[0]\n        text = []\n        for m in messages:\n            if isinstance(m, list):\n                for mm in m:\n                    text.append(mm[\"role\"],m[\"content\"])\n            else:\n                text.append((m[\"role\"],m[\"content\"]))\n        return cls(*text)\n\n    @classmethod\n    def from_api_response(cls, *response_dicts):\n        \"\"\"Text from a list of dicts with keys 'role' and 'content'\"\"\"\n        if isinstance(messages[0], list):\n            messages = messages[0]\n        text = []\n        for m in messages:\n            if isinstance(m, list):\n                for mm in m:\n                    text.append(mm[\"role\"],m[\"content\"])\n            else:\n                text.append((m[\"role\"],m[\"content\"]))\n        return cls(*text)\n\n\n    def item_print(self):\n        \"\"\"\n        Prints the Text instance with keys aligned over their corresponding values.\n        \"\"\"\n        # Extract the key-value pairs from the _content attribute\n        key_value_pairs = [pair  if isinstance(pair, tuple) else ('', pair) for pair in self.items()]\n\n        # Find the longest key and value for formatting purposes\n        longest_key = max(len(key) for key, _ in key_value_pairs)\n        longest_value = max(len(value) for _, value in key_value_pairs)\n\n        # Create the top and bottom lines for the keys and values\n        top_line = \"\"\n        bottom_line = \"\"\n        for key, value in key_value_pairs:\n            # Calculate padding to center the key\n            total_padding = longest_key - len(key)\n            left_padding = total_padding // 2\n            right_padding = total_padding - left_padding\n\n            # Add the formatted key and value to the respective lines\n            top_line += f\"{' ' * left_padding}{key}{' ' * right_padding}|\"\n            bottom_line += f\"{value.ljust(longest_value)}|\"\n\n        # Remove the trailing '|' character\n        top_line = top_line.rstrip('|')\n        bottom_line = bottom_line.rstrip('|')\n\n        # Print the formatted lines\n        print(top_line)\n        print(bottom_line)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def to_tensor(self):\n        from langtorch import TextTensor\n        return TextTensor(self.content)\n\n    @property\n    def identity(self):\n        return self.__class__()\n\n    @property\n    def content(self):\n        return [self.__class__(m, parse = False) for m in self._content]\n\n    @content.setter\n    def content(self, content):\n        if isinstance(content, tuple) and len(content) == 2 and (isinstance(content[0], str) and isinstance(content[1], str)):\n            AttributeError(\n            f\"When setting the .content attribute, passing a tuple of strings is ambiguous. Pass either a list with a (key, value) tuple or a list with two strings.\"\n            )\n\n        if isinstance(content, list):\n            content = tuple(content)\n        assert isinstance(content, tuple)\n        self._content = self.to_ast(content, parse=False, is_tuple=True)\n\n    def items(self):\n        \"\"\"\n        Retrieves key-value pairs from the Text object, allowing for structured data extraction\n        and further processing.\n\n        Returns:\n            List[Tuple[str, Union[str, Tuple[...]]]]: A list of key-value pairs representing the Text's content.\n        \"\"\"\n        return [(arg[0], arg[1]) if isinstance(arg, tuple) else ('', arg) for arg in self._content]\n        # return [(arg[0], str(self.__class__(arg[1])))) if isinstance(arg, tuple) else ('', str(self.__class__(arg))) for arg in self._content]\n\n    def keys(self):\n        return [s[0] for s in self.items()]\n\n    def values(self):\n        return [s[1] for s in self.items()]\n\n    def set_key(self, key, inplace = False):\n        \"\"\"\n        Override keys for the textual entries, used for restructuring the content.\n        Useful for substituting the key right before passing TextTensor to a Module.\n\n        Args:\n            key (Union[Text, str, List[str]]): The new key or keys to apply\n            inplace bool: .\n\n        Returns:\n            Text: A new Text instance with updated keys.\n        \"\"\"\n        #The use of Text._str_formatter(t) instead of str(t) here and elsewhere is for subclasses of Text to reimplement __str__\n        if isinstance(key, Text):\n            key = key.values()\n            if len(key) == 1:\n                key = key[0]\n\n        if isinstance(key, list):\n            assert len(key) == len(self.values()), f\"Number of keys ({len(key)}) must match number of values ({len(self.values())})\"\n            content = tuple((k,v) for k,v in zip(key, self.values()))\n        elif isinstance(key, str):\n            content = ((key, Text._str_formatter(self)),)\n        if inplace:\n            self.content = content\n            return self\n        else:\n            return self.__class__(*content)\n\n    def set_key_(self, keys):\n        self.set_key(keys, inplace=True)\n\n    def add_key(self, key, inplace=False):\n        \"\"\"\n        Add a top-level  key, placing the items of the original as a value under the new key.\n        Useful for working with nested keys like in Chat prompts.\n\n        Args:\n            key (Union[Text, str, List[str]]): The new key to add\n            inplace bool: .\n\n        Returns:\n            Text: A new Text instance with updated keys.\n        \"\"\"\n        #The use of Text._str_formatter(t) instead of str(t) here and elsewhere is for subclasses of Text to reimplement __str__\n        if isinstance(key, Text):\n            key = key.values()\n            if len(key) == 1:\n                key = key[0]\n\n        if isinstance(key, list) and len(key)&gt;1:\n            assert len(key) == len(self.content), f\"Number of keys ({len(key)}) must match number of entries ({len(self.content)})\"\n            content = tuple((k,v) for k,v in zip(key, self.content))\n        elif isinstance(key, str):\n            content = ((key, self.to_ast(self.items())),)\n\n        if inplace:\n            self.content = content\n            return self\n        else:\n            return self.__class__(*content)\n\n    def add_key_(self, keys):\n        return self.add_key(keys, inplace=True)\n\n    @property\n    def iloc(self):\n        class IlocIndexer:\n            def __init__(self, text_items, text_class):\n                self.text_items = text_items\n                self.text_class = text_class\n\n            def __getitem__(self, index):\n                # Handle single integer index\n                if isinstance(index, int):\n                    try:\n                        return self.text_class(self.text_items[index], parse=False)\n                    except IndexError:\n                        raise IndexError(\"Index out of range\")\n\n                # Handle slices\n                elif isinstance(index, slice):\n                    return self.text_class(self.text_items[index], parse=False)\n\n                # Handle a list of indices\n                elif isinstance(index, list):\n                    # Check if all elements in the list are integers\n                    if not all(isinstance(idx, int) for idx in index):\n                        raise IndexError(\"All indices must be integers\")\n                    try:\n                        return self.text_class([self.text_items[idx] for idx in index], parse=False)\n                    except IndexError:\n                        raise IndexError(\"List index out of range\")\n\n                else:\n                    raise TypeError(\"Invalid index type\")\n\n        # Assuming self._content stores the structured content in a list or tuple format\n        if isinstance(self._content, (list, tuple)):\n            items = self._content\n        else:\n            raise TypeError(\"Content type unsupported for iloc indexing.\")\n\n        return IlocIndexer(items, self.__class__)\n\n\n    @property\n    def loc(self):\n        class LocIndexer:\n            def __init__(self, text_instance):\n                self.text_instance = text_instance\n                self.text_class = text_instance.__class__\n\n            def __getitem__(self, key):\n                # Function to recursively get sub-items\n\n                def get_subitems(items, keys):\n                    if keys and isinstance(items, list):\n                        next_key = keys.pop(0)\n                        sub_items = [v for k, v in items if k == next_key]\n                        if not keys:  # No more subkeys, return what we have found\n                            return sub_items\n                        else:  # We have more keys to search for\n                            nested_items = []\n                            for _, v in sub_items:\n                                # Assuming v is a list of tuples as per Text structure\n                                nested_items.extend(get_subitems(v, keys.copy()))\n                            return nested_items\n                    if isinstance(items, list):\n                        return items\n                    else:\n                        return [items]\n\n                if isinstance(key, str):\n                    # Handle key with dot notation for nested access\n                    if '.' in key:\n                        keys = key.split('.')\n                        return self.text_class(get_subitems(self.text_instance.items(), keys), parse=False)\n                    # Single key, simple lookup\n                    return self.text_class([v for k, v in self.text_instance.items()  if k == key], parse=False)\n\n                # If the key is a list, return a subset with keys preserved.\n                elif isinstance(key, list):\n                    return self.text_class([(k, v) for k, v in self.text_instance.items() if k in key], parse=False)\n\n                else:\n                    raise KeyError(f\"Invalid key type: {type(key)}\")\n\n            def __setitem__(self, key, value):\n                def set_items_in_tuple(tup, keys, value, replace=True):\n                    value = self.text_instance.to_ast(value) # format value\n\n                    if isinstance(keys, str):\n                        keys = [keys]\n                        replace = False\n\n                    # Reconstruct the tuple with replacements\n                    rebuilt = []\n                    for item in tup:\n                        k, v = item if isinstance(item, tuple) else ('', item)\n                        if k in keys:\n                            # Replace the entire key-value pair\n                            new_item = value if replace else (k, value)\n                            rebuilt.append(new_item)\n                        else:\n                            if replace:\n                                raise KeyError(f\"Key {keys} not found in texts items {tup}. If you wanted to use .loc to append with key, use a string key not a list.\")\n                            else:\n                                rebuilt.append(self.text_instance.to_ast(item))\n                    return tuple(rebuilt)\n\n                self.text_instance._content = set_items_in_tuple(self.text_instance._content, key, value)\n\n\n\n        return LocIndexer(self)\n\n    def split(self, sep=\" \", mode = \"auto\"):\n        modes = [\"auto\",\"words\", \"sentances\",\"paragraphs\"]\n        assert mode in modes, f\"Mode must be one of {modes}\"\n        from langtorch import TextTensor\n        return TextTensor(str(self).split() if sep == \"\" else str(self).split(sep), parse=False)\n\n\n    def __getitem__(self, index):\n        if isinstance(index, str):\n            return self.loc[index] #Text(*[m for m in self.items() if m[0] == index])\n        return self.iloc[index]\n\n    def __or__(self, other):\n        return\n\n    def __iter__(self):\n        for s in self.content:\n            yield s\n\n    def __add__(self, other):\n        if isinstance(other, str) and not isinstance(other, Text):\n            return self.__class__(*self.content, other, parse = False)\n        elif isinstance(other, Text):\n            return self.__class__(*self.content, *other.content, parse = False)\n        else:\n            raise TypeError(f'Cannot add {type(other)}')\n\n    def __mul__(self, other, strict = False):\n        if isinstance(other, str) and not isinstance(other, Text):\n            try:\n                other = Text(other)\n            except ParseException:\n                other = Text(other, parse = False)\n        content = self.items()\n        result = content\n        formatted_indices = []\n        indices_to_delete = []\n        positional_j = 0\n        for i, (k, v) in enumerate(content):\n            if v == \"*\":\n                logging.debug(f\"Wildcard {(k, v)} ::filled with:: {other.items()}\")\n                if k == \"\":\n                    result = result[:i] + list(other.items()) + result[i+1:]\n                else:\n                    result[i] = (k,other.items())\n                return self.__class__(*result, parse = False) # TODO the case of the other having a re-key pattern\n        for j, (k_, v_) in enumerate(other.items()):\n            # Wildcard case -&gt; Adding a key\n            if v_ == \"*\":\n                for i in indices_to_delete:\n                    result.pop(i)\n                result = [(k_,result)]\n                indices_to_delete, formatted_indices = [],[]\n            elif k_ == \"\" and j not in formatted_indices: # Positional arguments\n                for i, (k, v) in enumerate(content):\n                    if v == str(positional_j):\n                        logging.debug(f\"Place at numbered spot: {(k,v_)} ::at:: {i}\")\n                        result[i] = (k,v_)\n                        positional_j += 1\n                        formatted_indices.append(i)\n                        break\n                else:\n                    for i, (k, v) in enumerate(content):\n                        if v == \"\":\n                            logging.debug(f\"Place: {(k,v_)} ::at:: {i}\")\n                            result[i] = (k,v_)\n                            formatted_indices.append(i)\n                            if v_ == k:\n                                indices_to_delete.append(i)\n                            break\n                    else:\n                        logging.debug(f\"Append {(k_,v_)} ::to:: {content}\")\n                        result += [(k_,v_)]\n            else:\n                for i, (k, v) in enumerate(content):\n                    if i not in formatted_indices:\n                        if (k,v) == (v_,k_):\n                            logging.debug(f\"Identity: {(k_,v_)} ::with:: {content[i]}\")\n                            formatted_indices.append(i)\n                            indices_to_delete.append(i)\n                            break\n                        elif v == k_:\n                            logging.debug(f\"Replace content: {(k_,v_)} ::with:: {content[i]}\")\n                            result[i] = (k,v_)\n                            formatted_indices.append(i)\n                            break\n                        elif k == v_:\n                            logging.debug(f\"Replace key: {(k_,v_)} ::with:: {content[i]}\")\n                            result[i] = (k_,v)\n                            formatted_indices.append(i)\n                            break\n                        elif k == k_:\n                            logging.debug(f\"Concatenate: {(k_,v_)} ::with:: {content[i]}\")\n                            result[i] = (k,v+v_)\n                            break\n                else:\n                    logging.debug(f\"Append {(k_, v_)} ::to:: {content}\")\n                    # Append the ones that didn't have a {key}\n                    # If you don't want this consider using | operation\n                    result += [(k_, v_)]\n        for i in indices_to_delete:\n            result.pop(i)\n        return self.__class__(*result, parse = False)\n\n\n    def format(self, *args, **kwargs):\n        other = Text(*args, parse = False) + Text(kwargs, parse = False)\n        return self.__mul__(other, strict = True)\n\n    def inv(self):\n        return self.__class__(*[(v,k) for k,v in self.items()], parse = False)\n\n    def __pow__(self, power):\n        if power == -1:\n            return self.inv()\n        else:\n            raise ValueError(\"Can only use power -1\")\n\n    def method_apply(self, method: str, *args, to = \"values\", **kwargs):\n        assert to in [\"values\",\"keys\",\"both\"]\n        if to == \"values\":\n            return self.__class__(*list((k, getattr(v, method)(*args, **kwargs)) for k,v in self.items()), parse = False)\n        elif to == \"keys\":\n            return self.__class__(*list((getattr(k, method)(*args, **kwargs), v) for k,v in self.items()), parse = False)\n        elif to == \"both\":\n            return self.__class__(*list((getattr(k, method)(*args, **kwargs), getattr(v, method)(*args, **kwargs)) for k,v in self.items()), parse = False)\n\n    def inspect(self):\n        return \"|\".join(f\"{v} \"+\"{\"+k+\"}, \" for k,v in self.items())\n\n    def upper(self):\n        return self.method_apply(\"upper\")\n\n    def lower(self):\n        return self.method_apply(\"lower\")\n\n    @classmethod\n    def from_pandoc_json(cls, ast_json: str) -&gt; 'Text':\n        \"\"\"\n        Creates a Text object from a Pandoc AST JSON string.\n\n        Args:\n            ast_json (str): A string containing a Pandoc AST in JSON format.\n\n        Returns:\n            Text: An instance of Text representing the parsed content.\n        \"\"\"\n        ast = json.loads(ast_json)\n        content = cls.parse_elements(ast['blocks'])\n        return cls(content)\n\n    @classmethod\n    def parse_elements(cls, elements: List[Any]) -&gt; List[Tuple[str, Any]]:\n        \"\"\"\n        Recursively parses Pandoc AST elements into a list of tuples.\n\n        Args:\n            elements (List[Any]): A list of Pandoc AST elements, which could be blocks or inlines.\n\n        Returns:\n            List[Tuple[str, Any]]: A list of tuples representing the content structure.\n        \"\"\"\n        result = []\n        for element in elements:\n            type_ = element['t']\n            if type_ == 'Header':\n                level, _, inlines = element['c']\n                text_content = ' '.join(cls.parse_elements(inlines))\n                result.append((f'header_h{level}', text_content))\n            elif type_ == 'Para':\n                text_content = ' '.join(cls.parse_elements(element['c']))\n                result.append(('p', text_content))\n            elif type_ == 'Str':\n                result.append(element['c'])\n            elif type_ == 'Space':\n                result.append(' ')\n            # Add more cases here for other types of blocks and inlines\n        return result\n\n        content = parse_blocks(ast['blocks'])\n        return Text(content)\n\n    @classmethod\n    def dict_of_dicts_to_abstract_syntax_tree(cls, dicts):\n        import ast\n        return ast.literal_eval(str(dicts).replace(\":\",\": \").replace(\"{\",\"{ \").replace(\"}\",\" }\"))\n\n    @classmethod\n    def guess_format(cls, text):\n        # Patterns for markup languages\n        patterns = {\n            'html': r'&lt;!DOCTYPE html&gt;|&lt;html&gt;',\n            'markdown': r'^# .+|^- |\\*\\*[^*]+\\*\\*|__[^\\_]+__',\n            'latex': r'\\\\documentclass',\n            # Add more patterns for other markup languages\n        }\n\n        # Check for markup language patterns\n        for format_name, pattern in patterns.items():\n            if re.search(pattern, text, re.MULTILINE):\n                return format_name\n\n        # Check for custom language\n        if Text.detect_custom_language(text):\n            return 'custom_language'\n\n        # Default to plain texts if no patterns match\n        return 'plain'\n\n    @classmethod\n    def detect_custom_language(cls, text):\n        named_string_pattern = r'(\\w+\\{\\:\\w+\\})|(`\\w+`\\{\\:\\w+\\})|(\\{\\w+\\:\\w+\\})|(\\{\\`\\w+\\`\\:\\w+\\})|(\\w+\\{\\`\\`\\:\\})|(\\{\\:\\w+\\})'\n        unnamed_string_pattern = r'(\\{\\w+\\[\\:\\]\\})|(\\{\\`\\w+\\`\\:\\})|(\\{\\`\\w+\\`\\})|(\\{\\`\\w+\\`\\:\\})|\\{\\}|\\`\\`'\n        full_pattern = fr'({named_string_pattern}|{unnamed_string_pattern})'\n        # Match the pattern exactly; ^ and $ are the start and end of the string anchors respectively\n        return bool(re.fullmatch(full_pattern, text))\n\n    @classmethod\n    def guess_language(cls, text):\n        return Text.guess_format(text)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.__new__","title":"<code>__new__(*args, parse='auto', **kwargs)</code>","text":"<p>Construct a new Text instance. Allows for various input formats.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Flexible input data. Can be a parsable string, string sequences, key-value pairs, dicts...                     If None is passed, it will be replaced with a Text instance with empty content.</p> <code>()</code> <code>parse</code> <code>Union[bool, str]</code> <p>Enable or disable the automatic parsing of string content.                     The default behavior is to automatically detect language and parse it only                     if the texts is written in the default langtorch syntax. Set to True to                     also enable parsing of the detected languages using pandoc.</p> <code>'auto'</code> <code>**kwargs</code> <p>Additional named textual data entries.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Text</code> <p>A structured textual instance.</p> <p>Raises:</p> Type Description <code>ParseException</code> <p>If automatic parsing fails. Consider disabling parsing if this occurs.</p> <code>ValueError</code> <p>When an unsupported input format is provided e.g. a TextTensor is passed.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>def __new__(cls, *args, parse = \"auto\", **kwargs):\n    \"\"\"\n    Construct a new Text instance. Allows for various input formats.\n\n    Args:\n        *args: Flexible input data. Can be a parsable string, string sequences, key-value pairs, dicts...\n                                If None is passed, it will be replaced with a Text instance with empty content.\n        parse (Union[bool, str], optional): Enable or disable the automatic parsing of string content.\n                                The default behavior is to automatically detect language and parse it only\n                                if the texts is written in the default langtorch syntax. Set to True to\n                                also enable parsing of the detected languages using pandoc.\n        **kwargs: Additional named textual data entries.\n\n    Returns:\n        Text: A structured textual instance.\n\n    Raises:\n        ParseException: If automatic parsing fails. Consider disabling parsing if this occurs.\n        ValueError: When an unsupported input format is provided e.g. a TextTensor is passed.\n    \"\"\"\n    if len(args) == 0 or (len(args) == 1 and args[0] is None):\n        instance = super().__new__(cls, \"\")\n        instance._content = tuple()\n        return instance\n    content = [c if c is not None else cls.identity for c in (list(args) + list(kwargs.items()))]\n    # cast TextTensors to strings\n    for i in range(len(content)):\n        if isinstance(content[i], torch.Tensor) and hasattr(content[i], \"content\"):\n            content[i] = content[i].sum().item()\n            assert isinstance(content[i], str) or (0 &lt; len(content[i]) &lt;= 2)\n    # returns the final content tuple\n    if len(content) == 1 and cls.is_valid_tree(content, is_tuple=True):\n        content = content[0]\n    else:\n        content = cls.to_ast(*content, parse = parse, is_tuple=True)\n\n    assert cls.is_valid_tree(content, is_tuple=True), f\"Invalid tree: {content}\"\n\n    instance = super().__new__(cls, cls.concatenate_terminals(content))\n    instance._content = content\n    return instance\n</code></pre>"},{"location":"reference/text/#langtorch.Text.add_key","title":"<code>add_key(key, inplace=False)</code>","text":"<p>Add a top-level  key, placing the items of the original as a value under the new key. Useful for working with nested keys like in Chat prompts.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[Text, str, List[str]]</code> <p>The new key to add</p> required <code>inplace</code> <code>bool</code> <p>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Text</code> <p>A new Text instance with updated keys.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>def add_key(self, key, inplace=False):\n    \"\"\"\n    Add a top-level  key, placing the items of the original as a value under the new key.\n    Useful for working with nested keys like in Chat prompts.\n\n    Args:\n        key (Union[Text, str, List[str]]): The new key to add\n        inplace bool: .\n\n    Returns:\n        Text: A new Text instance with updated keys.\n    \"\"\"\n    #The use of Text._str_formatter(t) instead of str(t) here and elsewhere is for subclasses of Text to reimplement __str__\n    if isinstance(key, Text):\n        key = key.values()\n        if len(key) == 1:\n            key = key[0]\n\n    if isinstance(key, list) and len(key)&gt;1:\n        assert len(key) == len(self.content), f\"Number of keys ({len(key)}) must match number of entries ({len(self.content)})\"\n        content = tuple((k,v) for k,v in zip(key, self.content))\n    elif isinstance(key, str):\n        content = ((key, self.to_ast(self.items())),)\n\n    if inplace:\n        self.content = content\n        return self\n    else:\n        return self.__class__(*content)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.from_api_response","title":"<code>from_api_response(*response_dicts)</code>  <code>classmethod</code>","text":"<p>Text from a list of dicts with keys 'role' and 'content'</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef from_api_response(cls, *response_dicts):\n    \"\"\"Text from a list of dicts with keys 'role' and 'content'\"\"\"\n    if isinstance(messages[0], list):\n        messages = messages[0]\n    text = []\n    for m in messages:\n        if isinstance(m, list):\n            for mm in m:\n                text.append(mm[\"role\"],m[\"content\"])\n        else:\n            text.append((m[\"role\"],m[\"content\"]))\n    return cls(*text)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.from_messages","title":"<code>from_messages(*messages)</code>  <code>classmethod</code>","text":"<p>Text from a list of dicts with keys 'role' and 'content'</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef from_messages(cls, *messages):\n    \"\"\"Text from a list of dicts with keys 'role' and 'content'\"\"\"\n    if isinstance(messages[0], list):\n        messages = messages[0]\n    text = []\n    for m in messages:\n        if isinstance(m, list):\n            for mm in m:\n                text.append(mm[\"role\"],m[\"content\"])\n        else:\n            text.append((m[\"role\"],m[\"content\"]))\n    return cls(*text)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.from_pandoc_json","title":"<code>from_pandoc_json(ast_json)</code>  <code>classmethod</code>","text":"<p>Creates a Text object from a Pandoc AST JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>ast_json</code> <code>str</code> <p>A string containing a Pandoc AST in JSON format.</p> required <p>Returns:</p> Name Type Description <code>Text</code> <code>Text</code> <p>An instance of Text representing the parsed content.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef from_pandoc_json(cls, ast_json: str) -&gt; 'Text':\n    \"\"\"\n    Creates a Text object from a Pandoc AST JSON string.\n\n    Args:\n        ast_json (str): A string containing a Pandoc AST in JSON format.\n\n    Returns:\n        Text: An instance of Text representing the parsed content.\n    \"\"\"\n    ast = json.loads(ast_json)\n    content = cls.parse_elements(ast['blocks'])\n    return cls(content)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.is_valid_tree","title":"<code>is_valid_tree(entry, is_tuple=False)</code>  <code>classmethod</code>","text":"<p>Checks if an entry is a valid tree for a Text instance.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef is_valid_tree(cls, entry, is_tuple = False):\n    \"\"\"\n    Checks if an entry is a valid tree for a Text instance.\n\n    \"\"\"\n    if isinstance(entry, cls):\n        entry = entry.items()\n    elif isinstance(entry, tuple) and is_tuple:\n        if len(entry) == 0:\n            return True\n        entry = list(entry) if len(entry)&gt;1 else entry[0]\n    # Base cases\n    if cls.is_terminal_node(entry):\n        return True\n\n    # Recursive cases\n    if isinstance(entry, tuple) and len(entry) != 2:\n        return False\n    if isinstance(entry, tuple) and len(entry) == 2 and isinstance(entry[0], str) and (\n            isinstance(entry[1], list) or isinstance(entry[1], tuple)):\n        return all(cls.is_valid_tree(child) for child in entry[1])\n    if isinstance(entry, list) and len(entry)&gt;1:\n        return all(cls.is_valid_tree(child) for child in entry)\n    if isinstance(entry, list) and len(entry)&lt;=1:\n        return False  # Single-element lists are not valid\n\n    # If none of the above cases match, it's not valid\n    return False\n</code></pre>"},{"location":"reference/text/#langtorch.Text.item_print","title":"<code>item_print()</code>","text":"<p>Prints the Text instance with keys aligned over their corresponding values.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>def item_print(self):\n    \"\"\"\n    Prints the Text instance with keys aligned over their corresponding values.\n    \"\"\"\n    # Extract the key-value pairs from the _content attribute\n    key_value_pairs = [pair  if isinstance(pair, tuple) else ('', pair) for pair in self.items()]\n\n    # Find the longest key and value for formatting purposes\n    longest_key = max(len(key) for key, _ in key_value_pairs)\n    longest_value = max(len(value) for _, value in key_value_pairs)\n\n    # Create the top and bottom lines for the keys and values\n    top_line = \"\"\n    bottom_line = \"\"\n    for key, value in key_value_pairs:\n        # Calculate padding to center the key\n        total_padding = longest_key - len(key)\n        left_padding = total_padding // 2\n        right_padding = total_padding - left_padding\n\n        # Add the formatted key and value to the respective lines\n        top_line += f\"{' ' * left_padding}{key}{' ' * right_padding}|\"\n        bottom_line += f\"{value.ljust(longest_value)}|\"\n\n    # Remove the trailing '|' character\n    top_line = top_line.rstrip('|')\n    bottom_line = bottom_line.rstrip('|')\n\n    # Print the formatted lines\n    print(top_line)\n    print(bottom_line)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.items","title":"<code>items()</code>","text":"<p>Retrieves key-value pairs from the Text object, allowing for structured data extraction and further processing.</p> <p>Returns:</p> Type Description <p>List[Tuple[str, Union[str, Tuple[...]]]]: A list of key-value pairs representing the Text's content.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>def items(self):\n    \"\"\"\n    Retrieves key-value pairs from the Text object, allowing for structured data extraction\n    and further processing.\n\n    Returns:\n        List[Tuple[str, Union[str, Tuple[...]]]]: A list of key-value pairs representing the Text's content.\n    \"\"\"\n    return [(arg[0], arg[1]) if isinstance(arg, tuple) else ('', arg) for arg in self._content]\n</code></pre>"},{"location":"reference/text/#langtorch.Text.parse_elements","title":"<code>parse_elements(elements)</code>  <code>classmethod</code>","text":"<p>Recursively parses Pandoc AST elements into a list of tuples.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>List[Any]</code> <p>A list of Pandoc AST elements, which could be blocks or inlines.</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, Any]]</code> <p>List[Tuple[str, Any]]: A list of tuples representing the content structure.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef parse_elements(cls, elements: List[Any]) -&gt; List[Tuple[str, Any]]:\n    \"\"\"\n    Recursively parses Pandoc AST elements into a list of tuples.\n\n    Args:\n        elements (List[Any]): A list of Pandoc AST elements, which could be blocks or inlines.\n\n    Returns:\n        List[Tuple[str, Any]]: A list of tuples representing the content structure.\n    \"\"\"\n    result = []\n    for element in elements:\n        type_ = element['t']\n        if type_ == 'Header':\n            level, _, inlines = element['c']\n            text_content = ' '.join(cls.parse_elements(inlines))\n            result.append((f'header_h{level}', text_content))\n        elif type_ == 'Para':\n            text_content = ' '.join(cls.parse_elements(element['c']))\n            result.append(('p', text_content))\n        elif type_ == 'Str':\n            result.append(element['c'])\n        elif type_ == 'Space':\n            result.append(' ')\n        # Add more cases here for other types of blocks and inlines\n    return result\n\n    content = parse_blocks(ast['blocks'])\n    return Text(content)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.set_key","title":"<code>set_key(key, inplace=False)</code>","text":"<p>Override keys for the textual entries, used for restructuring the content. Useful for substituting the key right before passing TextTensor to a Module.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[Text, str, List[str]]</code> <p>The new key or keys to apply</p> required <code>inplace</code> <code>bool</code> <p>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Text</code> <p>A new Text instance with updated keys.</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>def set_key(self, key, inplace = False):\n    \"\"\"\n    Override keys for the textual entries, used for restructuring the content.\n    Useful for substituting the key right before passing TextTensor to a Module.\n\n    Args:\n        key (Union[Text, str, List[str]]): The new key or keys to apply\n        inplace bool: .\n\n    Returns:\n        Text: A new Text instance with updated keys.\n    \"\"\"\n    #The use of Text._str_formatter(t) instead of str(t) here and elsewhere is for subclasses of Text to reimplement __str__\n    if isinstance(key, Text):\n        key = key.values()\n        if len(key) == 1:\n            key = key[0]\n\n    if isinstance(key, list):\n        assert len(key) == len(self.values()), f\"Number of keys ({len(key)}) must match number of values ({len(self.values())})\"\n        content = tuple((k,v) for k,v in zip(key, self.values()))\n    elif isinstance(key, str):\n        content = ((key, Text._str_formatter(self)),)\n    if inplace:\n        self.content = content\n        return self\n    else:\n        return self.__class__(*content)\n</code></pre>"},{"location":"reference/text/#langtorch.Text.to_ast","title":"<code>to_ast(*args, parse=True, is_tuple=False)</code>  <code>classmethod</code>","text":"<p>Reformats a wide array of construtor patterns into a unified AST-like format</p> Source code in <code>langtorch\\texts\\text.py</code> <pre><code>@classmethod\ndef to_ast(cls, *args, parse = True, is_tuple = False):\n    \"\"\"Reformats a wide array of construtor patterns into a unified AST-like format\"\"\"\n    if len(args) == 1 and isinstance(args[0], list):\n        args = args[0]\n    if len(args) == 1 and ((isinstance(args[0], tuple) and len(args[0]) &gt; 2) or isinstance(args[0], list)):\n        # List or tuple of strings / named strings\n        args = args[0]\n    elif isinstance(args[0], dict):\n        # Dictionary of named strings\n        args = args[0].items()\n    elif isinstance(args[0], Text) and len(args) == 1:\n        # Passing cls instance to itself\n        args = args[0].content\n    elif all([isinstance(arg, str) for arg in args]):\n        if parse:\n            try:\n                result = []\n                for arg in args:\n                    arg = cls.parse(arg)\n                    result += arg\n                args = result\n            except ParseException as E:\n                print(f\"Last parsed string: {arg}\")\n                raise ParseException(str(E) + \"\\nYou may want to disable string parsing, with parse = False\")\n        else:\n            pass\n    if any([isinstance(arg, torch.Tensor) for arg in args]):\n        raise ValueError(\"You cannot initialise Text from a TextTensor. Use tensors.item() or otherwise transform the tensors to a string, list or dictionary.\")\n    def simplify(arg, parse = False):\n        if isinstance(arg, tuple) and len(arg) == 2 and not (isinstance(arg[0], str) and isinstance(arg[1], str)):\n            # Fix tuple types\n            return (str(arg[0]), simplify(arg[1]))\n        elif isinstance(arg, tuple) and len(arg) == 2 and isinstance(arg[0], str) and isinstance(arg[1], str):\n            # CORRECT\n            return arg\n        elif isinstance(arg, tuple) and len(arg) == 1:\n            # CORRECT though should be avoided\n            return arg[0]\n        elif isinstance(arg, tuple) and len(arg) &gt; 2:\n            # Assume a tuple of length != 2 was supposed to be a list\n            logging.debug(f\"Tuples of length 2 represent (key, value) in Text objects. When parsing a Text entry was a tuple of length {len(arg)},\\nit was converted to a list and may lead to errors.\")\n            arg = list(arg)\n\n        # Not a named string\n        if isinstance(arg, list) and len(arg) == 1:\n            return arg[0]\n        elif isinstance(arg, list):\n            return [simplify(element, parse=parse) for element in arg]\n        elif isinstance(arg, dict):\n            return Text.to_ast(list(arg.items()), parse=parse)\n        elif isinstance(arg, Text):\n            if len(arg.items()) == 1:\n                return Text.to_ast(arg.items()[0], parse=parse)\n            else:\n                return Text.to_ast(arg.items(), parse=parse)\n        elif isinstance(arg, str):\n            # CORRECT\n            return arg\n        raise ParseException(f\"Could not parse {arg} of type {type(arg)} and len {len(arg)}\")\n\n    content = [simplify(arg, parse=parse) for arg in args]\n\n    if not is_tuple: # Recursive case: In these cases we are returning a node or tree with a single root node\n        return content[0] if isinstance(content, list) and len(content) == 1  else content\n    else: # Base case: In these cases we are returning a tuple of nodes for the ._content attribute\n        def check_for_lists(tree): # A basic check for lists in the tree\n            if isinstance(tree, list):\n                # Prune single-element lists\n                if len(tree) == 1:\n                    raise ValueError(f\"single-element list {tree}\")\n                else:\n                    return [check_for_lists(element) for element in tree]\n            elif isinstance(tree, tuple) and len(tree) == 2:\n                key, value = tree\n                return key, check_for_lists(value)\n            return tree\n\n        content = tuple(check_for_lists(arg) for arg in content)\n\n        return content\n</code></pre>"},{"location":"reference/textmodule/","title":"TextModule","text":"<p>...</p>"},{"location":"reference/textmodule/#reference","title":"Reference","text":"<p>             Bases: <code>Module</code></p> <p>A <code>TextModule</code> is an abstraction designed to facilitate operations on <code>TextTensor</code> objects using a chain of texts transformations and language model inferences. It inherits from <code>torch.nn.Module</code>.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>TextTensor</code> <p>A <code>TextTensor</code> containing the prompt template(s) that will be                   used to format input texts data.</p> <code>activation</code> <code>Module</code> <p>A callable module, typically representing a language model                           inference call, which serves as the 'activation function' for                           the module. This activation function is applied to the                           formatted texts to obtain the output.</p> <code>key</code> <code>str</code> <p>An optional string representing the key that will be automatically assigned to        the output <code>TextTensor</code>.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the <code>TextModule</code> with a prompt                                               template, an optional activation function,                                               and an optional key for the output tensors.</p> <code>forward</code> <p>TextTensor) -&gt; TextTensor: Processes the input <code>TextTensor</code> by formatting it                                       with the module's content and then passing the                                       result through the activation function. The output                                       is then returned with the specified key.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; activation = SomeLanguageModelAPI()  # This should be an instance of a class inheriting from torch.nn.Module.\n&gt;&gt;&gt; text_module = TextModule(TextTensor([\"Summarize the following texts: {texts}\"]),\n                             activation=activation, key=\"summary\")\n&gt;&gt;&gt; input_text = TextTensor({\"texts\": \"An example input texts.\"})\n&gt;&gt;&gt; summary = text_module(input_text)\n# summary is a TextTensor with the key \"summary\" containing the summarized texts.\n</code></pre> Source code in <code>langtorch\\tt\\to_tt\\textmodule.py</code> <pre><code>class TextModule(torch.nn.Module):\n    \"\"\"\n       A `TextModule` is an abstraction designed to facilitate operations on `TextTensor` objects using a\n       chain of texts transformations and language model inferences. It inherits from `torch.nn.Module`.\n\n       Attributes:\n           prompt (TextTensor): A `TextTensor` containing the prompt template(s) that will be\n                                 used to format input texts data.\n           activation (torch.nn.Module): A callable module, typically representing a language model\n                                         inference call, which serves as the 'activation function' for\n                                         the module. This activation function is applied to the\n                                         formatted texts to obtain the output.\n           key (str): An optional string representing the key that will be automatically assigned to\n                      the output `TextTensor`.\n\n       Methods:\n           __init__(prompt=[\"\"], activation=None, key=None): Initializes the `TextModule` with a prompt\n                                                             template, an optional activation function,\n                                                             and an optional key for the output tensors.\n           forward(input: TextTensor) -&gt; TextTensor: Processes the input `TextTensor` by formatting it\n                                                     with the module's content and then passing the\n                                                     result through the activation function. The output\n                                                     is then returned with the specified key.\n\n       Examples:\n           &gt;&gt;&gt; activation = SomeLanguageModelAPI()  # This should be an instance of a class inheriting from torch.nn.Module.\n           &gt;&gt;&gt; text_module = TextModule(TextTensor([\"Summarize the following texts: {texts}\"]),\n                                        activation=activation, key=\"summary\")\n           &gt;&gt;&gt; input_text = TextTensor({\"texts\": \"An example input texts.\"})\n           &gt;&gt;&gt; summary = text_module(input_text)\n           # summary is a TextTensor with the key \"summary\" containing the summarized texts.\n       \"\"\"\n    input_class = TextTensor\n    output_class = TextTensor\n    def __init__(self,\n                 prompt: Union[str, 'TextTensor'] = [\"\"],\n                 activation=None,\n                 key=None,\n                 memoize=False, *args, **kwargs):\n        super(TextModule, self).__init__(*args, **kwargs)\n        self._prompt = TextTensor(prompt) if not isinstance(prompt, TextTensor) else prompt\n        self.activation = activation  # An identity function if nothing is passed\n        self.memo = {} if memoize else None\n        self.target_embedding = None\n        self.key = key\n        self.register_forward_pre_hook(self.pre_forward_hook)\n        self.register_forward_hook(self.post_forward_hook)\n\n    @staticmethod\n    def pre_forward_hook(module, input: List[TextTensor]):\n        for tensor in input:\n            assert isinstance(tensor, module.input_class)\n\n\n    @staticmethod\n    def post_forward_hook(module, input, output):\n        if module.key is not None:\n            if isinstance(output,TextTensor):\n                print(f\"Could not set key '{module.key}', Module output is not a TextTensor\")\n            else:\n                output.set_key_(module.key)\n\n    @property\n    def prompt(self):\n        return self._prompt\n\n    @prompt.setter\n    def prompt(self, prompt):\n        self._prompt = prompt.content if isinstance(prompt, TextModule) else prompt if isinstance(prompt,\n                                                                                                      TextTensor) else TextTensor(\n            prompt)\n        assert self._prompt is not None\n\n    @property\n    def content(self):\n        return self.parameters()\n\n    @content.setter\n    def content(self, content):\n        raise NotImplementedError('Setting TextModule.content is ambiguous, as it holds all module parameters.\\nUse text_module.prompt = \"New prompt\" instead.')\n\n\n    def forward(self, *input) -&gt; TextTensor:\n        \"\"\"\n                By default, the TextModule forward formats the input using the prompt template tensors using TextTensor multiplication,\n                then if not None applies an activation function, then if not None a key is set for all entries of the output TextTensor.\n                Subclass this class to use forward hooks that check the input and output types and set the key.\n\n                Parameters:\n                    input (TextTensor): The input `TextTensor` that needs to be processed.\n\n                Returns:\n                    TextTensor: The output `TextTensor` with the content processed by the activation function\n                                and assigned the specified key.\n\n                Examples:\n                    &gt;&gt;&gt; input_text = TextTensor({\"texts\": \"An example input texts.\"})\n                    &gt;&gt;&gt; output = text_module(input_text)\n                    # output is a TextTensor with the processed content and key \"key_points\".\n                \"\"\"\n        if self.memo is None:\n            return self._forward(*input)\n        # Memoization\n        input_tuple = langtorch.tensor_or_tensors_to_tuple(input)\n        if input_tuple in self.memo:\n            return self.memo[input_tuple]\n\n        output = self._forward(*input)\n        self.memo[input_tuple] = output\n        return output\n\n    def _forward(self, input) -&gt; TextTensor:\n        return self.activation(self.content * input) if self.activation else self.content * input\n    def extra_repr(self):\n        # This method is used to provide extra information for the print representation of the module\n        repr = f'content={self.content}'\n        if self.activation:\n            repr += f', activation={self.activation}'\n        if self.key!=None:\n            repr += f', key={self.key}'\n        return repr\n\n    def __contains__(self, item):\n        return item in self.content\n\n    def embed(self):\n        self.embedding = get_embedding(self.content)\n</code></pre>"},{"location":"reference/textmodule/#langtorch.TextModule.forward","title":"<code>forward(*input)</code>","text":"<p>By default, the TextModule forward formats the input using the prompt template tensors using TextTensor multiplication, then if not None applies an activation function, then if not None a key is set for all entries of the output TextTensor. Subclass this class to use forward hooks that check the input and output types and set the key.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TextTensor</code> <p>The input <code>TextTensor</code> that needs to be processed.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>TextTensor</code> <code>TextTensor</code> <p>The output <code>TextTensor</code> with the content processed by the activation function         and assigned the specified key.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input_text = TextTensor({\"texts\": \"An example input texts.\"})\n&gt;&gt;&gt; output = text_module(input_text)\n# output is a TextTensor with the processed content and key \"key_points\".\n</code></pre> Source code in <code>langtorch\\tt\\to_tt\\textmodule.py</code> <pre><code>def forward(self, *input) -&gt; TextTensor:\n    \"\"\"\n            By default, the TextModule forward formats the input using the prompt template tensors using TextTensor multiplication,\n            then if not None applies an activation function, then if not None a key is set for all entries of the output TextTensor.\n            Subclass this class to use forward hooks that check the input and output types and set the key.\n\n            Parameters:\n                input (TextTensor): The input `TextTensor` that needs to be processed.\n\n            Returns:\n                TextTensor: The output `TextTensor` with the content processed by the activation function\n                            and assigned the specified key.\n\n            Examples:\n                &gt;&gt;&gt; input_text = TextTensor({\"texts\": \"An example input texts.\"})\n                &gt;&gt;&gt; output = text_module(input_text)\n                # output is a TextTensor with the processed content and key \"key_points\".\n            \"\"\"\n    if self.memo is None:\n        return self._forward(*input)\n    # Memoization\n    input_tuple = langtorch.tensor_or_tensors_to_tuple(input)\n    if input_tuple in self.memo:\n        return self.memo[input_tuple]\n\n    output = self._forward(*input)\n    self.memo[input_tuple] = output\n    return output\n</code></pre>"},{"location":"reference/texttensor/","title":"TextTensor","text":"<p>...</p>"},{"location":"reference/texttensor/#reference","title":"Reference","text":"<p>             Bases: <code>Tensor</code></p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>class TextTensor(torch.Tensor, metaclass=_ParameterMeta):\n    text_class = Text # The class of a Tensor entry, replaced in subclasses. May in the future move this logic to the metaclass\n    _embedding_module = None # TextTensor to Tensor with an embedding model\n    _tokenizer = None  # TextTensor to Tensor with a transformer tokenizer\n    parse = 'auto'\n\n    @classmethod\n    def input_formatter(cls, content):\n        # Default implementation, to be overridden by subclasses if needed\n        return content\n\n    @classmethod\n    def linter(cls, tensor):\n        # Default implementation, ensures correct class of Tensor content entries\n        tensor.content = chararray_to_TextArray(tensor.content, cls.text_class)\n        return tensor\n\n    @classmethod\n    def _str_formatter(cls, instance):\n        if len(instance.shape) == 0:\n            return str(instance.item())\n        string = str(np.array(instance.content, dtype=np.unicode_)).replace(\"\\\\n\",\"\\n\")\n        return f'{cls.__name__}({string}, requires_grad = {instance.requires_grad})'  # {super().__repr__().replace(\"TextTensor(\",\"\")}'\n\n    @classmethod\n    def _str_formatter(cls, array, indent='  '):\n        array = array.content if isinstance(array, TextTensor) else array\n        def format_entry(entry, max_lines, max_width):\n            # Split the entry into lines, pad them to match the max width, and ensure each entry has the same number of lines\n            lines = entry.split('\\n')\n            padded_lines = [line.ljust(max_width) for line in lines]\n            padded_lines += [' ' * max_width] * (max_lines - len(lines))  # Pad with empty lines if needed\n            return padded_lines\n\n        def format_2d(array_2d):\n            # Calculate max width for each column\n            max_width_per_col = [max(len(line) for element in col for line in element.split('\\n')) for col in\n                                 zip(*array_2d)]\n\n            # Create a list of lists for each formatted line\n            formatted_rows = []\n            for row in array_2d:\n                # Calculate the max number of lines for this row\n                max_lines_in_row = max(element.count('\\n') + 1 for element in row)\n\n                # Format each entry in the row with max_lines corresponding to the current row\n                formatted_entries = [format_entry(entry, max_lines_in_row, max_width)\n                                     for entry, max_width in zip(row, max_width_per_col)]\n\n                # Transpose to turn columns into rows and pad rows to have the same number of lines\n                transposed_entries = list(zip_longest(*formatted_entries, fillvalue=' ' * max_width_per_col[0]))\n\n                # Join the transposed lines into formatted strings\n                for i, transposed_line in enumerate(transposed_entries):\n                    if i == 0:\n                        formatted_rows.append(\"[\" + '  '.join(transposed_line))\n                    elif i == len(transposed_entries) - 1:\n                        formatted_rows.append(\" \" + '  '.join(transposed_line) + \"]\")\n                    else:\n                        formatted_rows.append(\" \" + '  '.join(transposed_line))\n            return formatted_rows\n\n        def format_1d(array_1d):\n            max_lines = max(element.count('\\n') + 1 for element in array_1d)\n            max_width = max(max(len(line) for line in element.split('\\n')) for element in array_1d)\n            formatted_entries = [format_entry(entry, max_lines, max_width) for entry in array_1d]\n\n            # Transpose to align multiline entries\n            transposed_entries = list(zip_longest(*formatted_entries, fillvalue=' ' * max_width))\n            formatted_rows = ['  '.join(line) for line in transposed_entries]\n            return '[\\n' + indent + ('\\n' + indent).join(formatted_rows) + '\\n' + indent[:-2] + ']'\n\n        if array.ndim == 0:\n            return str(array.item())\n        if array.ndim == 1:\n            return format_1d(array)\n        elif array.ndim == 2:\n            formatted_lines = format_2d(array)\n            return '[\\n' + indent + ('\\n' + indent).join(formatted_lines) + '\\n' + indent[:-2] + ']'\n        else:\n            inner_arrays = [cls._str_formatter(sub_array, indent + '  ') for sub_array in array]\n            inner_content = (',\\n' + indent).join(inner_arrays)\n            return '[\\n' + indent + inner_content + '\\n' + indent[:-2] + ']'\n\n    def __new__(cls, content=\"\", embedding: Optional[Union[torch.Tensor, List[float], np.ndarray, bool]] = None,\n                metadata=None, requires_grad: bool =True, is_gradient: bool = False, is_param=True, parse = True, **kwargs):\n        if metadata is None:\n            metadata = dict()\n        for attr in [\"content\", \"embedding\"]:\n            if not attr in metadata:\n                metadata[attr] = eval(attr)\n\n        # Set content to be an object array with cls.text_class entries\n        metadata[\"content\"] = cls.to_array(metadata[\"content\"], parse=parse)\n        # Apply input formatter\n        metadata[\"content\"] = cls.input_formatter(metadata[\"content\"])\n        if embedding == True:\n            metadata[\"embedding\"] = get_embedding([str(m) for m in metadata[\"content\"].flat])[\"embedding\"].to_list()\n\n        tensor = super().__new__(cls, torch.arange(metadata[\"content\"].size, dtype=torch.float32).reshape(\n            metadata[\"content\"].shape), **kwargs)\n\n        tensor.metadata = metadata\n        tensor._is_param = is_param\n        tensor._is_gradient = is_gradient\n        tensor.requires_grad = requires_grad\n        assert tensor._content is not None\n\n        # Apply linter\n        tensor = cls.linter(tensor)\n        return tensor\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def __hash__(self):\n        return id(self)\n\n    @classmethod\n    def to_array(cls, input, **kwargs):\n        return input.content if isinstance(input, cls) \\\n            else np.array(input, dtype=object) if isinstance(input, Text) \\\n            else chararray_to_TextArray(input, cls.text_class, **kwargs)\n\n    @classmethod\n    def from_file(cls, path, encoding = \"utf-8\", **kwargs):\n        with open(path, encoding = encoding) as f:\n            content = f.read()\n        return cls(content, **kwargs)\n\n    @property\n    def content(self):\n        return self._content\n\n    @content.setter\n    def content(self, content):\n        self._content = TextTensor.to_array(content)\n        assert self._content is not None\n\n    @property\n    def embedding(self):\n        if self._embedding is None and hasattr(self, 'metadata') and self.metadata is not None:\n            print(\"Warning: autoembedding ON\")\n            self.embed()\n        return self._embedding\n\n    @embedding.setter\n    def embedding(self, embedding: Union[torch.Tensor, List[float], np.ndarray, tuple]):\n        if embedding is None:\n            self._embedding = None\n            return\n        assert isinstance(embedding, torch.Tensor) or isinstance(embedding, np.ndarray) or isinstance(embedding, list)  or isinstance(embedding, list), \"Value of embedding should be a Tensor, array or list\"\n\n        self._embedding = embedding if isinstance(embedding, torch.Tensor) else torch.tensor(embedding)\n        try:\n            self._embedding = self._embedding.reshape(*self.content.shape, -1)\n        except ValueError:\n            raise ValueError(\"The shape of the embedding does not match the size of the TextTensor\")\n        self._metadata[\"embedding\"] = self._embedding\n\n    @property\n    def tokens(self):\n        if not hasattr(self, \"_tokens\"):\n            if self.tokenizer is not None:\n                self._tokens = self.tokenizer(self)\n            else:\n                raise AttributeError(\"Tokens unavailable as no tokenizer has been set\")\n        return self._tokens\n\n    input_ids = tokens\n\n    @tokens.setter\n    def tokens(self, tokens):\n        self._tokens = tokens\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n\n    @tokenizer.setter\n    def tokenizer(self, tokenizer):\n        if isinstance(tokenizer, str):\n            from transformers import AutoTokenizer\n            self._tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n        self._tokenizer = tokenizer\n\n    def tokenize(self, tokenizer = None):\n        if tokenizer is not None:\n            self.tokenizer = tokenizer\n        assert self.tokenizer is not None\n        input_dict = tokenizer.toke\n\n\n\n    @classmethod\n    def from_df(cls, df: 'DataFrame', **kwargs) -&gt; 'TextTensor':\n        text_list = []\n        if not \"parse\" in kwargs:\n            kwargs[\"parse\"] = False\n\n        for row in df.values:\n            # Create a Text object with (column_name, value) pairs\n            text_content = [(col_name, str(value)) for col_name, value in zip(df.columns, row) if value != np.nan and str(value)!=\"\"]\n            text_obj = Text(*text_content, **kwargs)\n            text_list.append(text_obj)\n\n        # Convert the list of Text objects to a TextTensor of shape (n, 1)\n        text_tensor = cls(text_list, **kwargs).reshape(-1, 1)\n\n        return text_tensor\n\n    def set_key(self, keys, inplace = False) -&gt; 'TextTensor':\n        reshaped_keys = utils.zeros_like(self.content) + keys\n        if inplace:\n            result = self\n        else:\n            result = self.copy()\n\n        # Apply set_key to each entry\n        for index, text_entry in np.ndenumerate(result.content):\n            result.content[index] = text_entry.set_key(reshaped_keys[index].item())\n\n        return result\n\n    def set_key_(self, keys) -&gt; 'TextTensor':\n        return self.set_key(keys, inplace = True)\n\n\n    def add_key(self, keys, inplace = False) -&gt; 'TextTensor':\n        reshaped_keys = utils.str_like(self.content, keys)\n        if inplace:\n            result = self\n        else:\n            result = self.copy()\n\n        # Apply set_key to each entry\n        for index, text_entry in np.ndenumerate(result.content):\n            result.content[index] = text_entry.add_key(reshaped_keys[index].item())\n\n        return result\n\n    def add_key_(self, keys) -&gt; 'TextTensor':\n        return self.add_key(keys, inplace = True)\n\n\n    def copy(self):\n        new_dict = deepcopy(self.metadata)\n        return self.__class__(metadata = new_dict, parse = False)\n\n\n    def apply(self, func):\n        \"\"\"Applies a function: Text -&gt; Text to each entry of self.content\"\"\"\n        # Ensure the function is callable\n        if not callable(func):\n            raise ValueError(\"Provided function is not callable.\")\n\n        # Apply the function to each entry\n        for index, text_entry in np.ndenumerate(self.content):\n            self.content[index] = func(text_entry)\n\n        return self\n\n    def inv(self):\n        # Apply inverse operation on each element of the char array\n        self.content = np.vectorize(lambda x: x.inv())(self.content)\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n\n    @metadata.setter\n    def metadata(self, metadata):\n        if not hasattr(self, \"_metadata\"): self._metadata = {}\n        # set core attributes\n        for attr in [\"content\", \"embedding\"]:\n            if attr in metadata:\n                setattr(self, attr, metadata.pop(attr))\n\n        for attr in [\"content\", \"embedding\"]:\n            metadata[attr] = getattr(self, \"_\"+attr)\n\n        self._metadata = metadata\n\n    def metadata_index(self, index):\n        \"\"\"Pick out entries of metadata with the provided index. Usually used to transfer metadata to a sub-tensors.\"\"\"\n\n\n        index = utils.positive_indices(index, self.content.shape)\n        index = index[0] if len(index) == 1 else index\n\n        if len(self.content.shape) == 0: # and (index == slice(None, None, None))\n            return self.metadata_apply(lambda v: v, lambda v: v, lambda v: v)\n\n        else:\n            return self.metadata_apply(lambda v: v[index], lambda v: v[index], lambda v: v)\n\n    def metadata_apply(self, f_tensor=None, f_embedding=None, f_scalar=None, *args, **kwargs):\n        \"\"\"\n        Apply a specified function to a copy of metadata elements of the TextTensor.\n\n        Parameters:\n        - f_tensor (function, optional): Function to be applied to tensors-shaped metadata attributes.\n        - f_embedding (function, optional): Function to be applied to the 'embedding' attribute.\n        - f_scalar (function, optional): Function to be applied to scalar metadata attributes.\n        - *args: Variable length argument list to be passed to the functions.\n        - **kwargs: Arbitrary keyword arguments to be passed to the functions.\n\n        Returns:\n        - dict: A dictionary with updated metadata attributes.\n        \"\"\"\n        tensor_attributes, scalar_attributes = [], []\n        for k, v in self._metadata.items():\n            try:\n                if tuple(v.shape) == tuple(self.shape):\n                    tensor_attributes.append(k)\n            except AttributeError:\n                if k != \"embedding\":\n                    scalar_attributes.append(k)\n\n        metadata = self._metadata.copy()\n\n        if f_tensor:\n            for k in tensor_attributes:\n                metadata[k] = f_tensor(metadata[k], *args, **kwargs)\n\n        if f_embedding and metadata[\"embedding\"]:\n            metadata[\"embedding\"] = f_embedding(metadata[\"embedding\"], *args, **kwargs)\n\n        if f_scalar:\n            for k in scalar_attributes:\n                metadata[k] = f_scalar(metadata[k], *args, **kwargs)\n\n        return metadata\n\n    def __repr__(self):\n        return str(self.sum().content)\n\n    def __str__(self):\n        return self._str_formatter(self)\n\n    def inspect(self):\n        details = np.vectorize(lambda x: x.inspect())(self.content)\n        return details\n\n    def __eq__(self, other):\n        return self.content == other.content if isinstance(other, TextTensor) else self.content == other\n\n    def sum(self, dim=None):\n        if len(self.shape) == 0:\n            return self\n        if dim is None:\n            result = self\n            connecting_char = [\" \"] + [\"\\n\" * (i + 1) for i in range(len(self.content.shape) - 1)]\n            for i, char in enumerate(connecting_char):\n                result = result.join_on(char, dim=-1)\n            assert len(result.shape) &lt;= 1\n        else:\n            result = self.join_on(\"\", dim=dim)\n        return result\n\n\n    def join_with(self, delimiter=\" \", dim=None):\n        return JoinTextTensor.apply(self, delimiter, dim)\n\n    join = join_with  # Alias\n\n    def format(self, **kwargs):\n        \"\"\"\n        Return a formatted version of self, using substitutions from kwargs.\n        The substitutions are identified by braces ('{' and '}').\n        \"\"\"\n        return FormatTextTensor.apply(self, kwargs)\n\n    def to_list(self):\n        return [self.text_class(m, parse = False) for m in self.content.flat]\n\n    def __pow__(self, power):\n        if power == -1:\n            self_copy = self.copy()\n            self_copy.inv()\n        else:\n            raise ValueError(\"Only power of -1 is supported for inversion.\")\n        return self_copy\n\n\n    def add__(self, other):\n        if isinstance(other, str) or isinstance(other, np.ndarray):\n            try:\n                other = TextTensor(other, parse = False) # TODO Parsing rules\n            except ParseException:\n                other = TextTensor(other, parse = False)\n        result = self.__class__(self.content + other.content, parse = False)\n        return result\n\n    def mul__(self, other):\n        if isinstance(other, str) or isinstance(other, np.ndarray):\n            try:\n                other = TextTensor(other, parse = False) # TODO parsing rules\n            except ParseException:\n                other = TextTensor(other, parse = False)\n        result = self.__class__(self.content * other.content, parse = False)\n        return result\n\n    def __add__(self,other):\n        if isinstance(other, str) or isinstance(other, np.ndarray):\n            try:\n                other = TextTensor(other, parse = False) # TODO parsing rules\n            except ParseException:\n                other = TextTensor(other, parse = False)\n        return AddTextTensor.apply(self, other)\n\n    def __mul__(self,other):\n        if isinstance(other, str) or isinstance(other, np.ndarray):\n            try:\n                other = TextTensor(other, parse = False) # TODO parsing rules\n            except ParseException:\n                other = TextTensor(other, parse = False)\n        return MulTextTensor.apply(self, other)\n\n    def __matmul__(self, other):\n        if isinstance(other, str) or isinstance(other, np.ndarray):\n            try:\n                other = TextTensor(other) #To parse or not to parse?\n            except ParseException:\n                other = TextTensor(other, parse = False)\n        result = self.__class__(self.content @ other.content, parse = False)\n        return result\n\n    def item(self):\n        return next(self.content.flat)\n\n    def items(self):\n        \"\"\"Get (key, value) pairs from all Text entries in tensors\"\"\"\n        return np.array([{\"items\": tuple(t.items())} for t in self.content.flat], dtype = object).reshape(self.content.shape)\n\n    def __getattr__(self, name):\n        try:\n            metadata = object.__getattribute__(self, \"_metadata\")\n        except AttributeError:\n            raise AttributeError(\"'TextTensor' object has no attribute 'metadata'. TextTensor was probably initialised outside of langtorch, e.g. when passing TextTensor to a torch function that expects torch.Tensor\")\n        if name in [\"_content\", \"content\"] or name in metadata:\n            if name in metadata:\n                return metadata[name]\n            try:\n                return object.__getattribute__(self, name)\n            except AttributeError:\n                return None\n        try:\n            return object.__getattribute__(self, name)\n        except AttributeError:\n            # Query np.ndarray attrs\n            try:\n                assert self.content is not None\n                return getattr(self._content, name)\n            except AttributeError:\n                # Query Text attrs\n\n                try:\n                    attr = np.array(np.vectorize(lambda obj: getattr(obj, name))(self._content), dtype=object)\n                    if callable(attr.flat[0]):\n                        return lambda: np.vectorize(lambda x: x())(attr)\n                    else:\n                        return attr\n                except:\n                    raise AttributeError(f\"Object {self}, Neither TextTensor nor Text has attribute '{name}'\")\n\n    def __getitem__(self, index):\n        return self.__class__(metadata=self.metadata_index(index), parse = False)\n\n    def __iter__(self):\n        \"\"\" For unpacking using ** into transformer models\"\"\"\n        return iter(self.content.flat)\n\n    def __contains__(self, item):\n        return item in self.content\n\n    @property\n    def TT(self):\n        from .tt import TextModule\n        return TextModule(self.mT)\n\n    @property\n    def T(self):  # diff!\n        return self.__class__(self.content.T, super().mT, parse = False)\n\n    @property\n    def mT(self):  # diff!\n        return self.__class__(self.content.T, super().mT, parse = False)\n\n    def embed(self, verbose = False):\n        self.embedding = get_embedding([str(m) for m in self.content.flat], verbose = verbose)[\"embedding\"].to_list()\n\n    def apply(self, func):\n        \"\"\"Applies a function to each entry of self.content.\"\"\"\n        # Ensure the function is callable\n        if not callable(func):\n            raise ValueError(\"Provided function is not callable.\")\n\n        # Apply the function to each entry\n        for index, text_entry in np.ndenumerate(self.content):\n            self.content[index] = func(text_entry)\n\n        return self\n\n    def reshape(self, *shape):\n        return ReshapeTextTensor.apply(self, shape)[0]\n\n    def usqueeze(tensor, dim=0):\n        return tensor.reshape(tensor.shape[:dim] + [1] + tensor.shape[dim:])\n\n    def squeeze(tensor, dim=None):\n        if dim is None:\n            # Remove all dimensions of size 1\n            return tensor.view([dim for dim in tensor.shape if dim != 1])\n        else:\n            if dim &lt; 0:\n                # Convert negative dimension to positive\n                dim = len(tensor.shape) + dim\n            # Remove only the specified dimension if it is of size 1\n            if tensor.shape[dim] == 1:\n                return tensor.view(tensor.shape[:dim] + tensor.shape[dim + 1:])\n            else:\n                return tensor\n\n    def expand(tensor, *sizes):\n        # Check if the number of dimensions of the tensors is less than the length of the target sizes\n        if tensor.dim() &lt; len(sizes):\n            # Add singleton dimensions to the front of the tensors\n            for _ in range(len(sizes) - tensor.dim()):\n                tensor = tensor.unsqueeze(0)\n\n        # Prepare a list to hold the expanded sizes\n        expanded_sizes = []\n\n        # Loop over the target sizes from last to first\n        for tensor_size, target_size in zip(reversed(tensor.shape), reversed(sizes)):\n            if tensor_size == 1:\n                # If the size of the tensors in this dimension is 1, it can be expanded\n                expanded_sizes.append(target_size)\n            elif tensor_size != target_size:\n                # If the size of the tensors in this dimension is not equal to the target size,\n                # and is not 1, it cannot be expanded\n                raise ValueError(f\"size mismatch for dimension {len(expanded_sizes)}, {tensor_size} != {target_size}\")\n            else:\n                # If the size of the tensors in this dimension is equal to the target size,\n                # it doesn't need to be expanded\n                expanded_sizes.append(tensor_size)\n\n        # Reverse the list of expanded sizes to match the original order of dimensions\n        expanded_sizes = list(reversed(expanded_sizes))\n\n        # Use the repeat method to create a new tensors that repeats the original tensors along each dimension\n        return tensor.repeat(*expanded_sizes)\n\n    def swapaxes(self, axis1: int, axis2: int):\n        # Create a list of axes in order\n        axes = list(range(len(self.shape)))\n        # Swap the specified axes\n        axes[axis1], axes[axis2] = axes[axis2], axes[axis1]\n        # Return the tensors with axes swapped\n        return self.permute(axes)\n\n    def permute(self, axes):\n        return PermuteTextTensor.apply(self, axes)\n\n    def view(self, *shape):\n        return self.__class__(\n            metadata=self.metadata_apply(lambda v: v.reshape(*shape), lambda v: v.reshape(*shape, v.shape[-1])),\n            parse = False)\n\n    def repeat(tensor, *sizes):\n        # Ensure the number of dimensions of tensors and sizes match\n        if len(tensor.shape) != len(sizes):\n            raise ValueError(\"Number of dimensions of tensors and sizes must match\")\n\n        # Manually create a repeated tensors\n        content = tensor.content\n        for dim, size in enumerate(sizes):\n            slices = [content] * size\n            content = np.concatenate(slices, axis=dim)\n        tensor = tensor.__class__(content, parse = False)\n        return tensor\n\n    def split(self, sep, dim=0):\n        \"\"\"\n        Return a new TextTensor, with an additional first dimension to split everything using sep as the delimiter.\n\n          sep\n            The delimiter according which to split the bytearray.\n            None (the default value) means split on ASCII whitespace characters\n            (space, tab, return, newline, formfeed, vertical tab).\n        \"\"\"\n        return SplitTextTensor.apply(self, sep, dim)\n\n    def format(self, **kwargs):\n        \"\"\"\n        Return a formatted version of self, using substitutions from args and kwargs.\n        The substitutions are identified by braces ('{' and '}').\n        \"\"\"\n        return FormatTextTensor.apply(self, kwargs)\n\n    def save(self, filename = \"saved_tensors.pt\"):\n        torch.save(self, filename)\n\n    def to_csv(self, filename, sep=\"\\t\"):\n        \"\"\"\n        Save a csv with texts or, if available, with embeddings (flattened shape)\n        \"\"\"\n        if self._embedding is None:\n            self.save(filename, sep=sep)  # Tab as default, because texts have a lot of commas\n        else:\n            try:\n                with open(filename, 'w', newline='') as f:\n                    for row in self._content:\n                        # Join the elements in the row with commas\n                        line = sep.join(row)\n                        # Write the line to the file\n                        f.write(line + '\\n')\n            except Exception as E:\n                raise Exception(f\"Failed to save with embeddings, {E}\")\n\n    def backward(\n            tensors: _TensorOrTensors,\n            grad_tensors: Optional[_TensorOrTensors] = None,\n            retain_graph: Optional[bool] = None,\n            create_graph: bool = False,\n            grad_variables: Optional[_TensorOrTensors] = None,\n            inputs: Optional[_TensorOrTensors] = None,\n    ) -&gt; None:\n        r\"\"\"See. docs\"\"\"\n        if torch._C._are_functorch_transforms_active():\n            raise RuntimeError(\n                \"backward() called inside a functorch transform. This is not \"\n                \"supported, please use functorch.grad or functorch.vjp instead \"\n                \"or call backward() outside of functorch transforms.\")\n\n        if grad_variables is not None:\n            if grad_tensors is None:\n                grad_tensors = grad_variables\n            else:\n                raise RuntimeError(\"'grad_tensors' and 'grad_variables' (deprecated) \"\n                                   \"arguments both passed to backward(). Please only \"\n                                   \"use 'grad_tensors'.\")\n        if inputs is not None and len(inputs) == 0:\n            raise RuntimeError(\"'inputs' argument to backward() cannot be empty.\")\n\n        if grad_tensors is None:\n            grad_tensors = utils.zeros_like(tensors).reshape(tensors.shape)\n        elif isinstance(grad_tensors, str):\n            grad_tensors = (TextTensor(grad_tensors),)\n\n        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)\n        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else \\\n            tuple(inputs) if inputs is not None else tuple()\n        grad_tensors_ = langtorch.torch_utils.tensor_or_tensors_to_tuple(grad_tensors, len(tensors))\n        grad_tensors_ = make_grads(tensors, grad_tensors_, is_grads_batched=False)\n        if retain_graph is None:\n            retain_graph = create_graph\n\n        # The reason we repeat same the comment below is that\n        # some Python versions print out the first line of a multi-line function\n        # calls in the traceback and some print out the last line\n        langtorch.autograd.run_backward(  # langtorch version of a C++ engine that torch uses to run the backward pass\n            tensors, grad_tensors_, retain_graph, create_graph, inputs,\n            allow_unreachable=True,\n            accumulate_grad=True)  # langtorch version of a C++ engine that torch uses to run the backward pass\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.__iter__","title":"<code>__iter__()</code>","text":"<p>For unpacking using ** into transformer models</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def __iter__(self):\n    \"\"\" For unpacking using ** into transformer models\"\"\"\n    return iter(self.content.flat)\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.apply","title":"<code>apply(func)</code>","text":"<p>Applies a function to each entry of self.content.</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def apply(self, func):\n    \"\"\"Applies a function to each entry of self.content.\"\"\"\n    # Ensure the function is callable\n    if not callable(func):\n        raise ValueError(\"Provided function is not callable.\")\n\n    # Apply the function to each entry\n    for index, text_entry in np.ndenumerate(self.content):\n        self.content[index] = func(text_entry)\n\n    return self\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.backward","title":"<code>backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)</code>","text":"<p>See. docs</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def backward(\n        tensors: _TensorOrTensors,\n        grad_tensors: Optional[_TensorOrTensors] = None,\n        retain_graph: Optional[bool] = None,\n        create_graph: bool = False,\n        grad_variables: Optional[_TensorOrTensors] = None,\n        inputs: Optional[_TensorOrTensors] = None,\n) -&gt; None:\n    r\"\"\"See. docs\"\"\"\n    if torch._C._are_functorch_transforms_active():\n        raise RuntimeError(\n            \"backward() called inside a functorch transform. This is not \"\n            \"supported, please use functorch.grad or functorch.vjp instead \"\n            \"or call backward() outside of functorch transforms.\")\n\n    if grad_variables is not None:\n        if grad_tensors is None:\n            grad_tensors = grad_variables\n        else:\n            raise RuntimeError(\"'grad_tensors' and 'grad_variables' (deprecated) \"\n                               \"arguments both passed to backward(). Please only \"\n                               \"use 'grad_tensors'.\")\n    if inputs is not None and len(inputs) == 0:\n        raise RuntimeError(\"'inputs' argument to backward() cannot be empty.\")\n\n    if grad_tensors is None:\n        grad_tensors = utils.zeros_like(tensors).reshape(tensors.shape)\n    elif isinstance(grad_tensors, str):\n        grad_tensors = (TextTensor(grad_tensors),)\n\n    tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)\n    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else \\\n        tuple(inputs) if inputs is not None else tuple()\n    grad_tensors_ = langtorch.torch_utils.tensor_or_tensors_to_tuple(grad_tensors, len(tensors))\n    grad_tensors_ = make_grads(tensors, grad_tensors_, is_grads_batched=False)\n    if retain_graph is None:\n        retain_graph = create_graph\n\n    # The reason we repeat same the comment below is that\n    # some Python versions print out the first line of a multi-line function\n    # calls in the traceback and some print out the last line\n    langtorch.autograd.run_backward(  # langtorch version of a C++ engine that torch uses to run the backward pass\n        tensors, grad_tensors_, retain_graph, create_graph, inputs,\n        allow_unreachable=True,\n        accumulate_grad=True)  # langtorch version of a C++ engine that torch uses to run the backward pass\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.format","title":"<code>format(**kwargs)</code>","text":"<p>Return a formatted version of self, using substitutions from args and kwargs. The substitutions are identified by braces ('{' and '}').</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def format(self, **kwargs):\n    \"\"\"\n    Return a formatted version of self, using substitutions from args and kwargs.\n    The substitutions are identified by braces ('{' and '}').\n    \"\"\"\n    return FormatTextTensor.apply(self, kwargs)\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.items","title":"<code>items()</code>","text":"<p>Get (key, value) pairs from all Text entries in tensors</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def items(self):\n    \"\"\"Get (key, value) pairs from all Text entries in tensors\"\"\"\n    return np.array([{\"items\": tuple(t.items())} for t in self.content.flat], dtype = object).reshape(self.content.shape)\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.metadata_apply","title":"<code>metadata_apply(f_tensor=None, f_embedding=None, f_scalar=None, *args, **kwargs)</code>","text":"<p>Apply a specified function to a copy of metadata elements of the TextTensor.</p> <p>Parameters: - f_tensor (function, optional): Function to be applied to tensors-shaped metadata attributes. - f_embedding (function, optional): Function to be applied to the 'embedding' attribute. - f_scalar (function, optional): Function to be applied to scalar metadata attributes. - args: Variable length argument list to be passed to the functions. - *kwargs: Arbitrary keyword arguments to be passed to the functions.</p> <p>Returns: - dict: A dictionary with updated metadata attributes.</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def metadata_apply(self, f_tensor=None, f_embedding=None, f_scalar=None, *args, **kwargs):\n    \"\"\"\n    Apply a specified function to a copy of metadata elements of the TextTensor.\n\n    Parameters:\n    - f_tensor (function, optional): Function to be applied to tensors-shaped metadata attributes.\n    - f_embedding (function, optional): Function to be applied to the 'embedding' attribute.\n    - f_scalar (function, optional): Function to be applied to scalar metadata attributes.\n    - *args: Variable length argument list to be passed to the functions.\n    - **kwargs: Arbitrary keyword arguments to be passed to the functions.\n\n    Returns:\n    - dict: A dictionary with updated metadata attributes.\n    \"\"\"\n    tensor_attributes, scalar_attributes = [], []\n    for k, v in self._metadata.items():\n        try:\n            if tuple(v.shape) == tuple(self.shape):\n                tensor_attributes.append(k)\n        except AttributeError:\n            if k != \"embedding\":\n                scalar_attributes.append(k)\n\n    metadata = self._metadata.copy()\n\n    if f_tensor:\n        for k in tensor_attributes:\n            metadata[k] = f_tensor(metadata[k], *args, **kwargs)\n\n    if f_embedding and metadata[\"embedding\"]:\n        metadata[\"embedding\"] = f_embedding(metadata[\"embedding\"], *args, **kwargs)\n\n    if f_scalar:\n        for k in scalar_attributes:\n            metadata[k] = f_scalar(metadata[k], *args, **kwargs)\n\n    return metadata\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.metadata_index","title":"<code>metadata_index(index)</code>","text":"<p>Pick out entries of metadata with the provided index. Usually used to transfer metadata to a sub-tensors.</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def metadata_index(self, index):\n    \"\"\"Pick out entries of metadata with the provided index. Usually used to transfer metadata to a sub-tensors.\"\"\"\n\n\n    index = utils.positive_indices(index, self.content.shape)\n    index = index[0] if len(index) == 1 else index\n\n    if len(self.content.shape) == 0: # and (index == slice(None, None, None))\n        return self.metadata_apply(lambda v: v, lambda v: v, lambda v: v)\n\n    else:\n        return self.metadata_apply(lambda v: v[index], lambda v: v[index], lambda v: v)\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.split","title":"<code>split(sep, dim=0)</code>","text":"<p>Return a new TextTensor, with an additional first dimension to split everything using sep as the delimiter.</p> <p>sep     The delimiter according which to split the bytearray.     None (the default value) means split on ASCII whitespace characters     (space, tab, return, newline, formfeed, vertical tab).</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def split(self, sep, dim=0):\n    \"\"\"\n    Return a new TextTensor, with an additional first dimension to split everything using sep as the delimiter.\n\n      sep\n        The delimiter according which to split the bytearray.\n        None (the default value) means split on ASCII whitespace characters\n        (space, tab, return, newline, formfeed, vertical tab).\n    \"\"\"\n    return SplitTextTensor.apply(self, sep, dim)\n</code></pre>"},{"location":"reference/texttensor/#langtorch.TextTensor.to_csv","title":"<code>to_csv(filename, sep='\\t')</code>","text":"<p>Save a csv with texts or, if available, with embeddings (flattened shape)</p> Source code in <code>langtorch\\tensors\\texttensor.py</code> <pre><code>def to_csv(self, filename, sep=\"\\t\"):\n    \"\"\"\n    Save a csv with texts or, if available, with embeddings (flattened shape)\n    \"\"\"\n    if self._embedding is None:\n        self.save(filename, sep=sep)  # Tab as default, because texts have a lot of commas\n    else:\n        try:\n            with open(filename, 'w', newline='') as f:\n                for row in self._content:\n                    # Join the elements in the row with commas\n                    line = sep.join(row)\n                    # Write the line to the file\n                    f.write(line + '\\n')\n        except Exception as E:\n            raise Exception(f\"Failed to save with embeddings, {E}\")\n</code></pre>"}]}