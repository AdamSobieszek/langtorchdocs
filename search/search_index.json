{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"<p>LangTorch is a framework that accelerates development of complex language model applications by leveraging familiar PyTorch concepts.</p> <p>While existing frameworks focus on connecting language models to other services, LangTorch aims to change the way you approach creating LLM applications by introducing system of simple rules that unifies working with texts, chats, markup languages, prompt templates and techniques, tokens, embeddings, LLM calls and tools with seamless parallelization working with OpenAI APIs and integrated with PyTorch. The result? Less code, faster development and more intricate LLM applications.</p>"},{"location":"#the-langtorch-framework","title":"The LangTorch Framework","text":"<p>Instead of providing wrapper classes for users to memorize, LangTorch introduces key objects that while governed by few simple rules, enable all kinds of text formatting and LLM operations. This lets developers think about what they want to build, instead of how the classes were named. LangTorch components subclass their numerical PyTorch counterparts, which lets users apply their existing coding skills to building novel LLM app architectures. The design of the package is geared towards users that want to test and create new architectures or methods that increase the utility of LLMs, rather than streamlining the most common chat and RAG architectures.</p> <p>Note the package is early in its development and some features may be unstable or unfinished!  Use for research rather than production purposes</p> TextTensors        inherits functionalities from <code>torch.Tensor</code>, but holds structured text as entries. <code>TextTensors</code> allow us to structure text information geometrically and dynamically inject this data into LLM calls.The utility of Tensors, as used in Torch, relies on their ability to calculate simultaneously products of several weights. The corresponding feature in TextTorch allows several prompts to be formatted on several inputs, by defining the product of <code>text1</code> and <code>text2</code> similarly to <code>text1.format(text2)</code>.       <pre><code>prompt_templates = TextTensor(\n                   [[\"{name}: {greeting}!\"],  \n                    [\"{greeting}, {name}!\"]]\n                   )\n\nx = prompt_templates * TextTensor(\n                    {\"greeting\": \"Hello\", \"name\": \"Alice\"}\n                    ) \nprint(x)\n</code></pre> Output:<pre><code>[[Alice: Hello!]\n [Hello, Alice!]]\n</code></pre> <pre><code>from langtorch import TextModule\n\nprompts = [[\"Calculate this equation:\\n\"], \n            [\"Write this equation in latex:\\n\"]]\nparallel_tasks= TextModule(prompts, activation=\"gpt-4\")\n\nparallel_tasks( Te xtTensor([\"sin(pi)* 10 =\", \"12**2+36 =\"]) )\n</code></pre> Supporting PyTorch utilities   LangTorch implements textual versions of many PyTorch utilities. TextTensors can be accessed, reshaped, squeezed, unsqueezed and so on just like torch tensors. Moreover, using regular torch functions they can be summed (text join), stacked, concatenated, used in tensor datasets, and even traced with autograd. In the near future this ability will be used in automatic prompt optimization given a text description of our \"loss function\".        Using tensor utilitiesUsing PyTorch DatasetsSomething else 2 (autograd?) <p> <pre><code>from torch.utils.data import DataLoader, TensorDataset\n\ninput_data, target_data = TextTensor(df[\"task\"]), TextTensor(df[\"answer\"])\ndataset = TensorDataset(input_data, target_data)\n\n# Simply select batch size for llm calls\ndataloader = DataLoader(dataset, batch_size=16)\n\nllm_call = TextModule(\"Task: \", activation=\"gpt-4\")\n\nfor i, (inputs, targets) in enumerate(dataloader):\n    outputs = llm_call(inputs)\n</code></pre> </p> <p> <pre><code>from torch.utils.data import DataLoader, TensorDataset\n\ninput_data, target_data = TextTensor(df[\"task\"]), TextTensor(df[\"answer\"])\ndataset = TensorDataset(input_data, target_data)\n\n# Simply select batch size for llm calls\ndataloader = DataLoader(dataset, batch_size=16)\n\nllm_call = TextModule(\"Task: \", activation=\"gpt-4\")\n\nfor i, (inputs, targets) in enumerate(dataloader):\n    outputs = llm_call(inputs)\n</code></pre> </p> <p> <pre><code>import torch.CosineSimilarity\n\ntensor1 =[[\"Yes\"], [\"No\"]],\ntensor 2 = [\"1\", \"0\", \"Nay\", \"Si\"]\n\ntorch.CosineSimilarity(tensor1,tensor)\n</code></pre> </p> <pre><code>import torch  \n\ntensor1 =TextTensor([[[\"Yes\"], [\"No\"]]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1,tensor2)\n</code></pre> Text, Tokens and Embeddings   Apart from streamlining work on texts, <code>TextTensors</code> can  hold multiple types of representations of the same text  and automatically switch between acting as strings when printed, as embeddings when inputted into <code>torch.cosine_similarity</code> and as tokens when passed to a local LLM.       Richer text representations   Every text entry in LangTorch has a set structure, such that they can be constructed from and formatted into any markup language. This unified structure allows for a systematic treatment of diverse objects like chat histories, chunked documents, document stores, dictionaries, text meta-data, extracted named entities and code.       <pre><code>\n</code></pre>"},{"location":"#textmodule","title":"TextModules","text":"are the compositional building blocks. A subclass of <code>torch.nn.Module</code>, <code>TextModules</code>  are reusable \"layers\", whose weights are <code>TextTensors</code>, and which instead of an activation function can include the activation of an LLM call on the dynamically created text inputs.Instead of many clunky retrievers, chains, chats or agent, all can be defined as TextModules with different forward calls. These can then be easily included as submodules in complex modules that in the forward pass perform any parts of common architectures, like operations on embeddings, retrieval, parallel LLM API calls, batched local LLM inference, actions and so on."},{"location":"#dive-in-and-get-started","title":"Dive In and Get Started","text":"<p>Install LangTorch with: <pre><code>pip install langtorch\n</code></pre></p> <p>Next steps:</p> <ul> <li> <p> Quick-Start Tutorial</p> <p>Get building with LangTorch in 5 minutes</p> <p> Getting started</p> </li> <li> <p> Reference</p> <p>Learn how to use <code>Text</code> objects, <code>TextTensors</code>, <code>TextModules</code>, <code>Activations</code>  and <code>torch</code> functions with LangTorch</p> <p> Reference.</p> </li> <li> <p> Discord</p> <p>Join the Discord community to for fast bug fixes and support!</p> <p> Discord</p> </li> <li> <p> Github</p> <p> Star </p> <p> GitHub Repo</p> </li> </ul> <p>Join us in improving the LLM application dev experience!</p>"},{"location":"#about","title":"About","text":"<p>LangTorch is an open source python package by Adam Sobieszek (University of Warsaw; Jutro Medical). Contributions, comments and issue submissions are very much welcomed.</p> <p>Contact: contact@langtorch.org</p> <p>I'd like to thank Tadeusz Price, Hubert Plisiecki, Jakub Podolak and Miko\u0142aj Boro\u0144ski for their help in testing and explaining the package.</p>"},{"location":"quickstart/","title":"Quickstart Guide: Dive into LangTorch","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>LangTorch works with Python 3.8 or higher. To install LangTorch using pip:</p> <pre><code>pip install langtorch\n</code></pre>"},{"location":"quickstart/#simple-chains","title":"Simple chains","text":""},{"location":"quickstart/#getting-tensor-text-data","title":"Getting tensor text data","text":"<p><code>TextTensors</code> are designed to simplify simultaneous handling of text data. They inherit most functionality from PyTorch's <code>torch.Tensor</code> but hold textual data, which you may provide from any source.</p> <pre><code>from langtorch import TextTensor\n\ntt = TextTensor([[\"prompt1\"], [\"prompt2\"]])\n\nlines = TextTensor(open('doc.txt','r').readlines())\n\n# Append to every entry in lines\nlines = lines + \" Have a great day!\"\n</code></pre> <p>Each entry of TextTensor is a Text class, that  %%  Examples of <code>TextTensor</code> you may want to use are  %%</p>"},{"location":"quickstart/#texttensor-operations-for-prompt-templating","title":"TextTensor operations for prompt templating","text":"<p><code>TextTensors</code> provide operations that allow for formatting and editing many entries at the same time according to array broadcasting rules. Adding a <code>TextTensors</code> appends its content to the other, while multiplication performs a more complex operation that can be used for template formatting:</p> <pre><code>completions = TextTensor([[{\"name\": \"Luciano\"}],\n                          [{\"name\": \"Massimo\"}]])\n\n# Template formatting\nformatted_tt = TextTensor([\"Hello, {name}!\"]) * \n</code></pre>"},{"location":"quickstart/#performing-a-task-with-textmodules","title":"Performing a task with TextModules","text":"<p>TextModules like <code>nn.Module</code> implement a forward method that works on (text) tensors. By default they can be initialized by passing a TextTensor of prompts, that in the forward pass will be formatted using the input TextTensor (just like in the example above).  </p> <p>However, as with nn.Modules, to achieve interesting behavior the multiplied tensors need to be passed to an \"activation function,\" which in this context is an LLM call on the formatted prompts.</p> <pre><code>from langtorch import TextModule, OpenAI\n\nllm = OpenAI(\"gpt4\", T = 0.) # Pass any API kwargs here to customize the call \n# Initialize TextModule with the activation\ntext_mod = TextModule(\"Translate this text to English: {}\", activation=llm)\n</code></pre>"},{"location":"quickstart/#parallel-and-chained-calls-with-textmodules","title":"Parallel and Chained calls with TextModules","text":"<p>LangTorch uses a custom implementation to speed up and cache api calls, that by default run in parallel for all TextTensor entries passed to an LLM activation. As such, running calls in parallel is done automatically if either multiple prompts, multiple input values or both are passed to an LLM. </p> <p>To create complex chains, you may, as in torch, define a module subclass that adds custom behavior or combines many submodules. The simplest way to chain TextModule is to directly use <code>torch.nn.Sequential</code>.</p> <pre><code>import torch\n\n# Custom TextModule override methods like __init__ and forward\nclass UsefulTextModule(TextModule):\n    def forward(self, input):\n        return super()(input + \" Please give me the wrong answer.\")\n\n\n# Create a sequential TextModule\npipeline = torch.nn.Sequential(\n    TextModule(\"Translate this text to English: {}\", activation=OpenAI()),\n    TextModule(\"Summarize the translated text: {}\", activation=OpenAI())\n)\n</code></pre> <p>Execute the pipeline by simply passing your TextTensor:</p> <pre><code>output = pipeline(tt)\n</code></pre>"},{"location":"quickstart/#implementing-popular-methods","title":"Implementing popular methods","text":""},{"location":"quickstart/#building-a-retriever","title":"Building a retriever","text":"<ul> <li>Cosine Similarities: to compute similarities between TextTensors.</li> </ul> <pre><code>import torch  \n\ntensor1 =TextTensor([[[\"Yes\"], [\"No\"]]])  \ntensor2 = TextTensor([\"Yeah\", \"Nope\", \"Yup\", \"Non\"])  \n\ntorch.cosine_similarity(tensor1,tensor2)\n</code></pre> <p>TODO retriever</p> <p>Using these </p> <pre><code>class Retriever(TextModule):  \n    def __init__(self, documents: TextTensor):  \n        super().__init__()  \n        self.documents = TextTensor(documents).view(-1)  \n\n    def forward(self, query: TextTensor, k: int = 5):  \n        cos_sim = torch.cosine_similarity(self.documents, query.reshape(1))  \n        return self.documents[cos_sim.topk(k)]\n\nretriever = Retriever(open(\"doc.txt\", \"r\").readlines())\nquery = TextTensor(\"How to build a retriever?\")\n\nprint(retriever(query))\n</code></pre> <p>Note how didn't require us learning about any new operation we would not find in regular PyTorch. This is why .. and why LangTorch does not pre-define such modules in the main package.</p> <p>We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:</p> <pre><code>class RAG(TextModule):  \n    def __init__(self, documents: TextTensor, *args, **kwargs):  \n        super().__init__(*args, **kwargs)  \n        self.retriever = Retriever(documents)  \n\n    def forward(self, user_message: TextTensor, k: int = 5):  \n        retrieved_context = self.retriever(user_message, k) +\"\\n\"  \n        user_message = user_message + \"\\nCONTEXT:\\n\" + retrieved_context.sum()  \n        return super().forward(user_message)\n\nrag_chat = RAG(paragraphs,  \n               prompt=\"Use the context to answer the following user query: \", \n               activation=OpenAI(\"gpt-3.5-turbo\"))\n</code></pre> <p>With only small modifications this module could also perform batched inference \u2014 performing multiple simultaneous queries without much additional latency.</p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>For a deeper understanding, explore the reference documentation for the langtorch functions and classes. For online support, bug reports and feature request join our discord or post on the GitHub Repo.</p>"},{"location":"reference/","title":"Documentation","text":"<ul> <li> <p> Torch Functions</p> <p>LangTorch integrates with many Torch functions, allowing <code>TextTensors</code> and <code>TextModules</code> to utilize PyTorch utilities and autograd.</p> <p> Torch Functions</p> </li> <li> <p> Text Objects</p> <p><code>Text</code> objects are the structured (markup) text entries used in TextTensors and Modules. They enable the formatting multiplication operation in TextTensors and provide support for convenient workflow with markup languages like markdown.</p> <p> Text Objects</p> </li> <li> <p> TextTensors</p> <p><code>TextTensors</code> is the main data structure for prompt templating, embeddings and tokens, over which we perform LLM calls.</p> <p> TextTensors</p> </li> <li> <p> TextModules</p> <p><code>TextModules</code> are the building blocks for creating sequences of operations on text data, including both operations on text and embeddings. Their use is governed by similar rules as PyTorch <code>nn.Modules</code> with composability and support for torch utilities.</p> <p> TextModules</p> </li> <li> <p> Activations</p> <p><code>Activations</code> are <code>TextModules</code> used to call LLMs on the input <code>TextTensor</code> entries. Currently they support convenient parallel calls to OpenAI, but a custom implementation can be used for local models and other APIs.</p> <p> Activations</p> </li> </ul>"},{"location":"reference/activation/","title":"<code>class</code> langtorch.Activation","text":"<p>Continuing the analogy with neural networks, a sequence of TextModule would not perform complex operations if we did not add a \"activation function\". In LangTorch such activations are LLM calls, done in parallel for each entry of the processed TextTensor. Activation by default supports any OpenAI model and its parameters:</p> <pre><code># write simple example\n</code></pre> <p>LangTorch Activations were designed to streamline . Apart from working on TextTensors, they cache results of deterministic calls, skip redundant parallel calls and speed up inference.</p> <p>Activations are TextModules, but are more commonly used in conjunction with a TextModule to perform a two-step operation of formatting prompts and getting completions for them. </p>"},{"location":"reference/langtorch/","title":"Index","text":"<p>LangTorch implements many of the same utility functions one can use in Torch. To streamline usage, these are also automatically invoked if one passes TextTensor inputs to the analogous torch function. </p> <p>Functions, that take in TextTensors and output TextTensors (note, functions like <code>reshape</code> are also TextTensor methods):</p> Operation Description stack Concatenates a sequence of TextTensors along a new dimension. cat Concatenates the given sequence of <code>seq</code> TextTensors in the given dimension. reshape Returns a TextTensor with the same data and number of elements as the input, but with the specified shape. squeeze Returns a TextTensor with all the dimensions of input of size 1 removed. unsqueeze Returns a TextTensor with a dimension of size one inserted at the specified position. swapaxes Swaps two axes of the TextTensor."},{"location":"reference/langtorch/#semantic-algebra-vs-tensor-algebra","title":"Semantic algebra vs Tensor algebra","text":"<p>One of the benefits of representing information geometrically in tensors is that we can have meaningful dimensions, that differentiate e.g. different answers to the same query. In such cases, often we wish to perform some action along this dimension. For the dimension of different answers we may want to synthesize the answers into one better answer, or pick out the best one. </p> <p>What LangTorch proposes is that we may think of such operations as taking the (semantic) mean and max value along that dimension. In another use cases the mean may be used to average the embeddings along a dimension. To enable both, the following function operate as numerical operations on embeddings if imported from torch, and as \"semantic\" functions that call an LLM to perform the analogous operation on texts: </p> Method Description torch.mean langtorch.mean torch.max langtorch.max unsqueeze swapaxes"},{"location":"reference/multiplication/","title":"1. Working with Text objects","text":"<p>The LangTorch <code>Text</code> class allows for complex operations on text objects, mimicking mathematical operations for strings. Here are several examples utilizing the <code>Text</code> multiplication operation for various purposes, based on the guide in my knowledge:</p> <p>1. Formatting Prompts: You can use Text multiplication to format prompts by matching keys in the second <code>Text</code> with values in the first <code>Text</code>. The same operation can be done element-wise by multiplying <code>TextTensor</code> objects.</p> <pre><code>from langtorch import Text, TextTensor  \n\nprompt_template = Text(\"{greeting}, {name}!\")\ninput_values = Text({\"greeting\": \"Hello\", \"name\": \"Alice\"})\nformatted_prompt = prompt_template * input_values \nprint(formatted_prompt)  # Outputs: Hello, Alice!\n\n# Or with TextTensors\n\nprompt_template = TextTensor([prompt_template, prompt_template])\ntext_values = TextTensor(input_values)\nformatted_prompt = prompt_template * input_values \nprint(formatted_prompt)   # Outputs: [Hello, Alice!  Hello, Alice!]\n</code></pre> <p>The next examples similarly are all applicable to <code>TextTensor</code> multiplication.</p> <p>2. Text Representations:  Texts represent textual content with a list of key-value pairs. Unlike dictionaries the same key may be used multiple times, serving as a name or tag for a particular value (the values being the actual substrings that make up the text). The keys are not visible from the string representation of Text, but we can use them to format strings in many ways. The pairs that make up a text object can be access with its .items() method and we can edit these entries using indexing similar to pandas, which uses .iloc and .loc: <pre><code>\n</code></pre></p> <p>3. Text Parsing: When creating a <code>Text</code> object from a string, by default whenever possible the input is parsed with a syntax similar to python's f-strings. In effect, the string is split into substrings with <code>{}</code> symbols being delimiters. This allows us to create a whole structured text object from one string, for example: <pre><code>prompt_template = Text(\"{object} says: {greeting}\")  \nprint(prompt_template.items())  \n# Outputs: [('', 'object'), ('',' says: '), ('', 'greeting')]\n</code></pre> To also set keys using this syntax, you can use pass strings written like this:  <code>\"{value:key}{value1:key1}\"</code>, which parses to <code>[('value','key),('value1','key1')]</code>. Below of supported patterns you can mix together: | pattern | result | | ------- | ------ | |         |        |</p> <ol> <li>The expressiveness of the (key, value) Text representations is based on the fact that we can set as text values another Text object instead of a string, allowing for tree-like structures that can represent e.g. chat templates: <pre><code>chat = Text(\n            ('user', \"Hi\"),\n            ('assistant', \"Hello\"),\n            ('user', 'Can you explain {theory} like im five?'),\n)\nchat.iloc[-1] *= Text({\"theory\": \"critical theory\"})\n</code></pre></li> </ol> <p>To  <pre><code>keyed_text = Text((\"\",\"key\")) * Text(\"key\",\"value\") \nprint(keyed_text)  # Outputs: Text((\"\", \"value\"))`\n</code></pre> if the contents match but the keys don't, a new <code>Text</code> object is created with the concatenated contents. <pre><code>keyed_text = Text((\"\",\"key\")) * Text(\"key\",\"value\") \nprint(keyed_text)  # Outputs: Text((\"\", \"value\"))`\n</code></pre></p> <p>3. Appending Chat Messages: For appending chat messages, you can concatenate <code>Text</code> objects to build up a conversation.</p> <pre><code>`message1 = Text((\"user\", \"How are you?\")) message2 = Text((\"bot\", \"I'm fine, thank you!\")) conversation = message1 + Text.identity + message2 print(conversation)  # Outputs: How are you?I'm fine, thank you!`\n</code></pre> <p>4. Renaming Entries: To rename entries, you can use the inverse operation, which swaps keys and contents, and then perform multiplication to apply the new names.</p> <pre><code>`original_text = Text((\"greeting\", \"Hello\"), (\"object\", \"world\")) renaming_template = Text((\"salutation\", \"greeting\"), (\"entity\", \"object\")).inv() renamed_text = original_text * renaming_template print(renamed_text)  # Should output Text with keys \"salutation\" and \"entity\"`\n</code></pre> <p>5. Combining Multiple Operations: The Text multiplication operation can be extended to combine multiple operations like formatting, adding keys, and appending in one expression. <pre><code>`# Create a conversation with prompts and dynamically insert names conversation_template = TextTensor([     \"{user1} says: {greeting}, {user2}.\",     \"{user2} replies: {reply}\" ]) user_messages = Text((\"greeting\", \"Hello\"), (\"reply\", \"Hi there!\")) user_names = Text((\"user1\", \"Alice\"), (\"user2\", \"Bob\")) formatted_conversation = conversation_template * user_messages * user_names print(formatted_conversation) # Outputs: Alice says: Hello, Bob. Bob replies: Hi there!`\n</code></pre> These examples show how the <code>Text</code> multiplication operation can be used for formatting, concatenating, and structuring text dynamically in LangTorch. Think of the multiplication like this:  </p>"},{"location":"reference/parsing/","title":"Text parsing","text":"<p>As Texts have an internal structure, this structure needs to be  from raw strings. That's why Text and TextTensors by default parse their inputs with an expanded f-string syntax, explained below.</p>"},{"location":"reference/parsing/#chats","title":"Chats","text":"<pre><code>  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n</code></pre>"},{"location":"reference/parsing/#using-text-sensibly","title":"Using Text sensibly","text":"<p>Text is in no way intuitive formally, but it is intuitive if we don\u2019t think about the implementation. To not think about the implementation Text objects can be parsed from regular strings that follow a simple syntax. This chapter will guide you on how to transform raw strings into structured <code>Text</code> objects, enabling a new set of operations beyond traditional strings.</p> <p>Before diving deep, let's get a feel for how simple names strings can be constructed:</p> <pre><code># These are all the ways a str2 object can be defined:\n\"input\" -&gt; Text(\"input\")\n\"{input}\" -&gt; Text(\"input\")\n\"input{:key}\"  -&gt; Text((\"key\", \"input\"))\n\"{input:key}\" -&gt; Text((\"key\", \"input\"))\n# To create an entry without a value or disable the syntax for part of a string you need to use backticks:\n\"{``:key}\"  -&gt; Text((\"key\", \"\"))\n\"`{Text in backticks:is not parsed:}`{:key}\" -&gt; Text((\"key\", \"{Text in backticks:is not parsed:}\"))\n</code></pre> <p>The syntax is similar to f strings where f\u201d{name:qualifiers}\u201d is replaced with \u201c{content:key}\u201d. This convention enables some simple ways to define complex texts:</p>"},{"location":"reference/parsing/#2-constructing-conversations","title":"2. Constructing Conversations:","text":"<p>Using this structure, you can create intuitive conversation-like strings. For example:</p> <pre><code>TextTensor(\"Answer 'orca' to the next question!{:user}Okay{:assistant}Go!{:user}\")\n</code></pre> <p>This results in a structured conversation like:</p> <ul> <li>User: \"Answer 'orca' to the next question!\"</li> <li>Assistant: \"Okay\"</li> <li>User: \"Go!\"</li> </ul>"},{"location":"reference/parsing/#3-power-of-multiplication","title":"3. Power of Multiplication:","text":"<p>LangTorch's parsing mechanism allows you to perform multiplication operations on text, making text manipulations incredibly expressive.</p> <p>For instance:</p> <pre><code>TextTensor(\"This is a text with a {key} that can be replaced with a multiplication\")*TextTensor(\"{value:key}\")\n</code></pre> <p>Produces:</p> <pre><code>\"This is a text with a value that can be replaced with a multiplication\"\n</code></pre> <p>Here's the magic: The content inside <code>{}</code> in the first <code>Text</code> object acts like a placeholder. When multiplied with another <code>Text</code> object that provides a value for that placeholder (in this case, <code>value</code> for <code>key</code>), the result is a merged text with the placeholder replaced by the provided value.</p>"},{"location":"reference/parsing/#4-practical-tips","title":"4. Practical Tips:","text":"<ol> <li>Values as Placeholders: Every substring value can act as a placeholder for a format operation via another \u201c{value:placeholder}\u201d string. They act as positions in the original text where you might want to insert or replace content.</li> <li>Embedding Text: You can embed one piece of text within another using this structure, making it versatile for various text manipulation tasks.</li> <li>Escaping Characters: If you want to use <code>{</code>, <code>}</code>, or <code>:</code> as regular characters without triggering the parsing mechanism, simply enclose the section of the string within backticks (```).</li> </ol>"},{"location":"reference/tensorattrs/","title":"TextTensor Attributes","text":"<p>Each\u00a0<code>TextTensor</code>\u00a0has 6 main attributes, two per each representation: texts (<code>content</code> and <code>ttype</code>), embeddings (<code>embedding</code> and <code>embedding_model</code>) and tokens (<code>tokens</code> and <code>tokenizer</code>). </p> <p>The <code>content</code> attribute is a numpy array with the textual entries. The <code>ttype</code> is the \"text type\" of these entries, which is any subclass of the <code>Text</code> class.</p> <p>The <code>embedding</code> holds a <code>torch.tensor</code> representation of the text entries, computed using the model specified in the <code>embedding_model</code> (name of OpenAI model or local Module).</p> <p><code>tokens</code> are the <code>torch.Tensor</code> tokenized text entries from <code>content</code>. While embeddings by default are computed using the \"text-embedding-3-small\" mode, to tokenize content the <code>tokenizer</code> attribute has to be set to a tokenizer from the Transformers library.</p>"},{"location":"reference/tensorattrs/#setting-attributes","title":"Setting attributes","text":"<p>The <code>content</code> is always set, as this is what we initialize a <code>TextTensor</code> on.</p> <p>The <code>embedding</code> and <code>tokens</code> are not calculated upon initialization to save costs. They are automatically computed directly before an operation that requires them or can be invoked manually with <code>.embed()</code> and <code>.tokenize()</code> respectively.</p>"},{"location":"reference/text/","title":"<code>class</code> langtorch.Text","text":"<p>The <code>Text</code> class represents strings as a sequence of named segments.   </p> <p>Texts represent textual content with a list of key-value pairs. Unlike dictionaries the same key may be used multiple times, serving as a name or tag for a particular value (the values being the actual substrings that make up the text). The keys are not visible from the string representation of Text, but we can use them to format strings in many ways. The pairs that make up a text object can be access with its .items() method and we can edit these entries using indexing similar to pandas, which uses .iloc and .loc:</p> <p>It supports markup language parsing for organized text handling. This design facilitates operations like text formatting through multiplication, streamlining text processing tasks.</p>"},{"location":"reference/text/#initialization","title":"Initialization","text":"<p>which can include:</p> <ul> <li>Strings</li> <li>Tuples of the form  representing text keys and values</li> <li>Dictionaries of correctly ordered keys and values</li> <li>Sequences of any of the above</li> </ul>"},{"location":"reference/text/#text-class","title":"Text Class","text":"<p>The <code>Text</code> class is integral to LangTorch, designed for sophisticated text manipulations akin to operations on ordered dictionaries but augmented for string processing.</p>"},{"location":"reference/text/#attributes","title":"Attributes","text":"<ul> <li><code>content</code>: Holds \"named strings\" or key-value pairs, enabling concatenation, formatting, and structured text manipulations.</li> <li><code>language</code>: Defines the language for content parsing and formatting. Defaults to <code>\"str\"</code>. Alterable to support diverse textual operations.</li> <li><code>allowed_keys</code>: Restricts to a list of permissible keys within a <code>Text</code> instance, enforcing schema via <code>ValueError</code> on deviation.</li> </ul>"},{"location":"reference/text/#initialization_1","title":"Initialization","text":"<p>Initialization accommodates strings, key-value pair tuples, dictionaries, or other <code>Text</code> instances, promoting flexible textual data structuring.</p>"},{"location":"reference/text/#notes","title":"Notes","text":"<ul> <li>Supports text formatting, concatenation, and splitting into <code>TextTensor</code> for advanced manipulations.</li> <li><code>iloc</code> and <code>loc</code> properties enable precise sub-element access and manipulation by index or key, respectively.</li> </ul>"},{"location":"reference/text/#warnings","title":"Warnings","text":"<ul> <li>Usage of unspecified keys in <code>allowed_keys</code> triggers a <code>ValueError</code>.</li> <li>Improper formatting operations or key/index accesses may cause unintended results.</li> </ul>"},{"location":"reference/text/#examples","title":"Examples","text":""},{"location":"reference/text/#initializing-a-text-instance","title":"Initializing a Text Instance","text":"<pre><code>text = Text(\"Hello, {name}!\", name=\"World\")\nprint(text)\n# Output: \"Hello, World!\"\n</code></pre>"},{"location":"reference/text/#concatenating-text-instances","title":"Concatenating Text Instances","text":"<pre><code>greeting = Text(\"Hello\")\ntarget = Text(name=\"World\")\ncombined = greeting + \", \" + target\nprint(combined)\n# Output: \"Hello, World\"\n</code></pre>"},{"location":"reference/text/#formatting-text-instances","title":"Formatting Text Instances","text":"<pre><code>template = Text(\"Dear {title} {last_name},\")\nformatted = template * {\"title\": \"Mr.\", \"last_name\": \"Doe\"}\nprint(formatted)\n# Output: \"Dear Mr. Doe,\"\n</code></pre>"},{"location":"reference/text/#accessing-attributes","title":"Accessing Attributes","text":"<pre><code>text = Text(\"First\", \"Second\", key1=\"Value1\", key2=\"Value2\")\nprint(text.keys())\n# Output: ['key1', 'key2']\nprint(text.items())\n# Output: [('key1', 'Value1'), ('key2', 'Value2')]\nprint(text.iloc[0])\n# Output: \"First\"\nprint(text.loc['key1'])\n# Output: \"Value1\"\n</code></pre>"},{"location":"reference/text/#indexing","title":"Indexing","text":"<p>DESC</p> <p>The Text class represents structured textual data within LangTorch, allowing for complex manipulations and operations on text similar to working with ordered dictionaries but with enhanced string manipulation capabilities.</p> <p>Attributes:</p> <ul> <li> <code>content</code>         \u2013          <p>The core data of a Text instance, holding a sequence of \"named strings\" or key-value pairs. This attribute      allows Text to support operations like concatenation, formatting, and structured manipulation.</p> </li> <li> <code>language</code>         \u2013          <p>Specifies the language for parsing and formatting the textual content. Default is \"str\", but can be set       to other languages to support various textual operations and transformations.</p> </li> <li> <code>allowed_keys</code>         \u2013          <p>Optionally specifies a list of keys that are permitted within the Text instance. Attempts to use           keys not in this list will raise a ValueError, enforcing a schema on the textual data.</p> </li> </ul> Initialization <p>Text instances can be initialized in multiple ways, including direct strings, tuples representing key-value pairs, dictionaries, or even from other Text instances. This flexibility allows developers to easily structure their textual data as needed for their applications.</p> Notes <ul> <li>Text instances support advanced text manipulation operations, including formatting via multiplication with other   Text instances or dictionaries, concatenation with other Text instances or strings, and even splitting into   TextTensor for further processing.</li> <li>The <code>iloc</code> and <code>loc</code> properties provide powerful mechanisms for accessing and manipulating sub-elements of a Text   instance based on index or key, respectively, facilitating easy modifications and queries.</li> </ul> <p>Examples:</p> <p>Initializing a Text instance:</p> <pre><code>&gt;&gt;&gt; text = Text(\"Hello, {name}!\", name=\"World\")\n&gt;&gt;&gt; print(text)\n\"Hello, World!\"\n</code></pre> <p>Concatenating Text instances:</p> <pre><code>&gt;&gt;&gt; greeting = Text(\"Hello\")\n&gt;&gt;&gt; target = Text(name=\"World\")\n&gt;&gt;&gt; combined = greeting + \", \" + target\n&gt;&gt;&gt; print(combined)\n\"Hello, World\"\n</code></pre> <p>Formatting Text instances:</p> <pre><code>&gt;&gt;&gt; template = Text(\"Dear {title} {last_name},\")\n&gt;&gt;&gt; formatted = template * {\"title\": \"Mr.\", \"last_name\": \"Doe\"}\n&gt;&gt;&gt; print(formatted)\n\"Dear Mr. Doe,\"\n</code></pre> <p>Accessing attributes:</p> <pre><code>&gt;&gt;&gt; text = Text(\"First\", \"Second\", key1=\"Value1\", key2=\"Value2\")\n&gt;&gt;&gt; print(text.keys())\n['key1', 'key2']\n&gt;&gt;&gt; print(text.items())\n[('key1', 'Value1'), ('key2', 'Value2')]\n&gt;&gt;&gt; print(text.iloc[0])\n\"First\"\n&gt;&gt;&gt; print(text.loc['key1'])\n\"Value1\"\n</code></pre>"},{"location":"reference/text/#langtorch.texts.Text.iloc","title":"<code>iloc</code>  <code>property</code>","text":"<p>Index-based indexing. You can access entries with nested keys using dot notation, e.g. .loc[\"key1.key2\"]</p>"},{"location":"reference/text/#langtorch.texts.Text.loc","title":"<code>loc</code>  <code>property</code>","text":"<p>Key-based indexing. You can access entries with nested keys using dot notation, e.g. .loc[\"key1.key2\"]</p>"},{"location":"reference/text/#langtorch.texts.Text.keys","title":"<code>keys()</code>","text":""},{"location":"reference/text/#langtorch.texts.Text.values","title":"<code>values()</code>","text":""},{"location":"reference/text/#langtorch.texts.Text.items","title":"<code>items()</code>","text":"<p>Retrieves key-value pairs from the Text object, allowing for structured data extraction and further processing.</p> <p>Returns:</p> <ul> <li> <code>List[Tuple[str, Union[str, Tuple[...]]]]</code>         \u2013          <p>A list of key-value pairs representing the Text's content.</p> </li> </ul>"},{"location":"reference/text/#changing-keys","title":"Changing keys","text":"<p>The Text class represents structured textual data within LangTorch, allowing for complex manipulations and operations on text similar to working with ordered dictionaries but with enhanced string manipulation capabilities.</p> <p>Attributes:</p> Name Type Description <code>content</code> <p>The core data of a Text instance, holding a sequence of \"named strings\" or key-value pairs. This attribute      allows Text to support operations like concatenation, formatting, and structured manipulation.</p> <code>language</code> <p>Specifies the language for parsing and formatting the textual content. Default is \"str\", but can be set       to other languages to support various textual operations and transformations.</p> <code>allowed_keys</code> <p>Optionally specifies a list of keys that are permitted within the Text instance. Attempts to use           keys not in this list will raise a ValueError, enforcing a schema on the textual data.</p> Initialization <p>Text instances can be initialized in multiple ways, including direct strings, tuples representing key-value pairs, dictionaries, or even from other Text instances. This flexibility allows developers to easily structure their textual data as needed for their applications.</p> Notes <ul> <li>Text instances support advanced text manipulation operations, including formatting via multiplication with other   Text instances or dictionaries, concatenation with other Text instances or strings, and even splitting into   TextTensor for further processing.</li> <li>The <code>iloc</code> and <code>loc</code> properties provide powerful mechanisms for accessing and manipulating sub-elements of a Text   instance based on index or key, respectively, facilitating easy modifications and queries.</li> </ul> <p>Examples:</p> <p>Initializing a Text instance:</p> <pre><code>&gt;&gt;&gt; text = Text(\"Hello, {name}!\", name=\"World\")\n&gt;&gt;&gt; print(text)\n\"Hello, World!\"\n</code></pre> <p>Concatenating Text instances:</p> <pre><code>&gt;&gt;&gt; greeting = Text(\"Hello\")\n&gt;&gt;&gt; target = Text(name=\"World\")\n&gt;&gt;&gt; combined = greeting + \", \" + target\n&gt;&gt;&gt; print(combined)\n\"Hello, World\"\n</code></pre> <p>Formatting Text instances:</p> <pre><code>&gt;&gt;&gt; template = Text(\"Dear {title} {last_name},\")\n&gt;&gt;&gt; formatted = template * {\"title\": \"Mr.\", \"last_name\": \"Doe\"}\n&gt;&gt;&gt; print(formatted)\n\"Dear Mr. Doe,\"\n</code></pre> <p>Accessing attributes:</p> <pre><code>&gt;&gt;&gt; text = Text(\"First\", \"Second\", key1=\"Value1\", key2=\"Value2\")\n&gt;&gt;&gt; print(text.keys())\n['key1', 'key2']\n&gt;&gt;&gt; print(text.items())\n[('key1', 'Value1'), ('key2', 'Value2')]\n&gt;&gt;&gt; print(text.iloc[0])\n\"First\"\n&gt;&gt;&gt; print(text.loc['key1'])\n\"Value1\"\n</code></pre>"},{"location":"reference/text/#langtorch.texts.Text.set_key","title":"<code>set_key(key, inplace=False)</code>","text":"<p>Override keys for the textual entries, used for restructuring the content. Useful for substituting the key right before passing TextTensor to a Module.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[Text, str, List[str]]</code> <p>The new key or keys to apply</p> required <code>inplace</code> <code>bool</code> <p>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Text</code> <p>A new Text instance with updated keys.</p>"},{"location":"reference/text/#langtorch.texts.Text.add_key","title":"<code>add_key(key, inplace=False)</code>","text":"<p>Add a top-level  key, placing the items of the original as a value under the new key. Useful for working with nested keys like in Chat prompts.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[Text, str, List[str]]</code> <p>The new key to add</p> required <code>inplace</code> <code>bool</code> <p>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Text</code> <p>A new Text instance with updated keys.</p>"},{"location":"reference/textmodule/","title":"<code>class</code> langtorch.TextModule","text":"<p><code>TextModule</code> integrates with <code>TextTensor</code> for text manipulation, extending <code>torch.nn.Module</code> for text-based ____. It facilitates the construction of models with complex, hierarchical architectures through subclassing and the organization of task-specific submodules.</p>"},{"location":"reference/textmodule/#example-text-transformation-model","title":"Example: Text Transformation Model","text":"<p>Simplify model construction by initializing shared activations once and reusing them, enhancing code efficiency and reducing redundancy.</p> <pre><code>import torch\nfrom langtorch import TextModule, Activation\n\nactivation = Activation(\"gpt-3.5-turbo\", \"You're an assistant.\", 0.5)\n\nclass TextTransformationModel(TextModule):\n    def __init__(self):\n        super().__init__()\n        self.translate = TextModule(\"Translate this text: {}\", activation)\n        self.summarize = TextModule(\"Summarize this text: {}\", activation)\n\n    def forward(self, input_text):\n        translated_text = self.translate(input_text)\n        return self.summarize(translated_text)\n</code></pre> <p>This model translates and summarizes text using a shared <code>Activation</code>, streamlining the process.</p>"},{"location":"reference/textmodule/#note","title":"Note","text":"<p>Ensure <code>super().__init__()</code> is called before adding submodules for correct initialization.</p>"},{"location":"reference/textmodule/#processing","title":"Processing","text":"<p><code>TextModule</code> supports both sequential and parallel processing, suitable for handling batched inputs and multiple prompts.</p>"},{"location":"reference/textmodule/#sequential-processing-example","title":"Sequential Processing Example","text":"<p>Construct pipelines with <code>torch.nn.Sequential</code>, passing <code>TextTensor</code> through defined <code>TextModule</code> operations.</p> <pre><code>pipeline = torch.nn.Sequential(\n    TextModule(\"Translate this text to English: {}\", activation),\n    TextModule(\"Summarize the translated text: {}\", activation)\n)\n\noutput = pipeline(TextTensor([\"Text to be processed.\"]))\n</code></pre> <p>This approach allows for straightforward chaining of translation and summarization steps using a unified <code>Activation</code>.</p>"},{"location":"reference/textmodule/#considerations","title":"Considerations","text":"<ul> <li>Match <code>TextTensor</code> inputs with prompt templates to prevent processing errors.</li> <li>Customize <code>TextModule</code> through subclassing for specific needs.</li> <li>Automated parallel processing enhances efficiency for complex input scenarios.</li> </ul>"},{"location":"reference/texttensor/","title":"<code>class</code> langtorch.TextTensor","text":"<p>A <code>TextTensor</code> is a multi-dimensional matrix containing elements of structured text data, each represented as a <code>Text</code> object  of structured text.</p>"},{"location":"reference/texttensor/#initializing-and-basic-operations","title":"Initializing and Basic Operations","text":"<p>A <code>TextTensor</code> can be constructed from Python lists or sequences of string-like inputs. Every entry is transformed into a Text object, so each entry must satisfy one of its many possible input formats, which can include:</p> <ul> <li>Strings</li> <li>Tuples of the form  representing text keys and values</li> <li>Dictionaries of correctly ordered keys and values</li> <li>Sequences of any of the above</li> </ul>"},{"location":"reference/texttensor/#construction","title":"Construction","text":"<p>You can construct a <code>TextTensor</code>, similar to <code>torch.tensor</code>, where the input sequence can be shapes into  nested lists or a numpy array:</p> <pre><code>from langtorch import TextTensor\n\n# 0-d TextTensor\n\n# 1-d TextTensor\n\n# 2-d TextTensor, and so on\nTextTensor([[\"Hello, world!\"], [\"How are you?\"]])\n\n# With complex text entries\n\n\n# Text\n\nTextTensor([{\"greeting\": \"Hello\", \"name\": \"Alice\"}])\n</code></pre> <p>TextTensors can be created with familiar operations: <pre><code># A tensor of empty strings\nlangtorch.zeros(shape)\nlangtorch.zeros_like(tensor)\n\n# A tensor with all \n</code></pre></p> <p>Note</p> <p>TextTensors almost never get initialised from torch Tensors</p>"},{"location":"reference/texttensor/#indexing-and-slicing","title":"Indexing and Slicing","text":"<p>Access and modify the contents of a <code>TextTensor</code> using Python's indexing and slicing notation:</p> <pre><code>tt = TextTensor([[\"Hello\", \"World\"], [\"LangTorch\", \"Framework\"]])\nprint(tt[0][1])  # Outputs: \"World\"\ntt[1][0] = \"Welcome to\"\nprint(tt)  # Outputs: TextTensor([[\"Hello\", \"World\"], [\"Welcome to\", \"Framework\"]])\n</code></pre> <p>Extract a single <code>Text</code> object from a <code>TextTensor</code>:</p> <pre><code>tt = TextTensor([[\"Hello\"]])\nsingle_text = tt.item()\nprint(single_text)  # Outputs: \"Hello\"\n</code></pre> <p>TextTensor is a specialized subclass of torch.Tensor designed for handling and manipulating textual data within the LangTorch framework. Each entry in a TextTensor is a structured Text object, enabling complex text operations and transformations, including natural language processing tasks and interaction with large language models (LLMs).</p> <p>TextTensor supports standard tensor operations adapted for text, such as concatenation and reshaping, alongside unique text-specific operations like prompt formatting through multiplication. It seamlessly integrates with PyTorch, allowing developers to leverage familiar tensor operations while working with textual data.</p> <p>Attributes:</p> Name Type Description <code>ttype</code> <code>Class</code> <p>Specifies the text type for entries in the TextTensor, ttype should be a subclass of the Text class.</p> <code>_embedding_model</code> <code>Union[str, TextModule]</code> <p>Name of an OpenAI embedding model or TextModule to convert TextTensor entries to embeddings.</p> <code>_tokenizer</code> <code>Tokenizer</code> <p>Tokenizer for converting TextTensor entries to tokens suitable for LLMs.</p> <code>parse</code> <code>bool</code> <p>Controls automatic parsing of text entries. Default is 'auto', which decides based on context.</p> Special Operations <ul> <li>Addition (<code>+</code>): Concatenates text entries.</li> <li>Multiplication (<code>*</code>): Formats prompt templates with values from another TextTensor or dictionary.</li> </ul> <p>Other Parameters:</p> Name Type Description <code>content</code> <code>(str, Text, list, dict)</code> <p>Initial content for the TextTensor. Can be a single string, a Text object, a list of strings or Text objects, or a dictionary for named entries.</p> <code>embedding</code> <code>Optional[Union[Tensor, List[float], ndarray, bool]]</code> <p>Optional embedding for the TextTensor. If True, computes embeddings for text entries automatically.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata for the TextTensor. requires_grad (bool, optional): Indicates whether the tensor should track gradients.</p> <code>is_gradient</code> <code>bool</code> <p>Marks the tensor as a gradient tensor.</p> <code>is_param</code> <code>bool</code> <p>Marks the tensor as a parameter tensor.</p> <code>**kwargs</code> <p>Additional keyword arguments passed to the underlying torch.Tensor.</p> <p>Examples:</p> <p>Creating a TextTensor with prompt templates:</p> <pre><code>&gt;&gt;&gt; tt = TextTensor([\"Hello, {name}!\"])\n&gt;&gt;&gt; tt * TextTensor({\"name\": \"World\"})\nTextTensor([\"Hello, World!\"])\n</code></pre> <p>Concatenating TextTensors:</p> <pre><code>&gt;&gt;&gt; tt1 = TextTensor([\"Hello\"])\n&gt;&gt;&gt; tt2 = TextTensor([\", World!\"])\n&gt;&gt;&gt; tt1 + tt2\nTextTensor([\"Hello, World!\"])\n</code></pre> Note <ul> <li>Ensure proper key matching when performing operations that rely on named entries.</li> <li>Consider setting <code>requires_grad</code> appropriately for training LLMs or gradient-based operations.</li> </ul> Warning <ul> <li>Incorrect usage of keys or mismatched shapes during operations may lead to unexpected results.</li> </ul>"},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.item","title":"<code>item()</code>","text":""},{"location":"reference/texttensor/#key-assignment","title":"Key Assignment","text":"<p>Explicitly assign or change keys in a <code>TextTensor</code> using the <code>set_key()</code> method:</p> <pre><code>tt = TextTensor([\"This is a sentence.\"])\ntt.set_key(\"sentence\")\n</code></pre> <p>For comprehensive details on creating and manipulating <code>TextTensors</code>, refer to the LangTorch documentation on <code>TextTensor</code> operations.</p> <p>TextTensor is a specialized subclass of torch.Tensor designed for handling and manipulating textual data within the LangTorch framework. Each entry in a TextTensor is a structured Text object, enabling complex text operations and transformations, including natural language processing tasks and interaction with large language models (LLMs).</p> <p>TextTensor supports standard tensor operations adapted for text, such as concatenation and reshaping, alongside unique text-specific operations like prompt formatting through multiplication. It seamlessly integrates with PyTorch, allowing developers to leverage familiar tensor operations while working with textual data.</p> <p>Attributes:</p> Name Type Description <code>ttype</code> <code>Class</code> <p>Specifies the text type for entries in the TextTensor, ttype should be a subclass of the Text class.</p> <code>_embedding_model</code> <code>Union[str, TextModule]</code> <p>Name of an OpenAI embedding model or TextModule to convert TextTensor entries to embeddings.</p> <code>_tokenizer</code> <code>Tokenizer</code> <p>Tokenizer for converting TextTensor entries to tokens suitable for LLMs.</p> <code>parse</code> <code>bool</code> <p>Controls automatic parsing of text entries. Default is 'auto', which decides based on context.</p> Special Operations <ul> <li>Addition (<code>+</code>): Concatenates text entries.</li> <li>Multiplication (<code>*</code>): Formats prompt templates with values from another TextTensor or dictionary.</li> </ul> <p>Other Parameters:</p> Name Type Description <code>content</code> <code>(str, Text, list, dict)</code> <p>Initial content for the TextTensor. Can be a single string, a Text object, a list of strings or Text objects, or a dictionary for named entries.</p> <code>embedding</code> <code>Optional[Union[Tensor, List[float], ndarray, bool]]</code> <p>Optional embedding for the TextTensor. If True, computes embeddings for text entries automatically.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata for the TextTensor. requires_grad (bool, optional): Indicates whether the tensor should track gradients.</p> <code>is_gradient</code> <code>bool</code> <p>Marks the tensor as a gradient tensor.</p> <code>is_param</code> <code>bool</code> <p>Marks the tensor as a parameter tensor.</p> <code>**kwargs</code> <p>Additional keyword arguments passed to the underlying torch.Tensor.</p> <p>Examples:</p> <p>Creating a TextTensor with prompt templates:</p> <pre><code>&gt;&gt;&gt; tt = TextTensor([\"Hello, {name}!\"])\n&gt;&gt;&gt; tt * TextTensor({\"name\": \"World\"})\nTextTensor([\"Hello, World!\"])\n</code></pre> <p>Concatenating TextTensors:</p> <pre><code>&gt;&gt;&gt; tt1 = TextTensor([\"Hello\"])\n&gt;&gt;&gt; tt2 = TextTensor([\", World!\"])\n&gt;&gt;&gt; tt1 + tt2\nTextTensor([\"Hello, World!\"])\n</code></pre> Note <ul> <li>Ensure proper key matching when performing operations that rely on named entries.</li> <li>Consider setting <code>requires_grad</code> appropriately for training LLMs or gradient-based operations.</li> </ul> Warning <ul> <li>Incorrect usage of keys or mismatched shapes during operations may lead to unexpected results.</li> </ul>"},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.set_key","title":"<code>set_key(keys=None, inplace=False)</code>","text":""},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.item","title":"<code>item()</code>","text":""},{"location":"reference/texttensor/#warnings-and-notes","title":"Warnings and Notes","text":"<ul> <li>Operations involving keys require accurate matching between the template and data.</li> <li><code>TextTensors</code> support a variety of operations for modifying and generating new <code>TextTensors</code>.</li> <li>For advanced text processing with LLMs, utilize <code>TextModule</code> along with <code>TextTensors</code>.</li> </ul> <p>TextTensor is a specialized subclass of torch.Tensor designed for handling and manipulating textual data within the LangTorch framework. Each entry in a TextTensor is a structured Text object, enabling complex text operations and transformations, including natural language processing tasks and interaction with large language models (LLMs).</p> <p>TextTensor supports standard tensor operations adapted for text, such as concatenation and reshaping, alongside unique text-specific operations like prompt formatting through multiplication. It seamlessly integrates with PyTorch, allowing developers to leverage familiar tensor operations while working with textual data.</p> <p>Attributes:</p> Name Type Description <code>ttype</code> <code>Class</code> <p>Specifies the text type for entries in the TextTensor, ttype should be a subclass of the Text class.</p> <code>_embedding_model</code> <code>Union[str, TextModule]</code> <p>Name of an OpenAI embedding model or TextModule to convert TextTensor entries to embeddings.</p> <code>_tokenizer</code> <code>Tokenizer</code> <p>Tokenizer for converting TextTensor entries to tokens suitable for LLMs.</p> <code>parse</code> <code>bool</code> <p>Controls automatic parsing of text entries. Default is 'auto', which decides based on context.</p> Special Operations <ul> <li>Addition (<code>+</code>): Concatenates text entries.</li> <li>Multiplication (<code>*</code>): Formats prompt templates with values from another TextTensor or dictionary.</li> </ul> <p>Other Parameters:</p> Name Type Description <code>content</code> <code>(str, Text, list, dict)</code> <p>Initial content for the TextTensor. Can be a single string, a Text object, a list of strings or Text objects, or a dictionary for named entries.</p> <code>embedding</code> <code>Optional[Union[Tensor, List[float], ndarray, bool]]</code> <p>Optional embedding for the TextTensor. If True, computes embeddings for text entries automatically.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata for the TextTensor. requires_grad (bool, optional): Indicates whether the tensor should track gradients.</p> <code>is_gradient</code> <code>bool</code> <p>Marks the tensor as a gradient tensor.</p> <code>is_param</code> <code>bool</code> <p>Marks the tensor as a parameter tensor.</p> <code>**kwargs</code> <p>Additional keyword arguments passed to the underlying torch.Tensor.</p> <p>Examples:</p> <p>Creating a TextTensor with prompt templates:</p> <pre><code>&gt;&gt;&gt; tt = TextTensor([\"Hello, {name}!\"])\n&gt;&gt;&gt; tt * TextTensor({\"name\": \"World\"})\nTextTensor([\"Hello, World!\"])\n</code></pre> <p>Concatenating TextTensors:</p> <pre><code>&gt;&gt;&gt; tt1 = TextTensor([\"Hello\"])\n&gt;&gt;&gt; tt2 = TextTensor([\", World!\"])\n&gt;&gt;&gt; tt1 + tt2\nTextTensor([\"Hello, World!\"])\n</code></pre> Note <ul> <li>Ensure proper key matching when performing operations that rely on named entries.</li> <li>Consider setting <code>requires_grad</code> appropriately for training LLMs or gradient-based operations.</li> </ul> Warning <ul> <li>Incorrect usage of keys or mismatched shapes during operations may lead to unexpected results.</li> </ul>"},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.split","title":"<code>split(sep, dim=0)</code>","text":"<p>Return a new TextTensor, with an additional first dimension to split everything using sep as the delimiter.</p> <p>sep     The delimiter according which to split the bytearray.     None (the default value) means split on ASCII whitespace characters     (space, tab, return, newline, formfeed, vertical tab).</p>"},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.apply","title":"<code>apply(func)</code>","text":"<p>Applies a function to each entry of self.content.</p>"},{"location":"reference/texttensor/#langtorch.tensors.TextTensor.backward","title":"<code>backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)</code>","text":"<p>See. docs</p>"},{"location":"reference/tt/","title":"langtorch.tt","text":"<pre><code>task_on_3_texts = TextLinear([[\"Text 1: \", \"Text 2: \", \"Text 3: \"]], \n                             bias = \"Task: What\")\n()\n</code></pre> <p>TextTensor([[\"\\n\"]*k]) @ retrieved_content.unsqueeze(1)</p>"}]}