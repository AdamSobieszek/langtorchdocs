
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../reference/">
      
      
      <link rel="icon" href="../langtorch_fav.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.9">
    
    
      
        <title>Quickstart tutorial - LangTorch Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.f2e4d321.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    
      <link rel="stylesheet" href="../css/custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#quickstart-guide-dive-into-langtorch" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="LangTorch Docs" class="md-header__button md-logo" aria-label="LangTorch Docs" data-md-component="logo">
      
  <img src="../langtorch_banner.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LangTorch Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Quickstart tutorial
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="LangTorch Docs" class="md-nav__button md-logo" aria-label="LangTorch Docs" data-md-component="logo">
      
  <img src="../langtorch_banner.png" alt="logo">

    </a>
    LangTorch Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Quickstart tutorial
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Quickstart tutorial
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-perform-multiple-llm-calls-with-texttensors" class="md-nav__link">
    <span class="md-ellipsis">
      1. Perform multiple LLM calls with TextTensors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Perform multiple LLM calls with TextTensors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparison-with-the-openai-package" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison with the OpenAI Package
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-implementing-popular-methods" class="md-nav__link">
    <span class="md-ellipsis">
      2. Implementing popular methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Implementing popular methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chained-calls-and-simple-zero-shot-chain-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      Chained Calls and Simple Zero-Shot Chain of Thought
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensemble-self-consistency" class="md-nav__link">
    <span class="md-ellipsis">
      Ensemble / Self-Consistency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-automatic-texttensor-embeddings-for-building-retrievers" class="md-nav__link">
    <span class="md-ellipsis">
      3. Automatic TextTensor Embeddings for Building Retrievers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-embeddings-and-documents-parsing-chunking-and-indexing" class="md-nav__link">
    <span class="md-ellipsis">
      Working with embeddings and documents (parsing, chunking and indexing)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Working with embeddings and documents (parsing, chunking and indexing)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#build-custom-retriever-and-rag-modules" class="md-nav__link">
    <span class="md-ellipsis">
      Build Custom Retriever and RAG modules
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Reference
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/langtorch/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            langtorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/text/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch.Text
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            langtorch.Text
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/parsing/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Text Parsing and Chats
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Text Parsing and Chats
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/texttensor/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch.TextTensor
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            langtorch.TextTensor
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/tensorattrs/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    TextTensor Attributes
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            TextTensor Attributes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/multiplication/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    TextTensor Multiplication
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            TextTensor Multiplication
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/tt/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch.tt
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            langtorch.tt
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/textmodule/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch.tt.TextModule
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            langtorch.tt.TextModule
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
    
    
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_10" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../reference/activation/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    langtorch.tt.Activation
  </span>
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_10">
            <span class="md-nav__icon md-icon"></span>
            langtorch.tt.Activation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-perform-multiple-llm-calls-with-texttensors" class="md-nav__link">
    <span class="md-ellipsis">
      1. Perform multiple LLM calls with TextTensors
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Perform multiple LLM calls with TextTensors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparison-with-the-openai-package" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison with the OpenAI Package
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-implementing-popular-methods" class="md-nav__link">
    <span class="md-ellipsis">
      2. Implementing popular methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Implementing popular methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chained-calls-and-simple-zero-shot-chain-of-thought" class="md-nav__link">
    <span class="md-ellipsis">
      Chained Calls and Simple Zero-Shot Chain of Thought
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensemble-self-consistency" class="md-nav__link">
    <span class="md-ellipsis">
      Ensemble / Self-Consistency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-automatic-texttensor-embeddings-for-building-retrievers" class="md-nav__link">
    <span class="md-ellipsis">
      3. Automatic TextTensor Embeddings for Building Retrievers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#working-with-embeddings-and-documents-parsing-chunking-and-indexing" class="md-nav__link">
    <span class="md-ellipsis">
      Working with embeddings and documents (parsing, chunking and indexing)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Working with embeddings and documents (parsing, chunking and indexing)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#build-custom-retriever-and-rag-modules" class="md-nav__link">
    <span class="md-ellipsis">
      Build Custom Retriever and RAG modules
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="quickstart-guide-dive-into-langtorch">Quickstart Guide: Dive into LangTorch</h1>
<blockquote>
<p><a href="https://colab.research.google.com/github/AdamSobieszek/langtorch/blob/main/quickstart.ipynb"><img align="center" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>  <strong>&lt; &lt; To go through this guide interactively, we recommend this Colab notebook version</strong></p>
</blockquote>
<hr />
<h2 id="installation">Installation</h2>
<p>To install LangTorch using pip:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>langtorch
</code></pre></div>
<p>To use the OpenAI API as our LLM, we need to set the <code>OPENAI_API_KEY</code> environment variable. You can find your API key on platform.openai.com</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;your_api_key&quot;</span> <span class="c1"># Replace with your actual OpenAI API key</span>
</code></pre></div>
<h2 id="1-perform-multiple-llm-calls-with-texttensors">1. Perform multiple LLM calls with TextTensors</h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">langtorch</span> <span class="kn">import</span> <span class="n">TextTensor</span> <span class="c1"># holds texts instead of weights, supports tensor operations</span>
<span class="kn">from</span> <span class="nn">langtorch</span> <span class="kn">import</span> <span class="n">TextModule</span> <span class="c1"># torch.nn modules working on TextTensors, perform prompt templating and llm calls</span>
</code></pre></div>
<p><strong><code>TextTensors</code></strong> are designed to streamline working with many pieces of text and performing parallel LLM calls. <code>langtorch.TextTensor</code> is a subclass of PyTorch's <code>torch.Tensor</code> that:</p>
<ul>
<li>
<p><strong>Holds text entries</strong> instead of numerical weights.</p>
</li>
<li>
<p><strong>Special Structure:</strong> <code>TextTensors</code> entries can represent chunked documents, prompt templates, completion dictionaries, chat histories, and more.</p>
</li>
<li>
<p><strong>Represents Geometrically:</strong> <code>TextTensors</code> have a shape and can be modified with PyTorch functions (reshape, stack, etc.).</p>
</li>
</ul>
<hr />
<p>In this example, we will create tensors holding prompt templates, fill them with a tensor of completion dictionaries, and send them to the OpenAI API.</p>
<div class="highlight"><pre><span></span><code><span class="n">prompt_tensor</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">([[</span><span class="s2">&quot;Is this an email address? </span><span class="si">{input_field}</span><span class="s2">&quot;</span><span class="p">],</span>  
                            <span class="p">[</span><span class="s2">&quot;Is this a valid web link? </span><span class="si">{input_field}</span><span class="s2">&quot;</span><span class="p">]])</span>  

<span class="c1"># Adding TextTensors appends their content according to broadcasting rules  </span>
<span class="n">prompt_tensor</span> <span class="o">+=</span> <span class="s2">&quot; (Answer &#39;Yes&#39; or &#39;No&#39;)&quot;</span>  
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_tensor</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape =&quot;</span><span class="p">,</span> <span class="n">prompt_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[[Is this an email address? input_field (Answer &#39;Yes&#39; or &#39;No&#39;)]
 [Is this a valid web link? input_field (Answer &#39;Yes&#39; or &#39;No&#39;)]]
Shape = torch.Size([2, 1])
</code></pre></div>

</div>
<p><code>TextModules</code> are <code>torch.nn.Modules</code> that work on <code>TextTensors</code>:</p>
<ul>
<li>
<p><strong>Tensor of Prompts:</strong> They hold a tensor of prompts instead of numerical weights.</p>
</li>
<li>
<p><strong>Input Handling:</strong> They accept <code>TextTensors</code> as input, which are used to format the prompt tensor.</p>
</li>
<li>
<p><strong>Formatting and Broadcasting:</strong> This allows formatting multiple prompts on multiple completions, controlling which prompt gets which input through broadcasting rules.</p>
</li>
<li>
<p><strong>Activation Function:</strong> Most torch layers end with an <em>activation function</em>. Similarly, <code>TextModules</code> end in an <em>activation</em> of an LLM call.</p>
</li>
</ul>
<p>In this example, we will create a <code>TextModule</code> that ends in a call to an OpenAI model. <strong>This module can now execute both tasks in parallel on as many inputs as we'd like:</strong></p>
<div class="highlight"><pre><span></span><code><span class="n">tasks_module</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="n">prompt_tensor</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="n">input_completions</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">([{</span><span class="s2">&quot;input_field&quot;</span><span class="p">:</span> <span class="s2">&quot;contact@langtorch.org&quot;</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;input_field&quot;</span><span class="p">:</span> <span class="s2">&quot;https://langtorch.org&quot;</span> <span class="p">}])</span>

<span class="c1"># The first row of the output are answers to &quot;Is this an email address?&quot;, second to &quot;Is this a valid web link?&quot;</span>

<span class="c1"># Columns are the two input completions</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tasks_module</span><span class="p">(</span><span class="n">input_completions</span><span class="p">))</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[[Yes   No ]
 [No    Yes]]
</code></pre></div>

</div>
<h3 id="comparison-with-the-openai-package">Comparison with the OpenAI Package</h3>
<p>The <code>TextModule</code> above both formats the prompts and sends them to the OpenAI activation (<code>langtorch.OpenAI</code>). Let's compare LangTorch to the OpenAI package.</p>
<p>First, we'll separate the formatting and API steps.</p>
<blockquote>
<p>A core feature of <code>TextTensors</code> is that they allow us to easily format several prompts on several inputs.</p>
<p>LangTorch achieves this by <strong>defining the product</strong> of two <code>TextTensors</code>: <code>text1*text2</code> as an operation akin to <code>text1.format(**text2)</code>. As shown below this is what happens in a <code>TextModule</code> before adding an activation:</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="c1"># Using TextModule  </span>
<span class="n">tasks_module</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="n">prompt_tensor</span><span class="p">)</span>  
<span class="n">prompts</span> <span class="o">=</span> <span class="n">tasks_module</span><span class="p">(</span><span class="n">input_completions</span><span class="p">)</span>  

<span class="c1"># Equivalently, using &quot;TextTensor multiplication&quot;  </span>
<span class="n">prompts</span> <span class="o">=</span> <span class="n">prompt_tensor</span><span class="o">*</span><span class="n">input_completions</span>  
<span class="nb">print</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[[Is this an email address? contact@langtorch.org (Answer &#39;Yes&#39; or &#39;No&#39;)   Is this an email address? https://langtorch.org (Answer &#39;Yes&#39; or &#39;No&#39;)]
 [Is this a valid web link? contact@langtorch.org (Answer &#39;Yes&#39; or &#39;No&#39;)   Is this a valid web link? https://langtorch.org (Answer &#39;Yes&#39; or &#39;No&#39;)]]
</code></pre></div>

</div>
<p>The code above introduces the multiplication operation (used in TextModules), which acts like a more powerful format operation and allows for the various features of TextTensors. For a more in depth look, see <a href="langtorch.org/reference/multiplication">TextTensor Multiplication</a>.</p>
<p>We can send the formatted prompts to the OpenAI API by creating a <code>langtorch.OpenAI</code> module (the "activation") and compare speed between three API use cases:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">openai</span>  
<span class="kn">import</span> <span class="nn">langtorch</span>  
<span class="kn">import</span> <span class="nn">time</span>  

<span class="n">langtorch_api</span> <span class="o">=</span> <span class="n">langtorch</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">,</span> <span class="n">max_token</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>  
<span class="n">openai_api</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">()</span>  

<span class="c1"># Open AI package  </span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  
<span class="n">responses</span> <span class="o">=</span> <span class="p">[]</span>  
<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>  
    <span class="n">responses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">openai_api</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>  
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>  
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>  
                  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>  
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span>  
    <span class="p">))</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;1.</span><span class="se">\n</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">responses</span><span class="p">)[:</span><span class="mi">125</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; OpenAI loop time taken: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>  

<span class="c1"># LangTorch  </span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  
<span class="n">responses</span> <span class="o">=</span> <span class="n">langtorch_api</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;2.</span><span class="se">\n</span><span class="si">{</span><span class="n">responses</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; LangTorch time taken: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>  

<span class="c1"># LangTorch on repeated requests  </span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>  
<span class="n">responses</span> <span class="o">=</span> <span class="n">langtorch_api</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;3.</span><span class="se">\n</span><span class="si">{</span><span class="n">responses</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; LangTorch on repeated requests time taken: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>1.
[ChatCompletion(id=&#39;chatcmpl-9WkJfJJ8jPGXivDlQfPT35UZS5q2K&#39;, choices=[Choice(finish_reason=&#39;length&#39;, index=0, logprobs=None, ...
 OpenAI loop time taken: 2.18 seconds
2.
[[Yes   No ]
 [No    Yes]]
 LangTorch time taken: 0.84 seconds
3.
[[Yes   No ]
 [No    Yes]]
 LangTorch on repeated requests time taken: 0.05 seconds
</code></pre></div>

</div>
<p>The OpenAI Activation in LangTorch (<code>langtorch.OpenAI</code>) isn't just a wrapper around the OpenAI package. The observed speed up comes from the fact that the LangTorch implementation:</p>
<ul>
<li>Sends API calls in parallel, allowing multiple completions to be generated much faster than calling the OpenAI chat completion endpoint in sequence.</li>
<li>Saves on tokens and speeds up subsequent calls by caching API results, especially for embeddings and when the temperature is set to zero.</li>
<li>Optimizes requested API calls removing duplicates</li>
</ul>
<p><code>langtorch.OpenAI</code> also:</p>
<ul>
<li>Operates directly on <code>TextTensors</code>, returning calls in the same shape as the input.</li>
<li>Handles API errors and retries by default</li>
</ul>
<h2 id="2-implementing-popular-methods">2. Implementing popular methods</h2>
<h3 id="chained-calls-and-simple-zero-shot-chain-of-thought">Chained Calls and Simple Zero-Shot Chain of Thought</h3>
<p>LangTorch integrates seamlessly with torch, allowing you to easily chain <code>TextModules</code> using <code>torch.nn.Sequential</code>. This can be used to chain multiple LLM calls or additional prompting methods. A simplified example is a zero-shot Chain of Thought, for which we can create a reusable <code>TextModule</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">CoT</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="s2">&quot;{*} Let&#39;s think step by step.&quot;</span><span class="p">)</span>
</code></pre></div>
<p><code>{}</code> in a prompt template is a positional argument, taking one input argument in each entry. For our chain of thought module we use the placeholder <code>{*}</code>, which is a "wildcard" key that places all the input entries in its place.</p>
<p>Now to chain these <code>torch.nn.Sequential</code>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>  
<span class="n">calculate</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="s2">&quot;Calculate the following: </span><span class="si">{}</span><span class="s2"> = ?&quot;</span><span class="p">)</span>  

<span class="n">calculate_w_CoT</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>  
    <span class="n">calculate</span><span class="p">,</span>  
    <span class="n">CoT</span><span class="p">,</span>  
    <span class="n">langtorch</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span> <span class="p">,</span>  
    <span class="c1"># You can add sequential calls here  </span>
<span class="p">)</span>  

<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">([</span><span class="s2">&quot;170*32&quot;</span><span class="p">,</span> <span class="s2">&quot;123*45/10&quot;</span><span class="p">,</span> <span class="s2">&quot;2**10*5&quot;</span><span class="p">])</span>  
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">calculate_w_CoT</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>  
<span class="n">output_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># We use torch methods to reshape TextTensors and view entries in columns</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[[To calculate 170 multiplied by 32, we can use↵   Step 1: Multiply 123 by 45         First, calculate 2**10 (2 to the power of 10):
   long multiplication:                            123 * 45 = 5535                    2**10 = 1024                                   

       170                                         Step 2: Divide the result by 10    Next, multiply the result by 5:                
  x     32                                         5535 / 10 = 553.5                  1024 * 5 = 5120                                
  _________                                                                                                                          
       340   (170 * 2)                             Therefore, 123 * 45 / 10 = 553.5   Therefore, 2**10*5 = 5120.                     
  +  5100   (170 * 30)                                                                                                               
  _________                                                                                                                          
      5440                                                                                                                           

  Therefore, 170 multiplied by 32 equals 5440.                                                                                      ]]
</code></pre></div>

</div>
<h3 id="ensemble-self-consistency">Ensemble / Self-Consistency</h3>
<p>Representing texts geometrically in a matrix or tensor allows for creating meaningful structures. Methods like ensemble voting and self-consistency involve generating multiple completions for the same task, easily represented by adding a dimension.</p>
<p>In this example, we build a module that creates multiple Chain-of-Thought answers for each input. These create separate <code>TextTensor</code> entries that we combine using a "linear layer" to marginalize over them, improving overall performance (see <a href="https://arxiv.org/abs/2203.11171">Wang et al., 2022</a>).</p>
<div class="highlight"><pre><span></span><code><span class="n">calculate</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="s2">&quot;Calculate the following: </span><span class="si">{}</span><span class="s2"> = ? Let&#39;s think step by step.&quot;</span><span class="p">)</span>  

<span class="n">ensemble_llm</span> <span class="o">=</span> <span class="n">langtorch</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span><span class="n">T</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 3 completions per input with high temperature  </span>

<span class="n">combine_answers</span> <span class="o">=</span> <span class="n">langtorch</span><span class="o">.</span><span class="n">Linear</span><span class="p">([[</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Answer </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: &quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="p">]])</span> <span class="c1"># Here we use properties of matrix multiplication:  </span>
<span class="c1"># Linear uses matmul, where row_of_labels @ column_of_completions == one long entry with labeled completions  </span>

<span class="n">chose</span> <span class="o">=</span> <span class="n">TextModule</span><span class="p">(</span><span class="s2">&quot;Select from these reasoning paths the most consistent final answer: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="p">)</span>  

<span class="n">llm</span> <span class="o">=</span> <span class="n">langtorch</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  

<span class="n">self_consistent_calculate</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>  
    <span class="n">calculate</span><span class="p">,</span>  
    <span class="n">ensemble_llm</span><span class="p">,</span>  
    <span class="n">combine_answers</span><span class="p">,</span>  
    <span class="n">chose</span><span class="p">,</span>  
    <span class="n">llm</span>  
<span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">(</span><span class="s2">&quot;171*33&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">self_consistent_calculate</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">))</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[The most consistent final answer is: 171 * 33 = 5643. This is the answer provided in both Answer 2 and Answer 3, which break down the multi
  plication process into steps and arrive at the same final result.                                                                           ]
</code></pre></div>

</div>
<p>Saving results from repeating these calls, let's us see accuracy increasing from 25% (using <code>calculate</code>) to well over 50% (using <code>self_consistent_calculate</code>) on this input.</p>
<h2 id="3-automatic-texttensor-embeddings-for-building-retrievers">3. Automatic TextTensor Embeddings for Building Retrievers</h2>
<p><code>TextTensors</code> offer a straightforward way to work with embeddings. Every <code>TextTensor</code> can generate its own embeddings -- held in a torch tensor that preserves their shape. Moreover, <code>TextTensors</code> automatically act as their embeddings when passed to torch functions like cosine similarity.</p>
<p>These representations (available under the <code>.embedding</code> attribute) are created automatically right before they are needed, using a set embedding model (default is OpenAI's <code>text-embedding-3-small</code>).</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>  

<span class="n">tensor1</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">([[[</span><span class="s2">&quot;Yes&quot;</span><span class="p">],</span>  
                       <span class="p">[</span><span class="s2">&quot;No&quot;</span><span class="p">]]])</span>  
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">([</span><span class="s2">&quot;Yeah&quot;</span><span class="p">,</span> <span class="s2">&quot;Nope&quot;</span><span class="p">,</span> <span class="s2">&quot;Yup&quot;</span><span class="p">,</span> <span class="s2">&quot;Non&quot;</span><span class="p">])</span>  

<span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>tensor([[[0.6923, 0.6644, 0.6318, 0.5749],
         [0.5458, 0.7727, 0.5386, 0.7036]]])
</code></pre></div>

</div>

<p>We can access the embedding tensor under <code>.embedding</code>, change the embedding model and embed using <code>.embed()</code>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># To change embedding model and embed</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">embedding_model</span> <span class="o">=</span> <span class="s2">&quot;text-embedding-3-large&quot;</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">embed</span><span class="p">()</span>
<span class="c1"># To access the embedding tensor</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">embedding</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>tensor([[[[-0.0338,  0.0298, -0.0105,  ..., -0.0194, -0.0076,  0.0153]],
         [[-0.0281,  0.0073, -0.0121,  ..., -0.0071,  0.0094,  0.0090]]]])
</code></pre></div>

</div>

<h2 id="working-with-embeddings-and-documents-parsing-chunking-and-indexing">Working with embeddings and documents (parsing, chunking and indexing)</h2>
<p>To enable its functionalities <code>TextTensor</code> entries aren't just strings, but structured <a href="https://langtorch.org/reference/text"><code>Text</code></a> objects, which can be created from f-string templates, dictionaries and markup documents and are represented by a sequence of <code>(label, text)</code> pairs.</p>
<p>For the next task we need chunked text data. We can use the above fact to conveniently manipulate markdown files -- in this example, a <a href="https://link.springer.com/article/10.1007/s11023-022-09602-0">paper on the abilities of language models</a>. Download the markdown file from here:</p>
<div class="highlight"><pre><span></span><code>&gt;<span class="w"> </span>wget<span class="w"> </span>https://raw.githubusercontent.com/adamsobieszek/langtorch/main/src/langtorch/conf/paper.md
</code></pre></div>
<p>We can create a tensor with each markdown block in a separate entry simply with:</p>
<div class="highlight"><pre><span></span><code><span class="n">paper</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">&quot;paper.md&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">paper</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> (...)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shape = </span><span class="si">{</span><span class="n">paper</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[# Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models
  Adam Sobieszek &amp; Tadeusz Price, 2022                                           
  ## Abstract                                                                    ] 
  (...)
shape = torch.Size([80])
</code></pre></div>

</div>
<p>As the text has headers and other text blocks, we need to extract only paragraphs. This is where text entries being structured becomes useful, as LangTorch provides <code>iloc</code> and <code>loc</code> accessors for <code>Text</code> entries and tensors:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Select paragraphs</span>
<span class="n">paragraphs</span> <span class="o">=</span> <span class="n">paper</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;Para&quot;</span><span class="p">]</span>  
<span class="c1"># Remove empty entries  </span>
<span class="n">paragraphs</span> <span class="o">=</span> <span class="n">paragraphs</span><span class="p">[</span><span class="n">paragraphs</span><span class="o">!=</span><span class="s2">&quot;&quot;</span><span class="p">]</span>  
<span class="nb">print</span><span class="p">(</span><span class="n">paragraphs</span><span class="p">[:]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[:</span><span class="mi">40</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;...&quot;</span><span class="p">))</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[This article contributes to the debate a...
  These are questions put to Salvador Dali...
  We take issue with some methodological u...
  We’ll show some situations in which reve...
  In the second part of this paper, we pro...
  In their paper, Floridi and Chiriatti pr...
  The logic of Floridi and Chiriatti’s rev...
  The most mature theory quantifying such ...
...
</code></pre></div>

</div>

<h3 id="build-custom-retriever-and-rag-modules">Build Custom Retriever and RAG modules</h3>
<p>For complex modules we can subclass <code>TextModule</code> and as in PyTorch define our own init and forward methods.</p>
<p>Using how <code>TextTensors</code> can automatically act as a<code>Tensor</code> of its embeddings, we can very compactly implement e.g. a retriever, which for an input entry finds  <code>k</code> entries with the highest cosine similarity among the documents it holds:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Retriever</span><span class="p">(</span><span class="n">TextModule</span><span class="p">):</span>  
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">:</span> <span class="n">TextTensor</span><span class="p">):</span>  
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">documents</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="n">TextTensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>  
        <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">documents</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>  
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">documents</span><span class="p">[</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">retriever</span> <span class="o">=</span> <span class="n">Retriever</span><span class="p">(</span><span class="n">paragraphs</span><span class="p">)</span>  
<span class="n">query</span> <span class="o">=</span> <span class="n">TextTensor</span><span class="p">(</span><span class="s2">&quot;What&#39;s the relationship between prediction and compression?&quot;</span><span class="p">)</span>  
<span class="n">retriever</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[Recall how a language model during training must compress an untenable number of conditional probabilities. The only wa
  to do this successfully is to pick up on the regularities in language (as pioneered by Shannon 1948). Why do we claim   
  that learning to predict words, as GPT does, can be treated as compressing some information? Let’s assume we’ve         
  calculated the conditional probability distribution given only the previous word of all English words. Consider, that   
  such a language model can either be used as a (Markavion) language generator or, following Shannon, be used for an      
  efficient compression of English texts. Continuing this duality, it has been shown, that if a language model such as GPT
  would be perfectly trained it can be used to optimally compress any English text (using arithmetic coding on its        
  predicted probabilities; Shmilovici et al., 2009). Thus the relationship between prediction and compression is that     
  training a language generator is equivalent to training a compressor, and a compressor must know something about the    
  regularities present in its domain (as formalized in AIXI theory; Mahoney 2006). To make good predictions it is not     
  enough to compress information about what words to use to remain grammatical (to have a syntactical capacity), but also 
...  ]
</code></pre></div>

</div>
<p>Note how the implementation didn't require us to learn about any new operations we would not find in regular PyTorch. One goal of LangTorch is to give developers control over these lower-level operations while being able to write compact code without a multitude of classes. For this reason, implementations such as the retriever above are not pre-defined classes in the main package.</p>
<p>We can now compose this module with a Module making LLM calls to get a custom Retrieval Augmented Generation pipeline:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">RAG</span><span class="p">(</span><span class="n">TextModule</span><span class="p">):</span>  
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">documents</span><span class="p">:</span> <span class="n">TextTensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">Retriever</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>  

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_message</span><span class="p">:</span> <span class="n">TextTensor</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>  
        <span class="n">retrieved_context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="p">(</span><span class="n">user_message</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>  
        <span class="n">user_message</span> <span class="o">=</span> <span class="n">user_message</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">CONTEXT:</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">retrieved_context</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">user_message</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">rag_chat</span> <span class="o">=</span> <span class="n">RAG</span><span class="p">(</span><span class="n">paragraphs</span><span class="p">,</span>  
               <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Use the context to answer the following user query: &quot;</span><span class="p">,</span>  
               <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>  
<span class="n">assistant_response</span> <span class="o">=</span> <span class="n">rag_chat</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>  
<span class="nb">print</span><span class="p">(</span><span class="n">assistant_response</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
<div class="code-ouput"> <!-- Replace #f0f0f0 with your desired color -->

<div class="highlight"><span class="filename">Output:</span><pre><span></span><code>[[The relationship between prediction and compression is that training a language generator, such as GPT, is equivalent to training a compres↵
  sor. This is because in order to make accurate predictions, the model must compress information about the regularities present in the langu↵ 
  age it is trained on. This compression of information allows for generalization, which in turn leads to the ability to operate on novel inp↵ 
  uts and create novel outputs. So, prediction leads to compression, which leads to generalization, and ultimately to computer intelligence.  ]]
</code></pre></div>

</div>
<p>With only small modifications to the retriever, this module could also perform batched inference — performing multiple simultaneous queries without much additional latency. Note, <code>prompt</code> and <code>activation</code> are arguments inherited from TextModule and need the <code>super().forward</code> call to work.</p>
<p>We are excited to see what you will build with LangTorch. If you want to share some examples or have any questions, feel free to ask on our <a href="https://discord.gg/jkreqtCCkv">discord</a>. In the likely event of encountering a bug, send it on discord or post on the <a href="https://github.com/AdamSobieszek/langtorch">GitHub Repo</a> and we will fix it ASAP.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.code.annotate", "content.code.copy", "navigation.indexes", "navigation.sections"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.8fd75fb4.min.js"></script>
      
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
      
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../css/tabs.js"></script>
      
    
  </body>
</html>